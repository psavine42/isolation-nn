
Epoch: 1/20, step: 43060, training_loss: 1.92772
Epoch: 1/20, step: 43080, training_loss: 1.80576
Epoch: 1/20, step: 43100, training_loss: 2.41112
Epoch: 1/20, step: 43120, training_loss: 3.06127
Epoch: 1/20, step: 43140, training_loss: 2.54313
Epoch: 1/20, step: 43160, training_loss: 1.99324
Epoch: 1/20, step: 43180, training_loss: 2.88456
Epoch: 1/20, step: 43200, training_loss: 1.97660
Epoch: 1/20, step: 43220, training_loss: 1.97959
Epoch: 1/20, step: 43240, training_loss: 2.05616
Epoch: 1/20, step: 43260, training_loss: 2.18792
Epoch: 1/20, step: 43280, training_loss: 2.74694
Epoch: 1/20, step: 43300, training_loss: 2.53193
Epoch: 1/20, step: 43320, training_loss: 1.98984
Epoch: 1/20, step: 43340, training_loss: 2.60585
Epoch: 1/20, step: 43360, training_loss: 2.91319
Epoch: 1/20, step: 43380, training_loss: 2.29569
Epoch: 1/20, step: 43400, training_loss: 2.24060
Epoch: 1/20, step: 43420, training_loss: 2.04400
Epoch: 1/20, step: 43440, training_loss: 3.11832
Epoch: 1/20, step: 43460, training_loss: 2.59187
Epoch: 1/20, step: 43480, training_loss: 2.42196
Epoch: 1/20, step: 43500, training_loss: 2.53940
Epoch: 1/20, step: 43520, training_loss: 2.82646
Epoch: 1/20, step: 43540, training_loss: 2.20755
Epoch: 1/20, step: 43560, training_loss: 2.44698
Epoch: 1/20, step: 43580, training_loss: 2.21035
Epoch: 1/20, step: 43600, training_loss: 2.04094
Epoch: 1/20, step: 43620, training_loss: 2.57870
Epoch: 1/20, step: 43640, training_loss: 3.23725
Epoch: 1/20, step: 43660, training_loss: 2.14513
Epoch: 1/20, step: 43680, training_loss: 3.05703
Epoch: 1/20, step: 43700, training_loss: 2.11644
Epoch: 1/20, step: 43720, training_loss: 1.59750
Epoch: 1/20, step: 43740, training_loss: 2.25050
Epoch: 1/20, step: 43760, training_loss: 1.83779
Epoch: 1/20, step: 43780, training_loss: 2.30039
Epoch: 1/20, step: 43800, training_loss: 1.34213
Epoch: 1/20, step: 43820, training_loss: 2.26016
Epoch: 1/20, step: 43840, training_loss: 2.04180
Epoch: 1/20, step: 43860, training_loss: 2.39112
Epoch: 1/20, step: 43880, training_loss: 3.01537
Epoch: 1/20, step: 43900, training_loss: 2.56551
Epoch: 1/20, step: 43920, training_loss: 2.48552
Epoch: 1/20, step: 43940, training_loss: 2.69840
Epoch: 1/20, step: 43960, training_loss: 2.89381
Epoch: 1/20, step: 43980, training_loss: 2.85926
Epoch: 1/20, step: 44000, training_loss: 2.34933
accuracy: 0.32, validation_loss: 2.528542995452881, num_samples: 100
Epoch: 1/20, step: 44020, training_loss: 2.10930
Epoch: 1/20, step: 44040, training_loss: 2.77897
Epoch: 1/20, step: 44060, training_loss: 2.63897
Epoch: 1/20, step: 44080, training_loss: 2.39289
Epoch: 1/20, step: 44100, training_loss: 2.67250
Epoch: 1/20, step: 44120, training_loss: 1.74178
Epoch: 1/20, step: 44140, training_loss: 2.39407
Epoch: 1/20, step: 44160, training_loss: 2.25031
Epoch: 1/20, step: 44180, training_loss: 2.86820
Epoch: 1/20, step: 44200, training_loss: 1.24284
Epoch: 1/20, step: 44220, training_loss: 2.86216
Epoch: 1/20, step: 44240, training_loss: 1.81755
Epoch: 1/20, step: 44260, training_loss: 2.22487
Epoch: 1/20, step: 44280, training_loss: 2.76017
Epoch: 1/20, step: 44300, training_loss: 2.84750
Epoch: 1/20, step: 44320, training_loss: 2.05248
Epoch: 1/20, step: 44340, training_loss: 2.41264
Epoch: 1/20, step: 44360, training_loss: 2.50572
Epoch: 1/20, step: 44380, training_loss: 2.76049
Epoch: 1/20, step: 44400, training_loss: 1.62154
Epoch: 1/20, step: 44420, training_loss: 2.15494
Epoch: 1/20, step: 44440, training_loss: 2.80534
Epoch: 1/20, step: 44460, training_loss: 2.53957
Epoch: 1/20, step: 44480, training_loss: 2.47107
Epoch: 1/20, step: 44500, training_loss: 1.83575
Epoch: 1/20, step: 44520, training_loss: 2.11285
Epoch: 1/20, step: 44540, training_loss: 2.29687
Epoch: 1/20, step: 44560, training_loss: 2.33483
Epoch: 1/20, step: 44580, training_loss: 2.57370
Epoch: 1/20, step: 44600, training_loss: 2.07624
Epoch: 1/20, step: 44620, training_loss: 2.32956
Epoch: 1/20, step: 44640, training_loss: 2.30756
Epoch: 1/20, step: 44660, training_loss: 3.08131
Epoch: 1/20, step: 44680, training_loss: 1.76568
Epoch: 1/20, step: 44700, training_loss: 2.93116
Epoch: 1/20, step: 44720, training_loss: 2.78380
Epoch: 1/20, step: 44740, training_loss: 1.41509
Epoch: 1/20, step: 44760, training_loss: 2.45485
Epoch: 1/20, step: 44780, training_loss: 2.09184
Epoch: 1/20, step: 44800, training_loss: 2.61756
Epoch: 1/20, step: 44820, training_loss: 3.12402
Epoch: 1/20, step: 44840, training_loss: 2.43153
Epoch: 1/20, step: 44860, training_loss: 2.91816
Epoch: 1/20, step: 44880, training_loss: 1.99294
Epoch: 1/20, step: 44900, training_loss: 2.34681
Epoch: 1/20, step: 44920, training_loss: 2.16869
Epoch: 1/20, step: 44940, training_loss: 2.52857
Epoch: 1/20, step: 44960, training_loss: 3.41664
Epoch: 1/20, step: 44980, training_loss: 2.72210
Epoch: 1/20, step: 45000, training_loss: 2.35256
accuracy: 0.4, validation_loss: 2.4236109256744385, num_samples: 100
Epoch: 1/20, step: 45020, training_loss: 1.86084
Epoch: 1/20, step: 45040, training_loss: 2.43066
Epoch: 1/20, step: 45060, training_loss: 2.80980
Epoch: 1/20, step: 45080, training_loss: 3.00130
Epoch: 1/20, step: 45100, training_loss: 2.88048
Epoch: 1/20, step: 45120, training_loss: 2.03692
Epoch: 1/20, step: 45140, training_loss: 2.18734
Epoch: 1/20, step: 45160, training_loss: 2.74695
Epoch: 1/20, step: 45180, training_loss: 1.44491
Epoch: 1/20, step: 45200, training_loss: 2.21718
Epoch: 1/20, step: 45220, training_loss: 2.58610
Epoch: 1/20, step: 45240, training_loss: 2.68147
Epoch: 1/20, step: 45260, training_loss: 2.53632
Epoch: 1/20, step: 45280, training_loss: 2.91736
Epoch: 1/20, step: 45300, training_loss: 2.41156
Epoch: 1/20, step: 45320, training_loss: 2.28459
Epoch: 1/20, step: 45340, training_loss: 2.53034
Epoch: 1/20, step: 45360, training_loss: 1.99717
Epoch: 1/20, step: 45380, training_loss: 2.26257
Epoch: 1/20, step: 45400, training_loss: 3.02836
Epoch: 1/20, step: 45420, training_loss: 2.06439
Epoch: 1/20, step: 45440, training_loss: 2.72449
Epoch: 1/20, step: 45460, training_loss: 2.85827
Epoch: 1/20, step: 45480, training_loss: 2.58528
Epoch: 1/20, step: 45500, training_loss: 3.05228
Epoch: 1/20, step: 45520, training_loss: 3.15191
Epoch: 1/20, step: 45540, training_loss: 1.32333
Epoch: 1/20, step: 45560, training_loss: 2.51774
Epoch: 1/20, step: 45580, training_loss: 2.76551
Epoch: 1/20, step: 45600, training_loss: 3.11769
Epoch: 1/20, step: 45620, training_loss: 2.70960
Epoch: 1/20, step: 45640, training_loss: 2.54378
Epoch: 1/20, step: 45660, training_loss: 1.77595
Epoch: 1/20, step: 45680, training_loss: 2.45147
Epoch: 1/20, step: 45700, training_loss: 2.39121
Epoch: 1/20, step: 45720, training_loss: 2.19976
Epoch: 1/20, step: 45740, training_loss: 2.13582
Epoch: 1/20, step: 45760, training_loss: 2.95292
Epoch: 1/20, step: 45780, training_loss: 1.86854
Epoch: 1/20, step: 45800, training_loss: 2.35153
Epoch: 1/20, step: 45820, training_loss: 2.23369
Epoch: 1/20, step: 45840, training_loss: 1.72573
Epoch: 1/20, step: 45860, training_loss: 2.23328
Epoch: 1/20, step: 45880, training_loss: 1.92431
Epoch: 1/20, step: 45900, training_loss: 2.12635
Epoch: 1/20, step: 45920, training_loss: 2.38663
Epoch: 1/20, step: 45940, training_loss: 2.74079
Epoch: 1/20, step: 45960, training_loss: 2.58826
Epoch: 1/20, step: 45980, training_loss: 2.56003
Epoch: 1/20, step: 46000, training_loss: 2.09092
accuracy: 0.48, validation_loss: 2.1672608852386475, num_samples: 100
Epoch: 1/20, step: 46020, training_loss: 2.50905
Epoch: 1/20, step: 46040, training_loss: 2.64453
Epoch: 1/20, step: 46060, training_loss: 2.28585
Epoch: 1/20, step: 46080, training_loss: 2.93179
Epoch: 1/20, step: 46100, training_loss: 2.60367
Epoch: 1/20, step: 46120, training_loss: 2.53870
Epoch: 1/20, step: 46140, training_loss: 1.91102
Epoch: 1/20, step: 46160, training_loss: 1.67818
Epoch: 1/20, step: 46180, training_loss: 2.42758
Epoch: 1/20, step: 46200, training_loss: 3.06969
Epoch: 1/20, step: 46220, training_loss: 1.85985
Epoch: 1/20, step: 46240, training_loss: 3.75342
Epoch: 1/20, step: 46260, training_loss: 1.97889
Epoch: 1/20, step: 46280, training_loss: 2.14664
Epoch: 1/20, step: 46300, training_loss: 3.05661
Epoch: 1/20, step: 46320, training_loss: 2.04698
Epoch: 1/20, step: 46340, training_loss: 2.11658
Epoch: 1/20, step: 46360, training_loss: 1.80977
Epoch: 1/20, step: 46380, training_loss: 2.14908
Epoch: 1/20, step: 46400, training_loss: 2.45924
Epoch: 1/20, step: 46420, training_loss: 2.17949
Epoch: 1/20, step: 46440, training_loss: 2.75041
Epoch: 1/20, step: 46460, training_loss: 1.28897
Epoch: 1/20, step: 46480, training_loss: 1.82765
Epoch: 1/20, step: 46500, training_loss: 3.14479
Epoch: 1/20, step: 46520, training_loss: 2.98090
Epoch: 1/20, step: 46540, training_loss: 3.09914
Epoch: 1/20, step: 46560, training_loss: 2.23465
Epoch: 1/20, step: 46580, training_loss: 2.19323
Epoch: 1/20, step: 46600, training_loss: 2.33471
Epoch: 1/20, step: 46620, training_loss: 2.08428
Epoch: 1/20, step: 46640, training_loss: 2.33220
Epoch: 1/20, step: 46660, training_loss: 2.33752
Epoch: 1/20, step: 46680, training_loss: 1.79374
Epoch: 1/20, step: 46700, training_loss: 2.90444
Epoch: 1/20, step: 46720, training_loss: 2.06522
Epoch: 1/20, step: 46740, training_loss: 2.47544
Epoch: 1/20, step: 46760, training_loss: 2.08372
Epoch: 1/20, step: 46780, training_loss: 2.11425
Epoch: 1/20, step: 46800, training_loss: 2.37383
Epoch: 1/20, step: 46820, training_loss: 2.22117
Epoch: 1/20, step: 46840, training_loss: 2.64794
Epoch: 1/20, step: 46860, training_loss: 2.46963
Epoch: 1/20, step: 46880, training_loss: 1.72487
Epoch: 1/20, step: 46900, training_loss: 2.03494
Epoch: 1/20, step: 46920, training_loss: 1.39855
Epoch: 1/20, step: 46940, training_loss: 2.42407
Epoch: 1/20, step: 46960, training_loss: 2.40211
Epoch: 1/20, step: 46980, training_loss: 2.14289
Epoch: 1/20, step: 47000, training_loss: 2.97368
accuracy: 0.44, validation_loss: 2.089643716812134, num_samples: 100
Epoch: 1/20, step: 47020, training_loss: 2.92734
Epoch: 1/20, step: 47040, training_loss: 2.64948
Epoch: 1/20, step: 47060, training_loss: 1.81084
Epoch: 1/20, step: 47080, training_loss: 2.92860
Epoch: 1/20, step: 47100, training_loss: 2.14634
Epoch: 1/20, step: 47120, training_loss: 2.07560
Epoch: 1/20, step: 47140, training_loss: 2.10496
Epoch: 1/20, step: 47160, training_loss: 2.31666
Epoch: 1/20, step: 47180, training_loss: 2.28690
Epoch: 1/20, step: 47200, training_loss: 2.12653
Epoch: 1/20, step: 47220, training_loss: 3.14892
Epoch: 1/20, step: 47240, training_loss: 2.83310
Epoch: 1/20, step: 47260, training_loss: 3.44141
Epoch: 1/20, step: 47280, training_loss: 2.53591
Epoch: 1/20, step: 47300, training_loss: 3.09102
Epoch: 1/20, step: 47320, training_loss: 1.99391
Epoch: 1/20, step: 47340, training_loss: 2.32668
Epoch: 1/20, step: 47360, training_loss: 2.81547
Epoch: 1/20, step: 47380, training_loss: 3.09608
Epoch: 1/20, step: 47400, training_loss: 2.24899
Epoch: 1/20, step: 47420, training_loss: 2.22835
Epoch: 1/20, step: 47440, training_loss: 2.51833
Epoch: 1/20, step: 47460, training_loss: 2.01471
Epoch: 1/20, step: 47480, training_loss: 2.65698
Epoch: 1/20, step: 47500, training_loss: 2.54660
Epoch: 1/20, step: 47520, training_loss: 2.68741
Epoch: 1/20, step: 47540, training_loss: 1.74429
Epoch: 1/20, step: 47560, training_loss: 1.58156
Epoch: 1/20, step: 47580, training_loss: 2.52246
Epoch: 1/20, step: 47600, training_loss: 1.64311
Epoch: 1/20, step: 47620, training_loss: 2.48469
Epoch: 1/20, step: 47640, training_loss: 1.99474
Epoch: 1/20, step: 47660, training_loss: 2.32463
Epoch: 1/20, step: 47680, training_loss: 1.30968
Epoch: 1/20, step: 47700, training_loss: 3.58723
Epoch: 1/20, step: 47720, training_loss: 1.72279
Epoch: 1/20, step: 47740, training_loss: 2.38247
Epoch: 1/20, step: 47760, training_loss: 3.28410
Epoch: 1/20, step: 47780, training_loss: 2.23168
Epoch: 1/20, step: 47800, training_loss: 1.54147
Epoch: 1/20, step: 47820, training_loss: 1.97302
Epoch: 1/20, step: 47840, training_loss: 1.80352
Epoch: 1/20, step: 47860, training_loss: 1.82285
Epoch: 1/20, step: 47880, training_loss: 2.08488
Epoch: 1/20, step: 47900, training_loss: 2.76187
Epoch: 1/20, step: 47920, training_loss: 2.09921
Epoch: 1/20, step: 47940, training_loss: 2.52687
Epoch: 1/20, step: 47960, training_loss: 2.48861
Epoch: 1/20, step: 47980, training_loss: 3.18798
Epoch: 1/20, step: 48000, training_loss: 2.49112
accuracy: 0.39, validation_loss: 2.254772901535034, num_samples: 100
Epoch: 1/20, step: 48020, training_loss: 1.37684
Epoch: 1/20, step: 48040, training_loss: 2.64795
Epoch: 1/20, step: 48060, training_loss: 2.53413
Epoch: 1/20, step: 48080, training_loss: 2.39809
Epoch: 1/20, step: 48100, training_loss: 2.75211
Epoch: 1/20, step: 48120, training_loss: 2.03955
Epoch: 1/20, step: 48140, training_loss: 1.84995
Epoch: 1/20, step: 48160, training_loss: 2.44769
Epoch: 1/20, step: 48180, training_loss: 2.86270
Epoch: 1/20, step: 48200, training_loss: 2.56425
Epoch: 1/20, step: 48220, training_loss: 2.32681
Epoch: 1/20, step: 48240, training_loss: 2.40260
Epoch: 1/20, step: 48260, training_loss: 2.21678
Epoch: 1/20, step: 48280, training_loss: 1.60840
Epoch: 1/20, step: 48300, training_loss: 3.13427
Epoch: 1/20, step: 48320, training_loss: 2.44512
Epoch: 1/20, step: 48340, training_loss: 2.83910
Epoch: 1/20, step: 48360, training_loss: 2.34318
Epoch: 1/20, step: 48380, training_loss: 2.75860
Epoch: 1/20, step: 48400, training_loss: 2.16809
Epoch: 1/20, step: 48420, training_loss: 1.44497
Epoch: 1/20, step: 48440, training_loss: 2.86369
Epoch: 1/20, step: 48460, training_loss: 1.78983
Epoch: 1/20, step: 48480, training_loss: 3.02374
Epoch: 1/20, step: 48500, training_loss: 2.35464
Epoch: 1/20, step: 48520, training_loss: 1.94319
Epoch: 1/20, step: 48540, training_loss: 1.99122
Epoch: 1/20, step: 48560, training_loss: 2.93812
Epoch: 1/20, step: 48580, training_loss: 1.75549
Epoch: 1/20, step: 48600, training_loss: 2.54619
Epoch: 1/20, step: 48620, training_loss: 2.30343
Epoch: 1/20, step: 48640, training_loss: 2.33747
Epoch: 1/20, step: 48660, training_loss: 2.57724
Epoch: 1/20, step: 48680, training_loss: 2.77067
Epoch: 1/20, step: 48700, training_loss: 2.00535
Epoch: 1/20, step: 48720, training_loss: 2.64303
Epoch: 1/20, step: 48740, training_loss: 2.63960
Epoch: 1/20, step: 48760, training_loss: 2.30515
Epoch: 1/20, step: 48780, training_loss: 2.60923
Epoch: 1/20, step: 48800, training_loss: 3.51657
Epoch: 1/20, step: 48820, training_loss: 2.91695
Epoch: 1/20, step: 48840, training_loss: 1.83420
Epoch: 1/20, step: 48860, training_loss: 2.80991
Epoch: 1/20, step: 48880, training_loss: 2.41266
Epoch: 1/20, step: 48900, training_loss: 2.27410
Epoch: 1/20, step: 48920, training_loss: 2.61215
Epoch: 1/20, step: 48940, training_loss: 2.42156
Epoch: 1/20, step: 48960, training_loss: 3.10885
Epoch: 1/20, step: 48980, training_loss: 2.66168
Epoch: 1/20, step: 49000, training_loss: 2.40466
accuracy: 0.32, validation_loss: 2.457481622695923, num_samples: 100
Epoch: 1/20, step: 49020, training_loss: 2.91281
Epoch: 1/20, step: 49040, training_loss: 1.25049
Epoch: 1/20, step: 49060, training_loss: 1.83571
Epoch: 1/20, step: 49080, training_loss: 2.77126
Epoch: 1/20, step: 49100, training_loss: 2.45333
Epoch: 1/20, step: 49120, training_loss: 2.48926
Epoch: 1/20, step: 49140, training_loss: 1.77643
Epoch: 1/20, step: 49160, training_loss: 3.00033
Epoch: 1/20, step: 49180, training_loss: 2.41079
Epoch: 1/20, step: 49200, training_loss: 2.29341
Epoch: 1/20, step: 49220, training_loss: 1.69385
Epoch: 1/20, step: 49240, training_loss: 1.81392
Epoch: 1/20, step: 49260, training_loss: 2.06997
Epoch: 1/20, step: 49280, training_loss: 1.87531
Epoch: 1/20, step: 49300, training_loss: 1.99589
Epoch: 1/20, step: 49320, training_loss: 2.67749
Epoch: 1/20, step: 49340, training_loss: 1.83195
Epoch: 1/20, step: 49360, training_loss: 2.86851
Epoch: 1/20, step: 49380, training_loss: 2.29333
Epoch: 1/20, step: 49400, training_loss: 2.49894
Epoch: 1/20, step: 49420, training_loss: 2.20353
Epoch: 1/20, step: 49440, training_loss: 2.37418
Epoch: 1/20, step: 49460, training_loss: 1.42796
Epoch: 1/20, step: 49480, training_loss: 2.01934
Epoch: 1/20, step: 49500, training_loss: 2.14936
Epoch: 1/20, step: 49520, training_loss: 2.39458
Epoch: 1/20, step: 49540, training_loss: 2.90066
Epoch: 1/20, step: 49560, training_loss: 2.20676
Epoch: 1/20, step: 49580, training_loss: 2.95083
Epoch: 1/20, step: 49600, training_loss: 2.89648
Epoch: 1/20, step: 49620, training_loss: 2.10978
Epoch: 1/20, step: 49640, training_loss: 2.05612
Epoch: 1/20, step: 49660, training_loss: 2.44149
Epoch: 1/20, step: 49680, training_loss: 2.58219
Epoch: 1/20, step: 49700, training_loss: 2.02230
Epoch: 1/20, step: 49720, training_loss: 2.54810
Epoch: 1/20, step: 49740, training_loss: 2.77142
Epoch: 1/20, step: 49760, training_loss: 2.68212
Epoch: 1/20, step: 49780, training_loss: 2.07494
Epoch: 1/20, step: 49800, training_loss: 2.14079
Epoch: 1/20, step: 49820, training_loss: 2.41206
Epoch: 1/20, step: 49840, training_loss: 1.96338
Epoch: 1/20, step: 49860, training_loss: 1.91147
Epoch: 1/20, step: 49880, training_loss: 2.44338
Epoch: 1/20, step: 49900, training_loss: 2.88369
Epoch: 1/20, step: 49920, training_loss: 2.02321
Epoch: 1/20, step: 49940, training_loss: 1.93881
Epoch: 1/20, step: 49960, training_loss: 2.19696
Epoch: 1/20, step: 49980, training_loss: 2.64001
Epoch: 1/20, step: 50000, training_loss: 2.13021
accuracy: 0.37, validation_loss: 2.494048833847046, num_samples: 100
Epoch: 1/20, step: 50020, training_loss: 1.54893
Epoch: 1/20, step: 50040, training_loss: 1.58386
Epoch: 1/20, step: 50060, training_loss: 2.33998
Epoch: 1/20, step: 50080, training_loss: 2.25109
Epoch: 1/20, step: 50100, training_loss: 2.60400
Epoch: 1/20, step: 50120, training_loss: 2.13514
Epoch: 1/20, step: 50140, training_loss: 2.84049
Epoch: 1/20, step: 50160, training_loss: 2.77142
Epoch: 1/20, step: 50180, training_loss: 2.26898
Epoch: 1/20, step: 50200, training_loss: 1.96736
Epoch: 1/20, step: 50220, training_loss: 2.07765
Epoch: 1/20, step: 50240, training_loss: 2.30346
Epoch: 1/20, step: 50260, training_loss: 1.71725
Epoch: 1/20, step: 50280, training_loss: 3.18630
Epoch: 1/20, step: 50300, training_loss: 2.48031
Epoch: 1/20, step: 50320, training_loss: 2.46584
Epoch: 1/20, step: 50340, training_loss: 2.32347
Epoch: 1/20, step: 50360, training_loss: 2.46232
Epoch: 1/20, step: 50380, training_loss: 1.99787
Epoch: 1/20, step: 50400, training_loss: 2.65982
Epoch: 1/20, step: 50420, training_loss: 1.60248
Epoch: 1/20, step: 50440, training_loss: 2.22062
Epoch: 1/20, step: 50460, training_loss: 3.03779
Epoch: 1/20, step: 50480, training_loss: 2.07141
Epoch: 1/20, step: 50500, training_loss: 2.16367
Epoch: 1/20, step: 50520, training_loss: 2.59137
Epoch: 1/20, step: 50540, training_loss: 2.09628
Epoch: 1/20, step: 50560, training_loss: 2.70921
Epoch: 1/20, step: 50580, training_loss: 2.61138
Epoch: 1/20, step: 50600, training_loss: 2.28162
Epoch: 1/20, step: 50620, training_loss: 2.34803
Epoch: 1/20, step: 50640, training_loss: 2.52307
Epoch: 1/20, step: 50660, training_loss: 2.12708
Epoch: 1/20, step: 50680, training_loss: 2.48188
Epoch: 1/20, step: 50700, training_loss: 2.41924
Epoch: 1/20, step: 50720, training_loss: 2.52203
Epoch: 1/20, step: 50740, training_loss: 3.12172
Epoch: 1/20, step: 50760, training_loss: 2.27211
Epoch: 1/20, step: 50780, training_loss: 2.38060
Epoch: 1/20, step: 50800, training_loss: 2.96999
Epoch: 1/20, step: 50820, training_loss: 2.10584
Epoch: 1/20, step: 50840, training_loss: 2.36767
Epoch: 1/20, step: 50860, training_loss: 2.82566
Epoch: 1/20, step: 50880, training_loss: 2.61680
Epoch: 1/20, step: 50900, training_loss: 2.30860
Epoch: 1/20, step: 50920, training_loss: 3.04341
Epoch: 1/20, step: 50940, training_loss: 2.86618
Epoch: 1/20, step: 50960, training_loss: 2.23767
Epoch: 1/20, step: 50980, training_loss: 2.06152
Epoch: 1/20, step: 51000, training_loss: 2.50626
accuracy: 0.45, validation_loss: 2.124706983566284, num_samples: 100
Epoch: 1/20, step: 51020, training_loss: 2.92064
Epoch: 1/20, step: 51040, training_loss: 2.89891
Epoch: 1/20, step: 51060, training_loss: 2.38504
Epoch: 1/20, step: 51080, training_loss: 1.71985
Epoch: 1/20, step: 51100, training_loss: 2.32972
Epoch: 1/20, step: 51120, training_loss: 1.89656
Epoch: 1/20, step: 51140, training_loss: 3.13610
Epoch: 1/20, step: 51160, training_loss: 1.72805
Epoch: 1/20, step: 51180, training_loss: 2.42188
Epoch: 1/20, step: 51200, training_loss: 2.52193
Epoch: 1/20, step: 51220, training_loss: 2.28775
Epoch: 1/20, step: 51240, training_loss: 2.05879
Epoch: 1/20, step: 51260, training_loss: 1.56958
Epoch: 1/20, step: 51280, training_loss: 2.23107
Epoch: 1/20, step: 51300, training_loss: 2.01929
Epoch: 1/20, step: 51320, training_loss: 1.35276
Epoch: 1/20, step: 51340, training_loss: 2.18210
Epoch: 1/20, step: 51360, training_loss: 2.32680
Epoch: 1/20, step: 51380, training_loss: 2.61232
Epoch: 1/20, step: 51400, training_loss: 2.80987
Epoch: 1/20, step: 51420, training_loss: 2.27233
Epoch: 1/20, step: 51440, training_loss: 2.46453
Epoch: 1/20, step: 51460, training_loss: 2.93193
Epoch: 1/20, step: 51480, training_loss: 2.57642
Epoch: 1/20, step: 51500, training_loss: 3.01233
Epoch: 1/20, step: 51520, training_loss: 2.79675
Epoch: 1/20, step: 51540, training_loss: 1.94060
Epoch: 1/20, step: 51560, training_loss: 2.40781
Epoch: 1/20, step: 51580, training_loss: 1.79327
Epoch: 1/20, step: 51600, training_loss: 2.12558
Epoch: 1/20, step: 51620, training_loss: 2.25586
Epoch: 1/20, step: 51640, training_loss: 1.82465
Epoch: 1/20, step: 51660, training_loss: 1.81503
Epoch: 1/20, step: 51680, training_loss: 2.43813
Epoch: 1/20, step: 51700, training_loss: 3.36890
Epoch: 1/20, step: 51720, training_loss: 1.71563
Epoch: 1/20, step: 51740, training_loss: 2.04415
Epoch: 1/20, step: 51760, training_loss: 2.62090
Epoch: 1/20, step: 51780, training_loss: 2.69051
Epoch: 1/20, step: 51800, training_loss: 2.49472
Epoch: 1/20, step: 51820, training_loss: 1.88783
Epoch: 1/20, step: 51840, training_loss: 2.46790
Epoch: 1/20, step: 51860, training_loss: 2.38904
Epoch: 1/20, step: 51880, training_loss: 2.18930
Epoch: 1/20, step: 51900, training_loss: 2.74952
Epoch: 1/20, step: 51920, training_loss: 2.23153
Epoch: 1/20, step: 51940, training_loss: 1.97044
Epoch: 1/20, step: 51960, training_loss: 1.50614
Epoch: 1/20, step: 51980, training_loss: 2.05383
Epoch: 1/20, step: 52000, training_loss: 1.49455
accuracy: 0.45, validation_loss: 2.2349119186401367, num_samples: 100
Epoch: 1/20, step: 52020, training_loss: 2.91923
Epoch: 1/20, step: 52040, training_loss: 2.69757
Epoch: 1/20, step: 52060, training_loss: 3.37045
Epoch: 1/20, step: 52080, training_loss: 2.72412
Epoch: 1/20, step: 52100, training_loss: 2.38980
Epoch: 1/20, step: 52120, training_loss: 2.78404
Epoch: 1/20, step: 52140, training_loss: 1.56789
Epoch: 1/20, step: 52160, training_loss: 2.19396
Epoch: 1/20, step: 52180, training_loss: 2.16341
Epoch: 1/20, step: 52200, training_loss: 2.94849
Epoch: 1/20, step: 52220, training_loss: 1.69763
Epoch: 1/20, step: 52240, training_loss: 3.27542
Epoch: 1/20, step: 52260, training_loss: 3.36024
Epoch: 1/20, step: 52280, training_loss: 2.10110
Epoch: 1/20, step: 52300, training_loss: 2.64172
Epoch: 1/20, step: 52320, training_loss: 2.06252
Epoch: 1/20, step: 52340, training_loss: 2.47883
Epoch: 1/20, step: 52360, training_loss: 2.75111
Epoch: 1/20, step: 52380, training_loss: 1.87092
Epoch: 1/20, step: 52400, training_loss: 2.81030
Epoch: 1/20, step: 52420, training_loss: 3.04908
Epoch: 1/20, step: 52440, training_loss: 1.48359
Epoch: 1/20, step: 52460, training_loss: 2.74976
Epoch: 1/20, step: 52480, training_loss: 1.94269
Epoch: 1/20, step: 52500, training_loss: 1.99503
Epoch: 1/20, step: 52520, training_loss: 2.58716
Epoch: 1/20, step: 52540, training_loss: 2.54488
Epoch: 1/20, step: 52560, training_loss: 2.78278
Epoch: 1/20, step: 52580, training_loss: 2.09428
Epoch: 1/20, step: 52600, training_loss: 2.79629
Epoch: 1/20, step: 52620, training_loss: 1.92175
Epoch: 1/20, step: 52640, training_loss: 1.62661
Epoch: 1/20, step: 52660, training_loss: 1.98239
Epoch: 1/20, step: 52680, training_loss: 2.23965
Epoch: 1/20, step: 52700, training_loss: 2.28220
Epoch: 1/20, step: 52720, training_loss: 2.43279
Epoch: 1/20, step: 52740, training_loss: 3.02558
Epoch: 1/20, step: 52760, training_loss: 1.99934
Epoch: 1/20, step: 52780, training_loss: 2.24353
Epoch: 1/20, step: 52800, training_loss: 2.12779
Epoch: 1/20, step: 52820, training_loss: 1.39824
Epoch: 1/20, step: 52840, training_loss: 2.43774
Epoch: 1/20, step: 52860, training_loss: 2.40947
Epoch: 1/20, step: 52880, training_loss: 1.71142
Epoch: 1/20, step: 52900, training_loss: 2.16241
Epoch: 1/20, step: 52920, training_loss: 1.63192
Epoch: 1/20, step: 52940, training_loss: 3.26865
Epoch: 1/20, step: 52960, training_loss: 2.52028
Epoch: 1/20, step: 52980, training_loss: 1.75680
Epoch: 1/20, step: 53000, training_loss: 1.39763
accuracy: 0.41, validation_loss: 2.33250093460083, num_samples: 100
Epoch: 1/20, step: 53020, training_loss: 2.40006
Epoch: 1/20, step: 53040, training_loss: 2.08197
Epoch: 1/20, step: 53060, training_loss: 2.46854
Epoch: 1/20, step: 53080, training_loss: 2.40526
Epoch: 1/20, step: 53100, training_loss: 2.59935
Epoch: 1/20, step: 53120, training_loss: 2.57924
Epoch: 1/20, step: 53140, training_loss: 1.62495
Epoch: 1/20, step: 53160, training_loss: 2.45973
Epoch: 1/20, step: 53180, training_loss: 2.62632
Epoch: 1/20, step: 53200, training_loss: 2.17595
Epoch: 1/20, step: 53220, training_loss: 2.52931
Epoch: 1/20, step: 53240, training_loss: 1.76014
Epoch: 1/20, step: 53260, training_loss: 2.14177
Epoch: 1/20, step: 53280, training_loss: 2.49710
Epoch: 1/20, step: 53300, training_loss: 2.35250
Epoch: 1/20, step: 53320, training_loss: 2.46681
Epoch: 1/20, step: 53340, training_loss: 2.25016
Epoch: 1/20, step: 53360, training_loss: 2.59505
Epoch: 1/20, step: 53380, training_loss: 2.33736
Epoch: 1/20, step: 53400, training_loss: 1.70531
Epoch: 1/20, step: 53420, training_loss: 2.55622
Epoch: 1/20, step: 53440, training_loss: 2.58445
Epoch: 1/20, step: 53460, training_loss: 2.28055
Epoch: 1/20, step: 53480, training_loss: 2.46618
Epoch: 1/20, step: 53500, training_loss: 2.21171
Epoch: 1/20, step: 53520, training_loss: 2.59270
Epoch: 1/20, step: 53540, training_loss: 2.30489
Epoch: 1/20, step: 53560, training_loss: 2.44788
Epoch: 1/20, step: 53580, training_loss: 2.31971
Epoch: 1/20, step: 53600, training_loss: 2.33880
Epoch: 1/20, step: 53620, training_loss: 2.55488
Epoch: 1/20, step: 53640, training_loss: 2.22019
Epoch: 1/20, step: 53660, training_loss: 2.76484
Epoch: 1/20, step: 53680, training_loss: 2.89850
Epoch: 1/20, step: 53700, training_loss: 2.21809
Epoch: 1/20, step: 53720, training_loss: 1.78938
Epoch: 1/20, step: 53740, training_loss: 3.04637
Epoch: 1/20, step: 53760, training_loss: 2.33221
Epoch: 1/20, step: 53780, training_loss: 1.60068
Epoch: 1/20, step: 53800, training_loss: 2.11135
Epoch: 1/20, step: 53820, training_loss: 2.08115
Epoch: 1/20, step: 53840, training_loss: 1.76039
Epoch: 1/20, step: 53860, training_loss: 2.75550
Epoch: 1/20, step: 53880, training_loss: 1.73458
Epoch: 1/20, step: 53900, training_loss: 2.12683
Epoch: 1/20, step: 53920, training_loss: 1.69742
Epoch: 1/20, step: 53940, training_loss: 2.62406
Epoch: 1/20, step: 53960, training_loss: 2.19851
Epoch: 1/20, step: 53980, training_loss: 1.83242
Epoch: 1/20, step: 54000, training_loss: 1.57609
accuracy: 0.48, validation_loss: 1.9708894491195679, num_samples: 100
Epoch: 1/20, step: 54020, training_loss: 2.01784
Epoch: 1/20, step: 54040, training_loss: 2.08417
Epoch: 1/20, step: 54060, training_loss: 1.94318
Epoch: 1/20, step: 54080, training_loss: 1.69424
Epoch: 1/20, step: 54100, training_loss: 2.69561
Epoch: 1/20, step: 54120, training_loss: 2.06990
Epoch: 1/20, step: 54140, training_loss: 2.02418
Epoch: 1/20, step: 54160, training_loss: 2.33559
Epoch: 1/20, step: 54180, training_loss: 2.25385
Epoch: 1/20, step: 54200, training_loss: 2.08614
Epoch: 1/20, step: 54220, training_loss: 2.11907
Epoch: 1/20, step: 54240, training_loss: 2.30582
Epoch: 1/20, step: 54260, training_loss: 2.22727
Epoch: 1/20, step: 54280, training_loss: 1.97661
Epoch: 1/20, step: 54300, training_loss: 2.48038
Epoch: 1/20, step: 54320, training_loss: 2.48834
Epoch: 1/20, step: 54340, training_loss: 2.27393
Epoch: 1/20, step: 54360, training_loss: 2.01267
Epoch: 1/20, step: 54380, training_loss: 2.65381
Epoch: 1/20, step: 54400, training_loss: 1.78380
Epoch: 1/20, step: 54420, training_loss: 1.88354
Epoch: 1/20, step: 54440, training_loss: 2.16436
Epoch: 1/20, step: 54460, training_loss: 2.03769
Epoch: 1/20, step: 54480, training_loss: 3.09497
Epoch: 1/20, step: 54500, training_loss: 2.34045
Epoch: 1/20, step: 54520, training_loss: 2.82180
Epoch: 1/20, step: 54540, training_loss: 2.79063
Epoch: 1/20, step: 54560, training_loss: 2.89247
Epoch: 1/20, step: 54580, training_loss: 1.39410
Epoch: 1/20, step: 54600, training_loss: 2.22475
Epoch: 1/20, step: 54620, training_loss: 2.13711
Epoch: 1/20, step: 54640, training_loss: 2.32226
Epoch: 1/20, step: 54660, training_loss: 2.52425
Epoch: 1/20, step: 54680, training_loss: 2.80842
Epoch: 1/20, step: 54700, training_loss: 2.60487
Epoch: 1/20, step: 54720, training_loss: 1.88658
Epoch: 1/20, step: 54740, training_loss: 2.67759
Epoch: 1/20, step: 54760, training_loss: 2.84473
Epoch: 1/20, step: 54780, training_loss: 1.79131
Epoch: 1/20, step: 54800, training_loss: 1.59765
Epoch: 1/20, step: 54820, training_loss: 1.95066
Epoch: 1/20, step: 54840, training_loss: 2.40927
Epoch: 1/20, step: 54860, training_loss: 3.03285
Epoch: 1/20, step: 54880, training_loss: 1.91209
Epoch: 1/20, step: 54900, training_loss: 2.69668
Epoch: 1/20, step: 54920, training_loss: 1.88161
Epoch: 1/20, step: 54940, training_loss: 3.46818
Epoch: 1/20, step: 54960, training_loss: 1.80317
Epoch: 1/20, step: 54980, training_loss: 2.71021
Epoch: 1/20, step: 55000, training_loss: 2.00623
accuracy: 0.45, validation_loss: 2.2190158367156982, num_samples: 100
Epoch: 1/20, step: 55020, training_loss: 3.72807
Epoch: 1/20, step: 55040, training_loss: 2.44205
Epoch: 1/20, step: 55060, training_loss: 2.08257
Epoch: 1/20, step: 55080, training_loss: 2.78149
Epoch: 1/20, step: 55100, training_loss: 3.09827
Epoch: 1/20, step: 55120, training_loss: 3.05327
Epoch: 1/20, step: 55140, training_loss: 3.15215
Epoch: 1/20, step: 55160, training_loss: 3.32461
Epoch: 1/20, step: 55180, training_loss: 2.14944
Epoch: 1/20, step: 55200, training_loss: 2.56910
Epoch: 1/20, step: 55220, training_loss: 2.25355
Epoch: 1/20, step: 55240, training_loss: 2.82089
Epoch: 1/20, step: 55260, training_loss: 1.57334
Epoch: 1/20, step: 55280, training_loss: 1.85017
Epoch: 1/20, step: 55300, training_loss: 2.21763
Epoch: 1/20, step: 55320, training_loss: 2.55918
Epoch: 1/20, step: 55340, training_loss: 2.56611
Epoch: 1/20, step: 55360, training_loss: 2.55892
Epoch: 1/20, step: 55380, training_loss: 2.93176
Epoch: 1/20, step: 55400, training_loss: 2.12732
Epoch: 1/20, step: 55420, training_loss: 1.71223
Epoch: 1/20, step: 55440, training_loss: 2.77447
Epoch: 1/20, step: 55460, training_loss: 2.77791
Epoch: 1/20, step: 55480, training_loss: 2.48616
Epoch: 1/20, step: 55500, training_loss: 2.42131
Epoch: 1/20, step: 55520, training_loss: 1.56148
Epoch: 1/20, step: 55540, training_loss: 2.36963
Epoch: 1/20, step: 55560, training_loss: 2.76666
Epoch: 1/20, step: 55580, training_loss: 2.21058
Epoch: 1/20, step: 55600, training_loss: 2.71962
Epoch: 1/20, step: 55620, training_loss: 2.64437
Epoch: 1/20, step: 55640, training_loss: 2.47423
Epoch: 1/20, step: 55660, training_loss: 2.37522
Epoch: 1/20, step: 55680, training_loss: 1.97235
Epoch: 1/20, step: 55700, training_loss: 2.06730
Epoch: 1/20, step: 55720, training_loss: 2.43032
Epoch: 1/20, step: 55740, training_loss: 2.33794
Epoch: 1/20, step: 55760, training_loss: 3.45647
Epoch: 1/20, step: 55780, training_loss: 2.17044
Epoch: 1/20, step: 55800, training_loss: 2.10974
Epoch: 1/20, step: 55820, training_loss: 2.43724
Epoch: 1/20, step: 55840, training_loss: 2.03962
Epoch: 1/20, step: 55860, training_loss: 2.36348
Epoch: 1/20, step: 55880, training_loss: 2.76677
Epoch: 1/20, step: 55900, training_loss: 1.53559
Epoch: 1/20, step: 55920, training_loss: 1.58358
Epoch: 1/20, step: 55940, training_loss: 2.44128
Epoch: 1/20, step: 55960, training_loss: 2.93752
Epoch: 1/20, step: 55980, training_loss: 2.73219
Epoch: 1/20, step: 56000, training_loss: 2.95224
accuracy: 0.36, validation_loss: 2.4050333499908447, num_samples: 100
Epoch: 1/20, step: 56020, training_loss: 2.70308
Epoch: 1/20, step: 56040, training_loss: 2.85190
Epoch: 1/20, step: 56060, training_loss: 2.03921
Epoch: 1/20, step: 56080, training_loss: 2.33386
Epoch: 1/20, step: 56100, training_loss: 3.27249
Epoch: 1/20, step: 56120, training_loss: 2.38689
Epoch: 1/20, step: 56140, training_loss: 2.47293
Epoch: 1/20, step: 56160, training_loss: 1.82504
Epoch: 1/20, step: 56180, training_loss: 1.84212
Epoch: 1/20, step: 56200, training_loss: 3.15770
Epoch: 1/20, step: 56220, training_loss: 2.50084
Epoch: 1/20, step: 56240, training_loss: 2.27812
Epoch: 1/20, step: 56260, training_loss: 1.90808
Epoch: 1/20, step: 56280, training_loss: 2.64847
Epoch: 1/20, step: 56300, training_loss: 2.14976
Epoch: 1/20, step: 56320, training_loss: 1.63215
Epoch: 1/20, step: 56340, training_loss: 1.89625
Epoch: 1/20, step: 56360, training_loss: 2.26078
Epoch: 1/20, step: 56380, training_loss: 2.60373
Epoch: 1/20, step: 56400, training_loss: 2.33530
Epoch: 1/20, step: 56420, training_loss: 1.92731
Epoch: 1/20, step: 56440, training_loss: 1.52729
Epoch: 1/20, step: 56460, training_loss: 1.85245
Epoch: 1/20, step: 56480, training_loss: 2.02850
Epoch: 1/20, step: 56500, training_loss: 2.47816
Epoch: 1/20, step: 56520, training_loss: 2.90346
Epoch: 1/20, step: 56540, training_loss: 2.23419
Epoch: 1/20, step: 56560, training_loss: 2.63726
Epoch: 1/20, step: 56580, training_loss: 2.33467
Epoch: 1/20, step: 56600, training_loss: 2.34349
Epoch: 1/20, step: 56620, training_loss: 2.10880
Epoch: 1/20, step: 56640, training_loss: 2.57017
Epoch: 1/20, step: 56660, training_loss: 1.69759
Epoch: 1/20, step: 56680, training_loss: 2.33606
Epoch: 1/20, step: 56700, training_loss: 2.56988
Epoch: 1/20, step: 56720, training_loss: 3.22065
Epoch: 1/20, step: 56740, training_loss: 2.70815
Epoch: 1/20, step: 56760, training_loss: 1.78206
Epoch: 1/20, step: 56780, training_loss: 3.10152
Epoch: 1/20, step: 56800, training_loss: 3.06099
Epoch: 1/20, step: 56820, training_loss: 2.64854
Epoch: 1/20, step: 56840, training_loss: 2.45664
Epoch: 1/20, step: 56860, training_loss: 2.38356
Epoch: 1/20, step: 56880, training_loss: 2.39376
Epoch: 1/20, step: 56900, training_loss: 2.58415
Epoch: 1/20, step: 56920, training_loss: 2.75649
Epoch: 1/20, step: 56940, training_loss: 2.08704
Epoch: 1/20, step: 56960, training_loss: 2.42834
Epoch: 1/20, step: 56980, training_loss: 2.73364
Epoch: 1/20, step: 57000, training_loss: 2.00606
accuracy: 0.42, validation_loss: 2.442147731781006, num_samples: 100
Epoch: 1/20, step: 57020, training_loss: 2.76901
Epoch: 1/20, step: 57040, training_loss: 2.66451
Epoch: 1/20, step: 57060, training_loss: 2.67676
Epoch: 1/20, step: 57080, training_loss: 2.61314
Epoch: 1/20, step: 57100, training_loss: 2.14749
Epoch: 1/20, step: 57120, training_loss: 2.66072
Epoch: 1/20, step: 57140, training_loss: 1.85771
Epoch: 1/20, step: 57160, training_loss: 2.20038
Epoch: 1/20, step: 57180, training_loss: 2.28429
Epoch: 1/20, step: 57200, training_loss: 1.50365
Epoch: 1/20, step: 57220, training_loss: 2.59370
Epoch: 1/20, step: 57240, training_loss: 2.71083
Epoch: 1/20, step: 57260, training_loss: 1.96508
Epoch: 1/20, step: 57280, training_loss: 3.60784
Epoch: 1/20, step: 57300, training_loss: 2.46447
Epoch: 1/20, step: 57320, training_loss: 2.70101
Epoch: 1/20, step: 57340, training_loss: 2.06497
Epoch: 1/20, step: 57360, training_loss: 1.98221
Epoch: 1/20, step: 57380, training_loss: 2.18526
Epoch: 1/20, step: 57400, training_loss: 1.98572
Epoch: 1/20, step: 57420, training_loss: 2.77210
Epoch: 1/20, step: 57440, training_loss: 2.54894
Epoch: 1/20, step: 57460, training_loss: 2.56930
Epoch: 1/20, step: 57480, training_loss: 1.65517
Epoch: 1/20, step: 57500, training_loss: 2.71638
Epoch: 1/20, step: 57520, training_loss: 2.52687
Epoch: 1/20, step: 57540, training_loss: 2.90968
Epoch: 1/20, step: 57560, training_loss: 1.88941
Epoch: 1/20, step: 57580, training_loss: 1.50130
Epoch: 1/20, step: 57600, training_loss: 3.40678
Epoch: 1/20, step: 57620, training_loss: 2.15098
Epoch: 1/20, step: 57640, training_loss: 1.92933
Epoch: 1/20, step: 57660, training_loss: 2.89111
Epoch: 1/20, step: 57680, training_loss: 2.02970
Epoch: 1/20, step: 57700, training_loss: 1.89820
Epoch: 1/20, step: 57720, training_loss: 2.43871
Epoch: 1/20, step: 57740, training_loss: 2.69864
Epoch: 1/20, step: 57760, training_loss: 1.55030
Epoch: 1/20, step: 57780, training_loss: 1.86116
Epoch: 1/20, step: 57800, training_loss: 1.96279
Epoch: 1/20, step: 57820, training_loss: 2.88209
Epoch: 1/20, step: 57840, training_loss: 2.07973
Epoch: 1/20, step: 57860, training_loss: 2.36636
Epoch: 1/20, step: 57880, training_loss: 2.63929
Epoch: 1/20, step: 57900, training_loss: 2.26541
Epoch: 1/20, step: 57920, training_loss: 2.01121
Epoch: 1/20, step: 57940, training_loss: 1.35748
Epoch: 1/20, step: 57960, training_loss: 1.74784
Epoch: 1/20, step: 57980, training_loss: 2.33922
Epoch: 1/20, step: 58000, training_loss: 2.21127
accuracy: 0.42, validation_loss: 2.2014129161834717, num_samples: 100
Epoch: 1/20, step: 58020, training_loss: 2.46885
Epoch: 1/20, step: 58040, training_loss: 1.76635
Epoch: 1/20, step: 58060, training_loss: 1.95394
Epoch: 1/20, step: 58080, training_loss: 2.07373
Epoch: 1/20, step: 58100, training_loss: 1.66735
Epoch: 1/20, step: 58120, training_loss: 2.40286
Epoch: 1/20, step: 58140, training_loss: 2.61178
Epoch: 1/20, step: 58160, training_loss: 1.68173
Epoch: 1/20, step: 58180, training_loss: 2.72191
Epoch: 1/20, step: 58200, training_loss: 2.34591
Epoch: 1/20, step: 58220, training_loss: 2.72895
Epoch: 1/20, step: 58240, training_loss: 1.94916
Epoch: 1/20, step: 58260, training_loss: 1.56354
Epoch: 1/20, step: 58280, training_loss: 1.66387
Epoch: 1/20, step: 58300, training_loss: 2.63547
Epoch: 1/20, step: 58320, training_loss: 2.39561
Epoch: 1/20, step: 58340, training_loss: 2.88397
Epoch: 1/20, step: 58360, training_loss: 2.81580
Epoch: 1/20, step: 58380, training_loss: 2.27799
Epoch: 1/20, step: 58400, training_loss: 2.58816
Epoch: 1/20, step: 58420, training_loss: 2.32907
Epoch: 1/20, step: 58440, training_loss: 1.84423
Epoch: 1/20, step: 58460, training_loss: 3.17063
Epoch: 1/20, step: 58480, training_loss: 2.11192
Epoch: 1/20, step: 58500, training_loss: 2.37249
Epoch: 1/20, step: 58520, training_loss: 2.80259
Epoch: 1/20, step: 58540, training_loss: 2.92592
Epoch: 1/20, step: 58560, training_loss: 2.95277
Epoch: 1/20, step: 58580, training_loss: 2.22378
Epoch: 1/20, step: 58600, training_loss: 2.44514
Epoch: 1/20, step: 58620, training_loss: 2.86440
Epoch: 1/20, step: 58640, training_loss: 1.65180
Epoch: 1/20, step: 58660, training_loss: 2.50940
Epoch: 1/20, step: 58680, training_loss: 2.91991
Epoch: 1/20, step: 58700, training_loss: 2.74449
Epoch: 1/20, step: 58720, training_loss: 1.69621
Epoch: 1/20, step: 58740, training_loss: 2.07986
Epoch: 1/20, step: 58760, training_loss: 2.55279
Epoch: 1/20, step: 58780, training_loss: 2.01059
Epoch: 1/20, step: 58800, training_loss: 2.19739
Epoch: 1/20, step: 58820, training_loss: 1.96866
Epoch: 1/20, step: 58840, training_loss: 2.59513
Epoch: 1/20, step: 58860, training_loss: 2.21294
Epoch: 1/20, step: 58880, training_loss: 3.10111
Epoch: 1/20, step: 58900, training_loss: 2.15654
Epoch: 1/20, step: 58920, training_loss: 2.73512
Epoch: 1/20, step: 58940, training_loss: 3.30137
Epoch: 1/20, step: 58960, training_loss: 2.38449
Epoch: 1/20, step: 58980, training_loss: 2.76144
Epoch: 1/20, step: 59000, training_loss: 1.92752
accuracy: 0.37, validation_loss: 2.469116449356079, num_samples: 100
Epoch: 1/20, step: 59020, training_loss: 1.46706
Epoch: 1/20, step: 59040, training_loss: 2.25107
Epoch: 1/20, step: 59060, training_loss: 2.02928
Epoch: 1/20, step: 59080, training_loss: 1.97464
Epoch: 1/20, step: 59100, training_loss: 2.08415
Epoch: 1/20, step: 59120, training_loss: 2.07026
Epoch: 1/20, step: 59140, training_loss: 2.80219
Epoch: 1/20, step: 59160, training_loss: 1.68468
Epoch: 1/20, step: 59180, training_loss: 2.20326
Epoch: 1/20, step: 59200, training_loss: 2.25135
Epoch: 1/20, step: 59220, training_loss: 3.17857
Epoch: 1/20, step: 59240, training_loss: 2.57966
Epoch: 1/20, step: 59260, training_loss: 2.58000
Epoch: 1/20, step: 59280, training_loss: 2.37989
Epoch: 1/20, step: 59300, training_loss: 2.68958
Epoch: 1/20, step: 59320, training_loss: 1.83567
Epoch: 1/20, step: 59340, training_loss: 2.07039
Epoch: 1/20, step: 59360, training_loss: 2.55507
Epoch: 1/20, step: 59380, training_loss: 2.64969
Epoch: 1/20, step: 59400, training_loss: 1.88974
Epoch: 1/20, step: 59420, training_loss: 1.75433
Epoch: 1/20, step: 59440, training_loss: 2.21219
Epoch: 1/20, step: 59460, training_loss: 2.66281
Epoch: 1/20, step: 59480, training_loss: 2.92094
Epoch: 1/20, step: 59500, training_loss: 1.91297
Epoch: 1/20, step: 59520, training_loss: 2.58032
Epoch: 1/20, step: 59540, training_loss: 2.76012
Epoch: 1/20, step: 59560, training_loss: 1.60100
Epoch: 1/20, step: 59580, training_loss: 1.82171
Epoch: 1/20, step: 59600, training_loss: 1.75532
Epoch: 1/20, step: 59620, training_loss: 1.97218
Epoch: 1/20, step: 59640, training_loss: 2.21207
Epoch: 1/20, step: 59660, training_loss: 1.38381
Epoch: 1/20, step: 59680, training_loss: 2.24371
Epoch: 1/20, step: 59700, training_loss: 2.20994
Epoch: 1/20, step: 59720, training_loss: 2.73672
Epoch: 1/20, step: 59740, training_loss: 2.81403
Epoch: 1/20, step: 59760, training_loss: 1.27809
Epoch: 1/20, step: 59780, training_loss: 2.44498
Epoch: 1/20, step: 59800, training_loss: 2.61501
Epoch: 1/20, step: 59820, training_loss: 1.94265
Epoch: 1/20, step: 59840, training_loss: 2.07082
Epoch: 1/20, step: 59860, training_loss: 2.80407
Epoch: 1/20, step: 59880, training_loss: 2.54012
Epoch: 1/20, step: 59900, training_loss: 1.96807
Epoch: 1/20, step: 59920, training_loss: 1.75598
Epoch: 1/20, step: 59940, training_loss: 1.94611
Epoch: 1/20, step: 59960, training_loss: 1.71555
Epoch: 1/20, step: 59980, training_loss: 2.42050
Epoch: 1/20, step: 60000, training_loss: 1.94164
accuracy: 0.44, validation_loss: 2.015819787979126, num_samples: 100
Epoch: 1/20, step: 60020, training_loss: 1.86877
Epoch: 1/20, step: 60040, training_loss: 3.09247
Epoch: 1/20, step: 60060, training_loss: 3.21036
Epoch: 1/20, step: 60080, training_loss: 2.66726
Epoch: 1/20, step: 60100, training_loss: 2.37665
Epoch: 1/20, step: 60120, training_loss: 2.07530
Epoch: 1/20, step: 60140, training_loss: 2.79624
Epoch: 1/20, step: 60160, training_loss: 2.87713
Epoch: 1/20, step: 60180, training_loss: 2.32988
Epoch: 1/20, step: 60200, training_loss: 2.17538
Epoch: 1/20, step: 60220, training_loss: 2.08480
Epoch: 1/20, step: 60240, training_loss: 2.87424
Epoch: 1/20, step: 60260, training_loss: 2.30729
Epoch: 1/20, step: 60280, training_loss: 2.10079
Epoch: 1/20, step: 60300, training_loss: 2.86477
Epoch: 1/20, step: 60320, training_loss: 2.22682
Epoch: 1/20, step: 60340, training_loss: 2.53296
Epoch: 1/20, step: 60360, training_loss: 2.73641
Epoch: 1/20, step: 60380, training_loss: 1.89045
Epoch: 1/20, step: 60400, training_loss: 2.19928
Epoch: 1/20, step: 60420, training_loss: 2.86396
Epoch: 1/20, step: 60440, training_loss: 2.34491
Epoch: 1/20, step: 60460, training_loss: 2.36171
Epoch: 1/20, step: 60480, training_loss: 2.09264
Epoch: 1/20, step: 60500, training_loss: 1.80720
Epoch: 1/20, step: 60520, training_loss: 2.95979
Epoch: 1/20, step: 60540, training_loss: 2.21191
Epoch: 1/20, step: 60560, training_loss: 1.91616
Epoch: 1/20, step: 60580, training_loss: 1.83497
Epoch: 1/20, step: 60600, training_loss: 2.67787
Epoch: 1/20, step: 60620, training_loss: 2.88968
Epoch: 1/20, step: 60640, training_loss: 2.39164
Epoch: 1/20, step: 60660, training_loss: 1.71816
Epoch: 1/20, step: 60680, training_loss: 1.66581
Epoch: 1/20, step: 60700, training_loss: 1.69906
Epoch: 1/20, step: 60720, training_loss: 2.48191
Epoch: 1/20, step: 60740, training_loss: 1.90613
Epoch: 1/20, step: 60760, training_loss: 2.33129
Epoch: 1/20, step: 60780, training_loss: 2.60470
Epoch: 1/20, step: 60800, training_loss: 2.09530
Epoch: 1/20, step: 60820, training_loss: 2.41005
Epoch: 1/20, step: 60840, training_loss: 3.27871
Epoch: 1/20, step: 60860, training_loss: 3.00749
Epoch: 1/20, step: 60880, training_loss: 1.86775
Epoch: 1/20, step: 60900, training_loss: 2.07804
Epoch: 1/20, step: 60920, training_loss: 2.24254
Epoch: 1/20, step: 60940, training_loss: 2.20894
Epoch: 1/20, step: 60960, training_loss: 2.79194
Epoch: 1/20, step: 60980, training_loss: 1.69120
Epoch: 1/20, step: 61000, training_loss: 1.96118
accuracy: 0.47, validation_loss: 1.9789612293243408, num_samples: 100
Epoch: 1/20, step: 61020, training_loss: 2.20494
Epoch: 1/20, step: 61040, training_loss: 2.26451
Epoch: 1/20, step: 61060, training_loss: 1.86610
Epoch: 1/20, step: 61080, training_loss: 2.57611
Epoch: 1/20, step: 61100, training_loss: 2.03395
Epoch: 1/20, step: 61120, training_loss: 1.92602
Epoch: 1/20, step: 61140, training_loss: 1.59767
Epoch: 1/20, step: 61160, training_loss: 2.88911
Epoch: 1/20, step: 61180, training_loss: 1.93457
Epoch: 1/20, step: 61200, training_loss: 2.21300
Epoch: 1/20, step: 61220, training_loss: 2.23124
Epoch: 1/20, step: 61240, training_loss: 3.05608
Epoch: 1/20, step: 61260, training_loss: 2.01701
Epoch: 1/20, step: 61280, training_loss: 3.04584
Epoch: 1/20, step: 61300, training_loss: 1.95497
Epoch: 1/20, step: 61320, training_loss: 2.69815
Epoch: 1/20, step: 61340, training_loss: 2.71990
Epoch: 1/20, step: 61360, training_loss: 1.48105
Epoch: 1/20, step: 61380, training_loss: 2.21916
Epoch: 1/20, step: 61400, training_loss: 1.79921
Epoch: 1/20, step: 61420, training_loss: 2.06862
Epoch: 1/20, step: 61440, training_loss: 2.44683
Epoch: 1/20, step: 61460, training_loss: 2.47812
Epoch: 1/20, step: 61480, training_loss: 1.42558
Epoch: 1/20, step: 61500, training_loss: 2.91012
Epoch: 1/20, step: 61520, training_loss: 1.57030
Epoch: 1/20, step: 61540, training_loss: 1.92757
Epoch: 1/20, step: 61560, training_loss: 2.57538
Epoch: 1/20, step: 61580, training_loss: 2.46357
Epoch: 1/20, step: 61600, training_loss: 2.66551
Epoch: 1/20, step: 61620, training_loss: 2.08732
Epoch: 1/20, step: 61640, training_loss: 1.91492
Epoch: 1/20, step: 61660, training_loss: 2.38980
Epoch: 1/20, step: 61680, training_loss: 2.87472
Epoch: 1/20, step: 61700, training_loss: 1.69461
Epoch: 1/20, step: 61720, training_loss: 2.86949
Epoch: 1/20, step: 61740, training_loss: 3.37734
Epoch: 1/20, step: 61760, training_loss: 2.19159
Epoch: 1/20, step: 61780, training_loss: 2.04874
Epoch: 1/20, step: 61800, training_loss: 2.18339
Epoch: 1/20, step: 61820, training_loss: 2.63059
Epoch: 1/20, step: 61840, training_loss: 2.92804
Epoch: 1/20, step: 61860, training_loss: 2.20956
Epoch: 1/20, step: 61880, training_loss: 1.96873
Epoch: 1/20, step: 61900, training_loss: 1.79271
Epoch: 1/20, step: 61920, training_loss: 2.68258
Epoch: 1/20, step: 61940, training_loss: 2.15859
Epoch: 1/20, step: 61960, training_loss: 3.16526
Epoch: 1/20, step: 61980, training_loss: 2.50791
Epoch: 1/20, step: 62000, training_loss: 1.99511
accuracy: 0.34, validation_loss: 2.501574754714966, num_samples: 100
Epoch: 1/20, step: 62020, training_loss: 1.36353
Epoch: 1/20, step: 62040, training_loss: 2.27117
Epoch: 1/20, step: 62060, training_loss: 2.80977
Epoch: 1/20, step: 62080, training_loss: 2.40946
Epoch: 1/20, step: 62100, training_loss: 2.41446
Epoch: 1/20, step: 62120, training_loss: 3.36461
Epoch: 1/20, step: 62140, training_loss: 1.98917
Epoch: 1/20, step: 62160, training_loss: 2.32228
Epoch: 1/20, step: 62180, training_loss: 2.02681
Epoch: 1/20, step: 62200, training_loss: 1.78521
Epoch: 1/20, step: 62220, training_loss: 1.80631
Epoch: 1/20, step: 62240, training_loss: 1.92550
Epoch: 1/20, step: 62260, training_loss: 2.92133
Epoch: 1/20, step: 62280, training_loss: 2.19098
Epoch: 1/20, step: 62300, training_loss: 2.49915
Epoch: 1/20, step: 62320, training_loss: 1.53820
Epoch: 1/20, step: 62340, training_loss: 2.45484
Epoch: 1/20, step: 62360, training_loss: 2.92149
Epoch: 1/20, step: 62380, training_loss: 2.46321
Epoch: 1/20, step: 62400, training_loss: 2.58369
Epoch: 1/20, step: 62420, training_loss: 2.52101
Epoch: 1/20, step: 62440, training_loss: 1.92881
Epoch: 1/20, step: 62460, training_loss: 2.41388
Epoch: 1/20, step: 62480, training_loss: 1.95337
Epoch: 1/20, step: 62500, training_loss: 2.60072
Epoch: 1/20, step: 62520, training_loss: 2.11789
Epoch: 1/20, step: 62540, training_loss: 3.32147
Epoch: 1/20, step: 62560, training_loss: 2.84342
Epoch: 1/20, step: 62580, training_loss: 2.70456
Epoch: 1/20, step: 62600, training_loss: 2.86334
Epoch: 1/20, step: 62620, training_loss: 1.44787
Epoch: 1/20, step: 62640, training_loss: 2.32801
Epoch: 1/20, step: 62660, training_loss: 1.36420
Epoch: 1/20, step: 62680, training_loss: 2.54998
Epoch: 1/20, step: 62700, training_loss: 2.07662
Epoch: 1/20, step: 62720, training_loss: 2.25720
Epoch: 1/20, step: 62740, training_loss: 2.64324
Epoch: 1/20, step: 62760, training_loss: 2.69463
Epoch: 1/20, step: 62780, training_loss: 2.24783
Epoch: 1/20, step: 62800, training_loss: 2.68203
Epoch: 1/20, step: 62820, training_loss: 2.70215
Epoch: 1/20, step: 62840, training_loss: 2.62970
Epoch: 1/20, step: 62860, training_loss: 2.30373
Epoch: 1/20, step: 62880, training_loss: 2.52122
Epoch: 1/20, step: 62900, training_loss: 2.03593
Epoch: 1/20, step: 62920, training_loss: 2.58984
Epoch: 1/20, step: 62940, training_loss: 3.00827
Epoch: 1/20, step: 62960, training_loss: 1.37388
Epoch: 1/20, step: 62980, training_loss: 2.53664
Epoch: 1/20, step: 63000, training_loss: 1.78465
accuracy: 0.4, validation_loss: 2.1069533824920654, num_samples: 100
Epoch: 1/20, step: 63020, training_loss: 2.17816
Epoch: 1/20, step: 63040, training_loss: 1.47730
Epoch: 1/20, step: 63060, training_loss: 1.88365
Epoch: 1/20, step: 63080, training_loss: 2.67964
Epoch: 1/20, step: 63100, training_loss: 2.49613
Epoch: 1/20, step: 63120, training_loss: 2.41587
Epoch: 1/20, step: 63140, training_loss: 2.29256
Epoch: 1/20, step: 63160, training_loss: 1.94478
Epoch: 1/20, step: 63180, training_loss: 2.38228
Epoch: 1/20, step: 63200, training_loss: 2.54935
Epoch: 1/20, step: 63220, training_loss: 1.67988
Epoch: 1/20, step: 63240, training_loss: 1.95265
Epoch: 1/20, step: 63260, training_loss: 1.53300
Epoch: 1/20, step: 63280, training_loss: 2.75125
Epoch: 1/20, step: 63300, training_loss: 2.29185
Epoch: 1/20, step: 63320, training_loss: 2.37184
Epoch: 1/20, step: 63340, training_loss: 1.81331
Epoch: 1/20, step: 63360, training_loss: 2.61724
Epoch: 1/20, step: 63380, training_loss: 1.79192
Epoch: 1/20, step: 63400, training_loss: 2.61031
Epoch: 1/20, step: 63420, training_loss: 2.32241
Epoch: 1/20, step: 63440, training_loss: 2.23471
Epoch: 1/20, step: 63460, training_loss: 2.07871
Epoch: 1/20, step: 63480, training_loss: 2.22834
Epoch: 1/20, step: 63500, training_loss: 2.47956
Epoch: 1/20, step: 63520, training_loss: 2.99927
Epoch: 1/20, step: 63540, training_loss: 2.81719
Epoch: 1/20, step: 63560, training_loss: 1.26540
Epoch: 1/20, step: 63580, training_loss: 1.95847
Epoch: 1/20, step: 63600, training_loss: 2.65977
Epoch: 1/20, step: 63620, training_loss: 2.28167
Epoch: 1/20, step: 63640, training_loss: 2.22358
Epoch: 1/20, step: 63660, training_loss: 2.02423
Epoch: 1/20, step: 63680, training_loss: 1.89092
Epoch: 1/20, step: 63700, training_loss: 3.05487
Epoch: 1/20, step: 63720, training_loss: 1.49942
Epoch: 1/20, step: 63740, training_loss: 1.96276
Epoch: 1/20, step: 63760, training_loss: 2.52319
Epoch: 1/20, step: 63780, training_loss: 3.10434
Epoch: 1/20, step: 63800, training_loss: 2.71096
Epoch: 1/20, step: 63820, training_loss: 2.55915
Epoch: 1/20, step: 63840, training_loss: 1.57223
Epoch: 1/20, step: 63860, training_loss: 2.93983
Epoch: 1/20, step: 63880, training_loss: 2.10402
Epoch: 1/20, step: 63900, training_loss: 2.03802
Epoch: 1/20, step: 63920, training_loss: 2.06578
Epoch: 1/20, step: 63940, training_loss: 1.85096
Epoch: 1/20, step: 63960, training_loss: 1.71985
Epoch: 1/20, step: 63980, training_loss: 2.89428
Epoch: 1/20, step: 64000, training_loss: 1.59884
accuracy: 0.44, validation_loss: 2.3085525035858154, num_samples: 100
Epoch: 1/20, step: 64020, training_loss: 1.96518
Epoch: 1/20, step: 64040, training_loss: 1.96005
Epoch: 1/20, step: 64060, training_loss: 2.11071
Epoch: 1/20, step: 64080, training_loss: 2.19148
Epoch: 1/20, step: 64100, training_loss: 2.34391
Epoch: 1/20, step: 64120, training_loss: 2.48049
Epoch: 1/20, step: 64140, training_loss: 2.33830
Epoch: 1/20, step: 64160, training_loss: 2.38823
Epoch: 1/20, step: 64180, training_loss: 3.04320
Epoch: 1/20, step: 64200, training_loss: 2.19170
Epoch: 1/20, step: 64220, training_loss: 1.51243
Epoch: 1/20, step: 64240, training_loss: 1.84610
Epoch: 1/20, step: 64260, training_loss: 2.50816
Epoch: 1/20, step: 64280, training_loss: 2.39890
Epoch: 1/20, step: 64300, training_loss: 2.27051
Epoch: 1/20, step: 64320, training_loss: 1.46188
Epoch: 1/20, step: 64340, training_loss: 2.97487
Epoch: 1/20, step: 64360, training_loss: 1.78469
Epoch: 1/20, step: 64380, training_loss: 2.18193
Epoch: 1/20, step: 64400, training_loss: 1.83381
Epoch: 1/20, step: 64420, training_loss: 1.72528
Epoch: 1/20, step: 64440, training_loss: 3.27939
Epoch: 1/20, step: 64460, training_loss: 1.97264
Epoch: 1/20, step: 64480, training_loss: 2.78506
Epoch: 1/20, step: 64500, training_loss: 2.23779
Epoch: 1/20, step: 64520, training_loss: 1.96958
Epoch: 1/20, step: 64540, training_loss: 2.18081
Epoch: 1/20, step: 64560, training_loss: 2.41274
Epoch: 1/20, step: 64580, training_loss: 1.45266
Epoch: 1/20, step: 64600, training_loss: 2.24467
Epoch: 1/20, step: 64620, training_loss: 1.98521
Epoch: 1/20, step: 64640, training_loss: 2.69167
Epoch: 1/20, step: 64660, training_loss: 2.12236
Epoch: 1/20, step: 64680, training_loss: 1.87263
Epoch: 1/20, step: 64700, training_loss: 2.14516
Epoch: 1/20, step: 64720, training_loss: 2.86991
Epoch: 1/20, step: 64740, training_loss: 2.62085
Epoch: 1/20, step: 64760, training_loss: 2.56906
Epoch: 1/20, step: 64780, training_loss: 2.51893
Epoch: 1/20, step: 64800, training_loss: 1.56770
Epoch: 1/20, step: 64820, training_loss: 2.16845
Epoch: 1/20, step: 64840, training_loss: 1.89993
Epoch: 1/20, step: 64860, training_loss: 2.20730
Epoch: 1/20, step: 64880, training_loss: 2.19676
Epoch: 1/20, step: 64900, training_loss: 1.80330
Epoch: 1/20, step: 64920, training_loss: 1.76148
Epoch: 1/20, step: 64940, training_loss: 2.77354
Epoch: 1/20, step: 64960, training_loss: 2.23642
Epoch: 1/20, step: 64980, training_loss: 2.13483
Epoch: 1/20, step: 65000, training_loss: 2.27930
accuracy: 0.39, validation_loss: 2.0124356746673584, num_samples: 100
Epoch: 1/20, step: 65020, training_loss: 2.27612
Epoch: 1/20, step: 65040, training_loss: 2.99869
Epoch: 1/20, step: 65060, training_loss: 2.36803
Epoch: 1/20, step: 65080, training_loss: 2.37096
Epoch: 1/20, step: 65100, training_loss: 2.55799
Epoch: 1/20, step: 65120, training_loss: 2.11435
Epoch: 1/20, step: 65140, training_loss: 1.97521
Epoch: 1/20, step: 65160, training_loss: 2.29961
Epoch: 1/20, step: 65180, training_loss: 2.79293
Epoch: 1/20, step: 65200, training_loss: 2.66089
Epoch: 1/20, step: 65220, training_loss: 1.86210
Epoch: 1/20, step: 65240, training_loss: 2.29963
Epoch: 1/20, step: 65260, training_loss: 1.94903
Epoch: 1/20, step: 65280, training_loss: 2.12038
Epoch: 1/20, step: 65300, training_loss: 2.48199
Epoch: 1/20, step: 65320, training_loss: 2.56842
Epoch: 1/20, step: 65340, training_loss: 2.80934
Epoch: 1/20, step: 65360, training_loss: 2.33711
Epoch: 1/20, step: 65380, training_loss: 1.79306
Epoch: 1/20, step: 65400, training_loss: 2.98458
Epoch: 1/20, step: 65420, training_loss: 2.03283
Epoch: 1/20, step: 65440, training_loss: 3.10470
Epoch: 1/20, step: 65460, training_loss: 1.64919
Epoch: 1/20, step: 65480, training_loss: 2.25034
Epoch: 1/20, step: 65500, training_loss: 2.67039
Epoch: 1/20, step: 65520, training_loss: 2.65178
Epoch: 1/20, step: 65540, training_loss: 2.92769
Epoch: 1/20, step: 65560, training_loss: 2.65575
Epoch: 1/20, step: 65580, training_loss: 2.32441
Epoch: 1/20, step: 65600, training_loss: 1.84290
Epoch: 1/20, step: 65620, training_loss: 2.31556
Epoch: 1/20, step: 65640, training_loss: 2.73627
Epoch: 1/20, step: 65660, training_loss: 2.34103
Epoch: 1/20, step: 65680, training_loss: 3.06048
Epoch: 1/20, step: 65700, training_loss: 2.42587
Epoch: 1/20, step: 65720, training_loss: 1.65623
Epoch: 1/20, step: 65740, training_loss: 2.60961
Epoch: 1/20, step: 65760, training_loss: 2.41166
Epoch: 1/20, step: 65780, training_loss: 2.78842
Epoch: 1/20, step: 65800, training_loss: 1.93440
Epoch: 1/20, step: 65820, training_loss: 1.87041
Epoch: 1/20, step: 65840, training_loss: 1.52267
Epoch: 1/20, step: 65860, training_loss: 2.51421
Epoch: 1/20, step: 65880, training_loss: 2.45410
Epoch: 1/20, step: 65900, training_loss: 2.22357
Epoch: 1/20, step: 65920, training_loss: 1.86417
Epoch: 1/20, step: 65940, training_loss: 1.90717
Epoch: 1/20, step: 65960, training_loss: 2.81017
Epoch: 1/20, step: 65980, training_loss: 2.13877
Epoch: 1/20, step: 66000, training_loss: 1.63818
accuracy: 0.37, validation_loss: 2.2496442794799805, num_samples: 100
Epoch: 1/20, step: 66020, training_loss: 2.38898
Epoch: 1/20, step: 66040, training_loss: 1.98696
Epoch: 1/20, step: 66060, training_loss: 2.42851
Epoch: 1/20, step: 66080, training_loss: 2.21462
Epoch: 1/20, step: 66100, training_loss: 1.76044
Epoch: 1/20, step: 66120, training_loss: 1.61184
Epoch: 1/20, step: 66140, training_loss: 3.31861
Epoch: 1/20, step: 66160, training_loss: 2.39359
Epoch: 1/20, step: 66180, training_loss: 2.79308
Epoch: 1/20, step: 66200, training_loss: 2.54115
Epoch: 1/20, step: 66220, training_loss: 2.65924
Epoch: 1/20, step: 66240, training_loss: 1.95751
Epoch: 1/20, step: 66260, training_loss: 1.01770
Epoch: 1/20, step: 66280, training_loss: 1.72205
Epoch: 1/20, step: 66300, training_loss: 3.00726
Epoch: 1/20, step: 66320, training_loss: 2.74710
Epoch: 1/20, step: 66340, training_loss: 1.75693
Epoch: 1/20, step: 66360, training_loss: 2.70222
Epoch: 1/20, step: 66380, training_loss: 2.29115
Epoch: 1/20, step: 66400, training_loss: 2.56440
Epoch: 1/20, step: 66420, training_loss: 2.60873
Epoch: 1/20, step: 66440, training_loss: 2.66423
Epoch: 1/20, step: 66460, training_loss: 1.74894
Epoch: 1/20, step: 66480, training_loss: 2.19321
Epoch: 1/20, step: 66500, training_loss: 1.96841
Epoch: 1/20, step: 66520, training_loss: 2.01004
Epoch: 1/20, step: 66540, training_loss: 3.12106
Epoch: 1/20, step: 66560, training_loss: 2.32277
Epoch: 1/20, step: 66580, training_loss: 2.20791
Epoch: 1/20, step: 66600, training_loss: 2.78496
Epoch: 1/20, step: 66620, training_loss: 2.44549
Epoch: 1/20, step: 66640, training_loss: 1.59642
Epoch: 1/20, step: 66660, training_loss: 2.45266
Epoch: 1/20, step: 66680, training_loss: 2.97447
Epoch: 1/20, step: 66700, training_loss: 2.32849
Epoch: 1/20, step: 66720, training_loss: 2.50085
Epoch: 1/20, step: 66740, training_loss: 2.33033
Epoch: 1/20, step: 66760, training_loss: 3.23087
Epoch: 1/20, step: 66780, training_loss: 1.86259
Epoch: 1/20, step: 66800, training_loss: 2.32043
Epoch: 1/20, step: 66820, training_loss: 3.35445
Epoch: 1/20, step: 66840, training_loss: 2.44192
Epoch: 1/20, step: 66860, training_loss: 1.72523
Epoch: 1/20, step: 66880, training_loss: 3.26346
Epoch: 1/20, step: 66900, training_loss: 2.23804
Epoch: 1/20, step: 66920, training_loss: 2.53744
Epoch: 1/20, step: 66940, training_loss: 2.16979
Epoch: 1/20, step: 66960, training_loss: 1.70395
Epoch: 1/20, step: 66980, training_loss: 2.06993
Epoch: 1/20, step: 67000, training_loss: 2.25553
accuracy: 0.42, validation_loss: 2.3943731784820557, num_samples: 100
Epoch: 1/20, step: 67020, training_loss: 2.70856
Epoch: 1/20, step: 67040, training_loss: 2.12527
Epoch: 1/20, step: 67060, training_loss: 2.42978
Epoch: 1/20, step: 67080, training_loss: 3.05062
Epoch: 1/20, step: 67100, training_loss: 1.93683
Epoch: 1/20, step: 67120, training_loss: 2.15435
Epoch: 1/20, step: 67140, training_loss: 3.06045
Epoch: 1/20, step: 67160, training_loss: 1.90848
Epoch: 1/20, step: 67180, training_loss: 2.32225
Epoch: 1/20, step: 67200, training_loss: 2.54429
Epoch: 1/20, step: 67220, training_loss: 1.76779
Epoch: 1/20, step: 67240, training_loss: 1.85679
Epoch: 1/20, step: 67260, training_loss: 2.22587
Epoch: 1/20, step: 67280, training_loss: 1.56622
Epoch: 1/20, step: 67300, training_loss: 1.80283
Epoch: 1/20, step: 67320, training_loss: 3.11641
Epoch: 1/20, step: 67340, training_loss: 1.94921
Epoch: 1/20, step: 67360, training_loss: 2.14285
Epoch: 1/20, step: 67380, training_loss: 2.62649
Epoch: 1/20, step: 67400, training_loss: 1.98079
Epoch: 1/20, step: 67420, training_loss: 1.46543
Epoch: 1/20, step: 67440, training_loss: 3.08729
Epoch: 1/20, step: 67460, training_loss: 1.96929
Epoch: 1/20, step: 67480, training_loss: 1.72965
Epoch: 1/20, step: 67500, training_loss: 3.17924
Epoch: 1/20, step: 67520, training_loss: 3.19668
Epoch: 1/20, step: 67540, training_loss: 1.53988
Epoch: 1/20, step: 67560, training_loss: 1.76247
Epoch: 1/20, step: 67580, training_loss: 3.25662
Epoch: 1/20, step: 67600, training_loss: 2.02252
Epoch: 1/20, step: 67620, training_loss: 1.86813
Epoch: 1/20, step: 67640, training_loss: 2.36544
Epoch: 1/20, step: 67660, training_loss: 2.01089
Epoch: 1/20, step: 67680, training_loss: 2.61943
Epoch: 1/20, step: 67700, training_loss: 2.68724
Epoch: 1/20, step: 67720, training_loss: 2.62673
Epoch: 1/20, step: 67740, training_loss: 2.28752
Epoch: 1/20, step: 67760, training_loss: 1.93202
Epoch: 1/20, step: 67780, training_loss: 2.10883
Epoch: 1/20, step: 67800, training_loss: 3.04185
Epoch: 1/20, step: 67820, training_loss: 3.11347
Epoch: 1/20, step: 67840, training_loss: 2.30120
Epoch: 1/20, step: 67860, training_loss: 2.03113
Epoch: 1/20, step: 67880, training_loss: 2.12635
Epoch: 1/20, step: 67900, training_loss: 1.72134
Epoch: 1/20, step: 67920, training_loss: 2.18861
Epoch: 1/20, step: 67940, training_loss: 3.04426
Epoch: 1/20, step: 67960, training_loss: 2.44079
Epoch: 1/20, step: 67980, training_loss: 2.01396
Epoch: 1/20, step: 68000, training_loss: 1.92371
accuracy: 0.44, validation_loss: 2.1179263591766357, num_samples: 100
Epoch: 1/20, step: 68020, training_loss: 2.02475
Epoch: 1/20, step: 68040, training_loss: 1.85620
Epoch: 1/20, step: 68060, training_loss: 2.09919
Epoch: 1/20, step: 68080, training_loss: 2.76209
Epoch: 1/20, step: 68100, training_loss: 2.46000
Epoch: 1/20, step: 68120, training_loss: 1.66068
Epoch: 1/20, step: 68140, training_loss: 1.82932
Epoch: 1/20, step: 68160, training_loss: 1.49683
Epoch: 1/20, step: 68180, training_loss: 1.76128
Epoch: 1/20, step: 68200, training_loss: 2.65771
Epoch: 1/20, step: 68220, training_loss: 2.96606
Epoch: 1/20, step: 68240, training_loss: 2.19541
Epoch: 1/20, step: 68260, training_loss: 2.33234
Epoch: 1/20, step: 68280, training_loss: 2.45826
Epoch: 1/20, step: 68300, training_loss: 2.22324
Epoch: 1/20, step: 68320, training_loss: 2.40027
Epoch: 1/20, step: 68340, training_loss: 3.07022
Epoch: 1/20, step: 68360, training_loss: 2.33080
Epoch: 1/20, step: 68380, training_loss: 2.13420
Epoch: 1/20, step: 68400, training_loss: 1.83253
Epoch: 1/20, step: 68420, training_loss: 2.02405
Epoch: 1/20, step: 68440, training_loss: 2.41629
Epoch: 1/20, step: 68460, training_loss: 2.25724
Epoch: 1/20, step: 68480, training_loss: 3.07408
Epoch: 1/20, step: 68500, training_loss: 2.31276
Epoch: 1/20, step: 68520, training_loss: 3.08983
Epoch: 1/20, step: 68540, training_loss: 3.00911
Epoch: 1/20, step: 68560, training_loss: 1.68810
Epoch: 1/20, step: 68580, training_loss: 3.20805
Epoch: 1/20, step: 68600, training_loss: 1.85344
Epoch: 1/20, step: 68620, training_loss: 1.32153
Epoch: 1/20, step: 68640, training_loss: 1.88548
Epoch: 1/20, step: 68660, training_loss: 1.42541
Epoch: 1/20, step: 68680, training_loss: 1.32943
Epoch: 1/20, step: 68700, training_loss: 1.89945
Epoch: 1/20, step: 68720, training_loss: 2.00192
Epoch: 1/20, step: 68740, training_loss: 1.08239
Epoch: 1/20, step: 68760, training_loss: 2.16191
Epoch: 1/20, step: 68780, training_loss: 2.10015
Epoch: 1/20, step: 68800, training_loss: 1.35799
Epoch: 1/20, step: 68820, training_loss: 1.98594
Epoch: 1/20, step: 68840, training_loss: 1.96580
Epoch: 1/20, step: 68860, training_loss: 1.71417
Epoch: 1/20, step: 68880, training_loss: 2.60576
Epoch: 1/20, step: 68900, training_loss: 1.79421
Epoch: 1/20, step: 68920, training_loss: 2.08944
Epoch: 1/20, step: 68940, training_loss: 2.19321
Epoch: 1/20, step: 68960, training_loss: 2.17836
Epoch: 1/20, step: 68980, training_loss: 2.67386
Epoch: 1/20, step: 69000, training_loss: 1.83666
accuracy: 0.34, validation_loss: 2.500699520111084, num_samples: 100
Epoch: 1/20, step: 69020, training_loss: 1.80272
Epoch: 1/20, step: 69040, training_loss: 3.33022
Epoch: 1/20, step: 69060, training_loss: 1.95973
Epoch: 1/20, step: 69080, training_loss: 2.93076
Epoch: 1/20, step: 69100, training_loss: 1.49804
Epoch: 1/20, step: 69120, training_loss: 2.94673
Epoch: 1/20, step: 69140, training_loss: 3.32261
Epoch: 1/20, step: 69160, training_loss: 1.79034
Epoch: 1/20, step: 69180, training_loss: 2.13213
Epoch: 1/20, step: 69200, training_loss: 1.83500
Epoch: 1/20, step: 69220, training_loss: 2.66008
Epoch: 1/20, step: 69240, training_loss: 1.92408
Epoch: 1/20, step: 69260, training_loss: 2.31940
Epoch: 1/20, step: 69280, training_loss: 2.35113
Epoch: 1/20, step: 69300, training_loss: 2.19104
Epoch: 1/20, step: 69320, training_loss: 1.96089
Epoch: 1/20, step: 69340, training_loss: 1.87292
Epoch: 1/20, step: 69360, training_loss: 2.43395
Epoch: 1/20, step: 69380, training_loss: 2.21879
Epoch: 1/20, step: 69400, training_loss: 2.65569
Epoch: 1/20, step: 69420, training_loss: 2.26759
Epoch: 1/20, step: 69440, training_loss: 2.48629
Epoch: 1/20, step: 69460, training_loss: 1.40573
Epoch: 1/20, step: 69480, training_loss: 2.15257
Epoch: 1/20, step: 69500, training_loss: 3.13090
Epoch: 1/20, step: 69520, training_loss: 2.79006
Epoch: 1/20, step: 69540, training_loss: 1.90846
Epoch: 1/20, step: 69560, training_loss: 2.01033
Epoch: 1/20, step: 69580, training_loss: 1.94816
Epoch: 1/20, step: 69600, training_loss: 1.87751
Epoch: 1/20, step: 69620, training_loss: 2.36896
Epoch: 1/20, step: 69640, training_loss: 1.64841
Epoch: 1/20, step: 69660, training_loss: 2.47326
Epoch: 1/20, step: 69680, training_loss: 2.60059
Epoch: 1/20, step: 69700, training_loss: 1.24565
Epoch: 1/20, step: 69720, training_loss: 2.14576
Epoch: 1/20, step: 69740, training_loss: 1.55967
Epoch: 1/20, step: 69760, training_loss: 2.40435
Epoch: 1/20, step: 69780, training_loss: 2.18801
Epoch: 1/20, step: 69800, training_loss: 2.66721
Epoch: 1/20, step: 69820, training_loss: 2.50274
Epoch: 1/20, step: 69840, training_loss: 3.24237
Epoch: 1/20, step: 69860, training_loss: 1.60891
Epoch: 1/20, step: 69880, training_loss: 2.11690
Epoch: 1/20, step: 69900, training_loss: 1.98856
Epoch: 1/20, step: 69920, training_loss: 2.49993
Epoch: 1/20, step: 69940, training_loss: 1.67244
Epoch: 1/20, step: 69960, training_loss: 1.67464
Epoch: 1/20, step: 69980, training_loss: 1.96040
Epoch: 1/20, step: 70000, training_loss: 2.66840
accuracy: 0.32, validation_loss: 2.1997733116149902, num_samples: 100
Epoch: 1/20, step: 70020, training_loss: 2.58904
Epoch: 1/20, step: 70040, training_loss: 2.31920
Epoch: 1/20, step: 70060, training_loss: 2.51077
Epoch: 1/20, step: 70080, training_loss: 2.10580
Epoch: 1/20, step: 70100, training_loss: 2.82233
Epoch: 1/20, step: 70120, training_loss: 2.38457
Epoch: 1/20, step: 70140, training_loss: 1.76534
Epoch: 1/20, step: 70160, training_loss: 2.35576
Epoch: 1/20, step: 70180, training_loss: 2.67540
Epoch: 1/20, step: 70200, training_loss: 2.28964
Epoch: 1/20, step: 70220, training_loss: 2.48911
Epoch: 1/20, step: 70240, training_loss: 1.85768
Epoch: 1/20, step: 70260, training_loss: 2.31032
Epoch: 1/20, step: 70280, training_loss: 1.85633
Epoch: 1/20, step: 70300, training_loss: 1.85934
Epoch: 1/20, step: 70320, training_loss: 2.59401
Epoch: 1/20, step: 70340, training_loss: 1.91414
Epoch: 1/20, step: 70360, training_loss: 3.15439
Epoch: 1/20, step: 70380, training_loss: 2.90683
Epoch: 1/20, step: 70400, training_loss: 2.61233
Epoch: 1/20, step: 70420, training_loss: 2.93539
Epoch: 1/20, step: 70440, training_loss: 2.99475
Epoch: 1/20, step: 70460, training_loss: 1.56284
Epoch: 1/20, step: 70480, training_loss: 2.49261
Epoch: 1/20, step: 70500, training_loss: 1.18126
Epoch: 1/20, step: 70520, training_loss: 1.82000
Epoch: 1/20, step: 70540, training_loss: 2.37144
Epoch: 1/20, step: 70560, training_loss: 2.23549
Epoch: 1/20, step: 70580, training_loss: 2.92817
Epoch: 1/20, step: 70600, training_loss: 3.05719
Epoch: 1/20, step: 70620, training_loss: 1.81267
Epoch: 1/20, step: 70640, training_loss: 2.75913
Epoch: 1/20, step: 70660, training_loss: 2.63675
Epoch: 1/20, step: 70680, training_loss: 2.12728
Epoch: 1/20, step: 70700, training_loss: 1.98339
Epoch: 1/20, step: 70720, training_loss: 2.56918
Epoch: 1/20, step: 70740, training_loss: 1.97195
Epoch: 1/20, step: 70760, training_loss: 2.60455
Epoch: 1/20, step: 70780, training_loss: 1.82235
Epoch: 1/20, step: 70800, training_loss: 1.75130
Epoch: 1/20, step: 70820, training_loss: 1.92390
Epoch: 1/20, step: 70840, training_loss: 2.65032
Epoch: 1/20, step: 70860, training_loss: 2.46062
Epoch: 1/20, step: 70880, training_loss: 2.38669
Epoch: 1/20, step: 70900, training_loss: 1.65071
Epoch: 1/20, step: 70920, training_loss: 1.91124
Epoch: 1/20, step: 70940, training_loss: 2.46229
Epoch: 1/20, step: 70960, training_loss: 1.84093
Epoch: 1/20, step: 70980, training_loss: 1.90597
Epoch: 1/20, step: 71000, training_loss: 2.45050
accuracy: 0.32, validation_loss: 2.469881057739258, num_samples: 100
Epoch: 1/20, step: 71020, training_loss: 2.43558
Epoch: 1/20, step: 71040, training_loss: 1.81903
Epoch: 1/20, step: 71060, training_loss: 1.91505
Epoch: 1/20, step: 71080, training_loss: 1.83504
Epoch: 1/20, step: 71100, training_loss: 2.17435
Epoch: 1/20, step: 71120, training_loss: 2.28786
Epoch: 1/20, step: 71140, training_loss: 1.83513
Epoch: 1/20, step: 71160, training_loss: 2.43773
Epoch: 1/20, step: 71180, training_loss: 2.00320
Epoch: 1/20, step: 71200, training_loss: 2.21849
Epoch: 1/20, step: 71220, training_loss: 1.80349
Epoch: 1/20, step: 71240, training_loss: 2.20648
Epoch: 1/20, step: 71260, training_loss: 2.10092
Epoch: 1/20, step: 71280, training_loss: 2.51875
Epoch: 1/20, step: 71300, training_loss: 2.41247
Epoch: 1/20, step: 71320, training_loss: 2.63315
Epoch: 1/20, step: 71340, training_loss: 1.86573
Epoch: 1/20, step: 71360, training_loss: 1.95473
Epoch: 1/20, step: 71380, training_loss: 2.19934
Epoch: 1/20, step: 71400, training_loss: 2.18714
Epoch: 1/20, step: 71420, training_loss: 2.22718
Epoch: 1/20, step: 71440, training_loss: 2.27325
Epoch: 1/20, step: 71460, training_loss: 1.84187
Epoch: 1/20, step: 71480, training_loss: 2.00000
Epoch: 1/20, step: 71500, training_loss: 2.34923
Epoch: 1/20, step: 71520, training_loss: 2.32158
Epoch: 1/20, step: 71540, training_loss: 1.68550
Epoch: 1/20, step: 71560, training_loss: 2.58143
Epoch: 1/20, step: 71580, training_loss: 2.20605
Epoch: 1/20, step: 71600, training_loss: 1.88516
Epoch: 1/20, step: 71620, training_loss: 2.09134
Epoch: 1/20, step: 71640, training_loss: 1.64336
Epoch: 1/20, step: 71660, training_loss: 2.08017
Epoch: 1/20, step: 71680, training_loss: 2.70737
Epoch: 1/20, step: 71700, training_loss: 1.71917
Epoch: 1/20, step: 71720, training_loss: 2.68301
Epoch: 1/20, step: 71740, training_loss: 2.33272
Epoch: 1/20, step: 71760, training_loss: 2.54243
Epoch: 1/20, step: 71780, training_loss: 2.19767
Epoch: 1/20, step: 71800, training_loss: 2.27504
Epoch: 1/20, step: 71820, training_loss: 3.00379
Epoch: 1/20, step: 71840, training_loss: 2.02886
Epoch: 1/20, step: 71860, training_loss: 2.24978
Epoch: 1/20, step: 71880, training_loss: 2.47372
Epoch: 1/20, step: 71900, training_loss: 2.36370
Epoch: 1/20, step: 71920, training_loss: 3.07597
Epoch: 1/20, step: 71940, training_loss: 1.78649
Epoch: 1/20, step: 71960, training_loss: 3.11959
Epoch: 1/20, step: 71980, training_loss: 1.55275
Epoch: 1/20, step: 72000, training_loss: 2.55353
accuracy: 0.39, validation_loss: 2.0486650466918945, num_samples: 100
Epoch: 1/20, step: 72020, training_loss: 2.46081
Epoch: 1/20, step: 72040, training_loss: 1.86768
Epoch: 1/20, step: 72060, training_loss: 1.31333
Epoch: 1/20, step: 72080, training_loss: 2.30771
Epoch: 1/20, step: 72100, training_loss: 2.79930
Epoch: 1/20, step: 72120, training_loss: 3.21878
Epoch: 1/20, step: 72140, training_loss: 1.58408
Epoch: 1/20, step: 72160, training_loss: 2.35832
Epoch: 1/20, step: 72180, training_loss: 2.86565
Epoch: 1/20, step: 72200, training_loss: 2.06399
Epoch: 1/20, step: 72220, training_loss: 1.56416
Epoch: 1/20, step: 72240, training_loss: 2.09352
Epoch: 1/20, step: 72260, training_loss: 1.86318
Epoch: 1/20, step: 72280, training_loss: 2.60136
Epoch: 1/20, step: 72300, training_loss: 2.02262
Epoch: 1/20, step: 72320, training_loss: 1.99436
Epoch: 1/20, step: 72340, training_loss: 2.37697
Epoch: 1/20, step: 72360, training_loss: 1.86449
Epoch: 1/20, step: 72380, training_loss: 2.77038
Epoch: 1/20, step: 72400, training_loss: 2.02058
Epoch: 1/20, step: 72420, training_loss: 2.03188
Epoch: 1/20, step: 72440, training_loss: 2.25635
Epoch: 1/20, step: 72460, training_loss: 1.79650
Epoch: 1/20, step: 72480, training_loss: 2.32024
Epoch: 1/20, step: 72500, training_loss: 1.78698
Epoch: 1/20, step: 72520, training_loss: 2.56017
Epoch: 1/20, step: 72540, training_loss: 2.16750
Epoch: 1/20, step: 72560, training_loss: 1.61446
Epoch: 1/20, step: 72580, training_loss: 2.14180
Epoch: 1/20, step: 72600, training_loss: 1.73813
Epoch: 1/20, step: 72620, training_loss: 2.33412
Epoch: 1/20, step: 72640, training_loss: 1.80524
Epoch: 1/20, step: 72660, training_loss: 2.29696
Epoch: 1/20, step: 72680, training_loss: 2.57008
Epoch: 1/20, step: 72700, training_loss: 3.51374
Epoch: 1/20, step: 72720, training_loss: 1.81906
Epoch: 1/20, step: 72740, training_loss: 2.33026
Epoch: 1/20, step: 72760, training_loss: 2.00247
Epoch: 1/20, step: 72780, training_loss: 2.01618
Epoch: 1/20, step: 72800, training_loss: 3.36635
Epoch: 1/20, step: 72820, training_loss: 1.85448
Epoch: 1/20, step: 72840, training_loss: 2.12771
Epoch: 1/20, step: 72860, training_loss: 2.08505
Epoch: 1/20, step: 72880, training_loss: 2.09711
Epoch: 1/20, step: 72900, training_loss: 2.36851
Epoch: 1/20, step: 72920, training_loss: 2.00928
Epoch: 1/20, step: 72940, training_loss: 2.51362
Epoch: 1/20, step: 72960, training_loss: 2.17008
Epoch: 1/20, step: 72980, training_loss: 2.13922
Epoch: 1/20, step: 73000, training_loss: 2.84641
accuracy: 0.4, validation_loss: 2.099937677383423, num_samples: 100
Epoch: 1/20, step: 73020, training_loss: 2.46193
Epoch: 1/20, step: 73040, training_loss: 3.40282
Epoch: 1/20, step: 73060, training_loss: 2.86126
Epoch: 1/20, step: 73080, training_loss: 2.67805
Epoch: 1/20, step: 73100, training_loss: 1.58132
Epoch: 1/20, step: 73120, training_loss: 1.83719
Epoch: 1/20, step: 73140, training_loss: 2.71504
Epoch: 1/20, step: 73160, training_loss: 2.70350
Epoch: 1/20, step: 73180, training_loss: 1.97867
Epoch: 1/20, step: 73200, training_loss: 1.80483
Epoch: 1/20, step: 73220, training_loss: 2.37575
Epoch: 1/20, step: 73240, training_loss: 2.15630
Epoch: 1/20, step: 73260, training_loss: 2.49395
Epoch: 1/20, step: 73280, training_loss: 2.10210
Epoch: 1/20, step: 73300, training_loss: 1.29992
Epoch: 1/20, step: 73320, training_loss: 1.93457
Epoch: 1/20, step: 73340, training_loss: 3.26017
Epoch: 1/20, step: 73360, training_loss: 2.36020
Epoch: 1/20, step: 73380, training_loss: 2.55508
Epoch: 1/20, step: 73400, training_loss: 2.62708
Epoch: 1/20, step: 73420, training_loss: 3.36964
Epoch: 1/20, step: 73440, training_loss: 2.95946
Epoch: 1/20, step: 73460, training_loss: 1.71574
Epoch: 1/20, step: 73480, training_loss: 1.69408
Epoch: 1/20, step: 73500, training_loss: 2.87112
Epoch: 1/20, step: 73520, training_loss: 2.45617
Epoch: 1/20, step: 73540, training_loss: 1.75533
Epoch: 1/20, step: 73560, training_loss: 1.80068
Epoch: 1/20, step: 73580, training_loss: 1.72037
Epoch: 1/20, step: 73600, training_loss: 1.88458
Epoch: 1/20, step: 73620, training_loss: 2.91630
Epoch: 1/20, step: 73640, training_loss: 2.32593
Epoch: 1/20, step: 73660, training_loss: 2.11514
Epoch: 1/20, step: 73680, training_loss: 2.92336
Epoch: 1/20, step: 73700, training_loss: 2.03276
Epoch: 1/20, step: 73720, training_loss: 2.11231
Epoch: 1/20, step: 73740, training_loss: 2.03854
Epoch: 1/20, step: 73760, training_loss: 1.73903
Epoch: 1/20, step: 73780, training_loss: 1.80516
Epoch: 1/20, step: 73800, training_loss: 2.39672
Epoch: 1/20, step: 73820, training_loss: 2.48589
Epoch: 1/20, step: 73840, training_loss: 1.87100
Epoch: 1/20, step: 73860, training_loss: 2.16942
Epoch: 1/20, step: 73880, training_loss: 2.09601
Epoch: 1/20, step: 73900, training_loss: 1.51190
Epoch: 1/20, step: 73920, training_loss: 1.70987
Epoch: 1/20, step: 73940, training_loss: 1.96120
Epoch: 1/20, step: 73960, training_loss: 1.84806
Epoch: 1/20, step: 73980, training_loss: 3.30510
Epoch: 1/20, step: 74000, training_loss: 1.92604
accuracy: 0.41, validation_loss: 2.178638219833374, num_samples: 100
Epoch: 1/20, step: 74020, training_loss: 2.48283
Epoch: 1/20, step: 74040, training_loss: 2.03761
Epoch: 1/20, step: 74060, training_loss: 2.61352
Epoch: 1/20, step: 74080, training_loss: 2.38369
Epoch: 1/20, step: 74100, training_loss: 2.44755
Epoch: 1/20, step: 74120, training_loss: 2.68800
Epoch: 1/20, step: 74140, training_loss: 1.93864
Epoch: 1/20, step: 74160, training_loss: 2.05973
Epoch: 1/20, step: 74180, training_loss: 2.30409
Epoch: 1/20, step: 74200, training_loss: 1.65042
Epoch: 1/20, step: 74220, training_loss: 2.07749
Epoch: 1/20, step: 74240, training_loss: 2.43458
Epoch: 1/20, step: 74260, training_loss: 2.20117
Epoch: 1/20, step: 74280, training_loss: 1.79092
Epoch: 1/20, step: 74300, training_loss: 2.11548
Epoch: 1/20, step: 74320, training_loss: 2.27352
Epoch: 1/20, step: 74340, training_loss: 2.75671
Epoch: 1/20, step: 74360, training_loss: 2.96047
Epoch: 1/20, step: 74380, training_loss: 2.18898
Epoch: 1/20, step: 74400, training_loss: 2.17851
Epoch: 1/20, step: 74420, training_loss: 2.27096
Epoch: 1/20, step: 74440, training_loss: 2.32441
Epoch: 1/20, step: 74460, training_loss: 2.49483
Epoch: 1/20, step: 74480, training_loss: 2.25073
Epoch: 1/20, step: 74500, training_loss: 2.15809
Epoch: 1/20, step: 74520, training_loss: 1.43613
Epoch: 1/20, step: 74540, training_loss: 1.57831
Epoch: 1/20, step: 74560, training_loss: 1.98088
Epoch: 1/20, step: 74580, training_loss: 2.28603
Epoch: 1/20, step: 74600, training_loss: 1.92709
Epoch: 1/20, step: 74620, training_loss: 2.14851
Epoch: 1/20, step: 74640, training_loss: 1.95982
Epoch: 1/20, step: 74660, training_loss: 1.98114
Epoch: 1/20, step: 74680, training_loss: 2.28708
Epoch: 1/20, step: 74700, training_loss: 2.17361
Epoch: 1/20, step: 74720, training_loss: 2.18544
Epoch: 1/20, step: 74740, training_loss: 2.15433
Epoch: 1/20, step: 74760, training_loss: 2.39612
Epoch: 1/20, step: 74780, training_loss: 2.66169
Epoch: 1/20, step: 74800, training_loss: 1.41981
Epoch: 1/20, step: 74820, training_loss: 2.45763
Epoch: 1/20, step: 74840, training_loss: 2.03351
Epoch: 1/20, step: 74860, training_loss: 1.94297
Epoch: 1/20, step: 74880, training_loss: 2.10208
Epoch: 1/20, step: 74900, training_loss: 2.26873
Epoch: 1/20, step: 74920, training_loss: 2.19448
Epoch: 1/20, step: 74940, training_loss: 2.51357
Epoch: 1/20, step: 74960, training_loss: 2.31640
Epoch: 1/20, step: 74980, training_loss: 1.96448
Epoch: 1/20, step: 75000, training_loss: 1.67712
accuracy: 0.45, validation_loss: 2.1308608055114746, num_samples: 100
Epoch: 1/20, step: 75020, training_loss: 2.52201
Epoch: 1/20, step: 75040, training_loss: 1.12701
Epoch: 1/20, step: 75060, training_loss: 2.17984
Epoch: 1/20, step: 75080, training_loss: 2.95273
Epoch: 1/20, step: 75100, training_loss: 1.77927
Epoch: 1/20, step: 75120, training_loss: 2.62258
Epoch: 1/20, step: 75140, training_loss: 2.47938
Epoch: 1/20, step: 75160, training_loss: 1.81395
Epoch: 1/20, step: 75180, training_loss: 2.03234
Epoch: 1/20, step: 75200, training_loss: 2.30358
Epoch: 1/20, step: 75220, training_loss: 2.62332
Epoch: 1/20, step: 75240, training_loss: 2.05056
Epoch: 1/20, step: 75260, training_loss: 2.79329
Epoch: 1/20, step: 75280, training_loss: 1.98530
Epoch: 1/20, step: 75300, training_loss: 2.66267
Epoch: 1/20, step: 75320, training_loss: 2.23922
Epoch: 1/20, step: 75340, training_loss: 3.17348
Epoch: 1/20, step: 75360, training_loss: 1.93490
Epoch: 1/20, step: 75380, training_loss: 2.08016
Epoch: 1/20, step: 75400, training_loss: 1.99625
Epoch: 1/20, step: 75420, training_loss: 2.12605
Epoch: 1/20, step: 75440, training_loss: 2.11082
Epoch: 1/20, step: 75460, training_loss: 2.78633
Epoch: 1/20, step: 75480, training_loss: 2.58063
Epoch: 1/20, step: 75500, training_loss: 2.68546
Epoch: 1/20, step: 75520, training_loss: 2.66095
Epoch: 1/20, step: 75540, training_loss: 2.51290
Epoch: 1/20, step: 75560, training_loss: 1.92551
Epoch: 1/20, step: 75580, training_loss: 3.00034
Epoch: 1/20, step: 75600, training_loss: 2.35557
Epoch: 1/20, step: 75620, training_loss: 3.26519
Epoch: 1/20, step: 75640, training_loss: 2.45767
Epoch: 1/20, step: 75660, training_loss: 2.43610
Epoch: 1/20, step: 75680, training_loss: 1.34331
Epoch: 1/20, step: 75700, training_loss: 2.40782
Epoch: 1/20, step: 75720, training_loss: 2.44515
Epoch: 1/20, step: 75740, training_loss: 2.52109
Epoch: 1/20, step: 75760, training_loss: 2.26459
Epoch: 1/20, step: 75780, training_loss: 1.83029
Epoch: 1/20, step: 75800, training_loss: 1.96403
Epoch: 1/20, step: 75820, training_loss: 2.76205
Epoch: 1/20, step: 75840, training_loss: 2.46437
Epoch: 1/20, step: 75860, training_loss: 1.26322
Epoch: 1/20, step: 75880, training_loss: 1.97513
Epoch: 1/20, step: 75900, training_loss: 2.20383
Epoch: 1/20, step: 75920, training_loss: 2.27546
Epoch: 1/20, step: 75940, training_loss: 2.83705
Epoch: 1/20, step: 75960, training_loss: 2.04253
Epoch: 1/20, step: 75980, training_loss: 1.58682
Epoch: 1/20, step: 76000, training_loss: 2.30180
accuracy: 0.47, validation_loss: 1.9202558994293213, num_samples: 100
Epoch: 1/20, step: 76020, training_loss: 1.68699
Epoch: 1/20, step: 76040, training_loss: 2.63688
Epoch: 1/20, step: 76060, training_loss: 2.45946
Epoch: 1/20, step: 76080, training_loss: 2.34418
Epoch: 1/20, step: 76100, training_loss: 2.67832
Epoch: 1/20, step: 76120, training_loss: 2.43506
Epoch: 1/20, step: 76140, training_loss: 2.33948
Epoch: 1/20, step: 76160, training_loss: 3.06352
Epoch: 1/20, step: 76180, training_loss: 3.27129
Epoch: 1/20, step: 76200, training_loss: 1.96727
Epoch: 1/20, step: 76220, training_loss: 2.10616
Epoch: 1/20, step: 76240, training_loss: 2.85809
Epoch: 1/20, step: 76260, training_loss: 2.79098
Epoch: 1/20, step: 76280, training_loss: 1.76131
Epoch: 1/20, step: 76300, training_loss: 2.23178
Epoch: 1/20, step: 76320, training_loss: 1.71981
Epoch: 1/20, step: 76340, training_loss: 2.31756
Epoch: 1/20, step: 76360, training_loss: 1.92869
Epoch: 1/20, step: 76380, training_loss: 2.56825
Epoch: 1/20, step: 76400, training_loss: 1.64378
Epoch: 1/20, step: 76420, training_loss: 2.41577
Epoch: 1/20, step: 76440, training_loss: 1.75927
Epoch: 1/20, step: 76460, training_loss: 2.23730
Epoch: 1/20, step: 76480, training_loss: 2.06968
Epoch: 1/20, step: 76500, training_loss: 2.57056
Epoch: 1/20, step: 76520, training_loss: 2.18809
Epoch: 1/20, step: 76540, training_loss: 1.17384
Epoch: 1/20, step: 76560, training_loss: 2.62134
Epoch: 1/20, step: 76580, training_loss: 2.86578
Epoch: 1/20, step: 76600, training_loss: 2.05991
Epoch: 1/20, step: 76620, training_loss: 2.10232
Epoch: 1/20, step: 76640, training_loss: 1.94065
Epoch: 1/20, step: 76660, training_loss: 2.74363
Epoch: 1/20, step: 76680, training_loss: 1.99601
Epoch: 1/20, step: 76700, training_loss: 2.32472
Epoch: 1/20, step: 76720, training_loss: 1.78015
Epoch: 1/20, step: 76740, training_loss: 1.68993
Epoch: 1/20, step: 76760, training_loss: 2.25745
Epoch: 1/20, step: 76780, training_loss: 1.62926
Epoch: 1/20, step: 76800, training_loss: 2.97795
Epoch: 1/20, step: 76820, training_loss: 2.44484
Epoch: 1/20, step: 76840, training_loss: 2.10981
Epoch: 1/20, step: 76860, training_loss: 2.48675
Epoch: 1/20, step: 76880, training_loss: 2.06942
Epoch: 1/20, step: 76900, training_loss: 2.33090
Epoch: 1/20, step: 76920, training_loss: 2.44329
Epoch: 1/20, step: 76940, training_loss: 2.29755
Epoch: 1/20, step: 76960, training_loss: 1.53914
Epoch: 1/20, step: 76980, training_loss: 2.07455
Epoch: 1/20, step: 77000, training_loss: 1.95983
accuracy: 0.34, validation_loss: 2.4904654026031494, num_samples: 100
Epoch: 1/20, step: 77020, training_loss: 1.48631
Epoch: 1/20, step: 77040, training_loss: 3.08183
Epoch: 1/20, step: 77060, training_loss: 2.29435
Epoch: 1/20, step: 77080, training_loss: 1.69256
Epoch: 1/20, step: 77100, training_loss: 1.78118
Epoch: 1/20, step: 77120, training_loss: 2.37544
Epoch: 1/20, step: 77140, training_loss: 2.78356
Epoch: 1/20, step: 77160, training_loss: 2.59309
Epoch: 1/20, step: 77180, training_loss: 2.51366
Epoch: 1/20, step: 77200, training_loss: 2.20717
Epoch: 1/20, step: 77220, training_loss: 2.83474
Epoch: 1/20, step: 77240, training_loss: 1.79483
Epoch: 1/20, step: 77260, training_loss: 2.59739
Epoch: 1/20, step: 77280, training_loss: 1.44138
Epoch: 1/20, step: 77300, training_loss: 1.38660
Epoch: 1/20, step: 77320, training_loss: 1.95035
Epoch: 1/20, step: 77340, training_loss: 2.39963
Epoch: 1/20, step: 77360, training_loss: 3.46572
Epoch: 1/20, step: 77380, training_loss: 2.50019
Epoch: 1/20, step: 77400, training_loss: 1.68102
Epoch: 1/20, step: 77420, training_loss: 2.38334
Epoch: 1/20, step: 77440, training_loss: 1.46680
Epoch: 1/20, step: 77460, training_loss: 1.73512
Epoch: 1/20, step: 77480, training_loss: 2.02355
Epoch: 1/20, step: 77500, training_loss: 2.13075
Epoch: 1/20, step: 77520, training_loss: 1.35897
Epoch: 1/20, step: 77540, training_loss: 2.43702
Epoch: 1/20, step: 77560, training_loss: 2.03014
Epoch: 1/20, step: 77580, training_loss: 2.79983
Epoch: 1/20, step: 77600, training_loss: 2.17104
Epoch: 1/20, step: 77620, training_loss: 1.89699
Epoch: 1/20, step: 77640, training_loss: 2.36099
Epoch: 1/20, step: 77660, training_loss: 3.03521
Epoch: 1/20, step: 77680, training_loss: 1.21715
Epoch: 1/20, step: 77700, training_loss: 1.98495
Epoch: 1/20, step: 77720, training_loss: 1.74997
Epoch: 1/20, step: 77740, training_loss: 3.00102
Epoch: 1/20, step: 77760, training_loss: 2.18775
Epoch: 1/20, step: 77780, training_loss: 2.64331
Epoch: 1/20, step: 77800, training_loss: 2.04798
Epoch: 1/20, step: 77820, training_loss: 1.55484
Epoch: 1/20, step: 77840, training_loss: 1.41544
Epoch: 1/20, step: 77860, training_loss: 2.34729
Epoch: 1/20, step: 77880, training_loss: 2.34475
Epoch: 1/20, step: 77900, training_loss: 2.19379
Epoch: 1/20, step: 77920, training_loss: 1.83269
Epoch: 1/20, step: 77940, training_loss: 2.62526
Epoch: 1/20, step: 77960, training_loss: 1.94178
Epoch: 1/20, step: 77980, training_loss: 2.58681
Epoch: 1/20, step: 78000, training_loss: 2.58517
accuracy: 0.48, validation_loss: 1.9506076574325562, num_samples: 100
Epoch: 1/20, step: 78020, training_loss: 2.59832
Epoch: 1/20, step: 78040, training_loss: 2.10051
Epoch: 1/20, step: 78060, training_loss: 1.84117
Epoch: 1/20, step: 78080, training_loss: 3.18193
Epoch: 1/20, step: 78100, training_loss: 2.48308
Epoch: 1/20, step: 78120, training_loss: 3.14336
Epoch: 1/20, step: 78140, training_loss: 3.17697
Epoch: 1/20, step: 78160, training_loss: 1.72091
Epoch: 1/20, step: 78180, training_loss: 2.18064
Epoch: 1/20, step: 78200, training_loss: 3.05172
Epoch: 1/20, step: 78220, training_loss: 1.90197
Epoch: 1/20, step: 78240, training_loss: 1.58514
Epoch: 1/20, step: 78260, training_loss: 1.70355
Epoch: 1/20, step: 78280, training_loss: 2.63443
Epoch: 1/20, step: 78300, training_loss: 2.95051
Epoch: 1/20, step: 78320, training_loss: 2.38635
Epoch: 1/20, step: 78340, training_loss: 3.06421
Epoch: 1/20, step: 78360, training_loss: 2.50749
Epoch: 1/20, step: 78380, training_loss: 2.79403
Epoch: 1/20, step: 78400, training_loss: 2.13222
Epoch: 1/20, step: 78420, training_loss: 2.22905
Epoch: 1/20, step: 78440, training_loss: 2.10872
Epoch: 1/20, step: 78460, training_loss: 1.87292
Epoch: 1/20, step: 78480, training_loss: 2.02990
Epoch: 1/20, step: 78500, training_loss: 1.97866
Epoch: 1/20, step: 78520, training_loss: 2.09356
Epoch: 1/20, step: 78540, training_loss: 2.32181
Epoch: 1/20, step: 78560, training_loss: 1.99158
Epoch: 1/20, step: 78580, training_loss: 2.46790
Epoch: 1/20, step: 78600, training_loss: 3.27263
Epoch: 1/20, step: 78620, training_loss: 2.37776
Epoch: 1/20, step: 78640, training_loss: 0.92675
Epoch: 1/20, step: 78660, training_loss: 2.27768
Epoch: 1/20, step: 78680, training_loss: 2.80109
Epoch: 1/20, step: 78700, training_loss: 2.50812
Epoch: 1/20, step: 78720, training_loss: 2.23566
Epoch: 1/20, step: 78740, training_loss: 2.51472
Epoch: 1/20, step: 78760, training_loss: 1.91070
Epoch: 1/20, step: 78780, training_loss: 2.53262
Epoch: 1/20, step: 78800, training_loss: 2.45902
Epoch: 1/20, step: 78820, training_loss: 1.69419
Epoch: 1/20, step: 78840, training_loss: 2.26627
Epoch: 1/20, step: 78860, training_loss: 2.24553
Epoch: 1/20, step: 78880, training_loss: 2.77466
Epoch: 1/20, step: 78900, training_loss: 3.05831
Epoch: 1/20, step: 78920, training_loss: 1.99100
Epoch: 1/20, step: 78940, training_loss: 1.81184
Epoch: 1/20, step: 78960, training_loss: 2.06250
Epoch: 1/20, step: 78980, training_loss: 2.27155
Epoch: 1/20, step: 79000, training_loss: 2.25941
accuracy: 0.36, validation_loss: 2.449690341949463, num_samples: 100
Epoch: 1/20, step: 79020, training_loss: 2.07193
Epoch: 1/20, step: 79040, training_loss: 2.39587
Epoch: 1/20, step: 79060, training_loss: 2.61729
Epoch: 1/20, step: 79080, training_loss: 1.45965
Epoch: 1/20, step: 79100, training_loss: 2.95826
Epoch: 1/20, step: 79120, training_loss: 1.73316
Epoch: 1/20, step: 79140, training_loss: 1.81806
Epoch: 1/20, step: 79160, training_loss: 3.49847
Epoch: 1/20, step: 79180, training_loss: 2.78968
Epoch: 1/20, step: 79200, training_loss: 2.38808
Epoch: 1/20, step: 79220, training_loss: 2.22502
Epoch: 1/20, step: 79240, training_loss: 2.62310
Epoch: 1/20, step: 79260, training_loss: 1.90398
Epoch: 1/20, step: 79280, training_loss: 1.50381
Epoch: 1/20, step: 79300, training_loss: 2.66958
Epoch: 1/20, step: 79320, training_loss: 2.73827
Epoch: 1/20, step: 79340, training_loss: 1.93146
Epoch: 1/20, step: 79360, training_loss: 2.13385
Epoch: 1/20, step: 79380, training_loss: 2.21951
Epoch: 1/20, step: 79400, training_loss: 2.44223
Epoch: 1/20, step: 79420, training_loss: 1.46316
Epoch: 1/20, step: 79440, training_loss: 2.68403
Epoch: 1/20, step: 79460, training_loss: 1.71799
Epoch: 1/20, step: 79480, training_loss: 2.14266
Epoch: 1/20, step: 79500, training_loss: 1.96331
Epoch: 1/20, step: 79520, training_loss: 2.08103
Epoch: 1/20, step: 79540, training_loss: 2.83309
Epoch: 1/20, step: 79560, training_loss: 2.31109
Epoch: 1/20, step: 79580, training_loss: 2.91167
Epoch: 1/20, step: 79600, training_loss: 1.92808
Epoch: 1/20, step: 79620, training_loss: 2.11712
Epoch: 1/20, step: 79640, training_loss: 1.74299
Epoch: 1/20, step: 79660, training_loss: 2.13049
Epoch: 1/20, step: 79680, training_loss: 2.27813
Epoch: 1/20, step: 79700, training_loss: 1.63304
Epoch: 1/20, step: 79720, training_loss: 2.88782
Epoch: 1/20, step: 79740, training_loss: 2.08045
Epoch: 1/20, step: 79760, training_loss: 2.44290
Epoch: 1/20, step: 79780, training_loss: 2.06057
Epoch: 1/20, step: 79800, training_loss: 2.65246
Epoch: 1/20, step: 79820, training_loss: 2.57195
Epoch: 1/20, step: 79840, training_loss: 2.43425
Epoch: 1/20, step: 79860, training_loss: 2.24136
Epoch: 1/20, step: 79880, training_loss: 1.70572
Epoch: 1/20, step: 79900, training_loss: 1.94508
Epoch: 1/20, step: 79920, training_loss: 1.96226
Epoch: 1/20, step: 79940, training_loss: 2.20808
Epoch: 1/20, step: 79960, training_loss: 2.33018
Epoch: 1/20, step: 79980, training_loss: 2.93910
Epoch: 1/20, step: 80000, training_loss: 1.80334
accuracy: 0.46, validation_loss: 2.171663999557495, num_samples: 100
Epoch: 1/20, step: 80020, training_loss: 2.10700
Epoch: 1/20, step: 80040, training_loss: 2.47170
Epoch: 1/20, step: 80060, training_loss: 1.38494
Epoch: 1/20, step: 80080, training_loss: 2.85159
Epoch: 1/20, step: 80100, training_loss: 2.26087
Epoch: 1/20, step: 80120, training_loss: 2.08046
Epoch: 1/20, step: 80140, training_loss: 2.24665
Epoch: 1/20, step: 80160, training_loss: 1.75016
Epoch: 1/20, step: 80180, training_loss: 2.84644
Epoch: 1/20, step: 80200, training_loss: 2.17367
Epoch: 1/20, step: 80220, training_loss: 1.70649
Epoch: 1/20, step: 80240, training_loss: 1.51176
Epoch: 1/20, step: 80260, training_loss: 1.76311
Epoch: 1/20, step: 80280, training_loss: 2.60893
Epoch: 1/20, step: 80300, training_loss: 1.76852
Epoch: 1/20, step: 80320, training_loss: 2.42077
Epoch: 1/20, step: 80340, training_loss: 2.06863
Epoch: 1/20, step: 80360, training_loss: 2.42362
Epoch: 1/20, step: 80380, training_loss: 2.40121
Epoch: 1/20, step: 80400, training_loss: 2.39041
Epoch: 1/20, step: 80420, training_loss: 3.00236
Epoch: 1/20, step: 80440, training_loss: 1.78809
Epoch: 1/20, step: 80460, training_loss: 2.18005
Epoch: 1/20, step: 80480, training_loss: 2.23376
Epoch: 1/20, step: 80500, training_loss: 2.31663
Epoch: 1/20, step: 80520, training_loss: 2.75848
Epoch: 1/20, step: 80540, training_loss: 2.09809
Epoch: 1/20, step: 80560, training_loss: 2.14187
Epoch: 1/20, step: 80580, training_loss: 2.51991
Epoch: 1/20, step: 80600, training_loss: 1.62023
Epoch: 1/20, step: 80620, training_loss: 1.93091
Epoch: 1/20, step: 80640, training_loss: 2.43124
Epoch: 1/20, step: 80660, training_loss: 2.84177
Epoch: 1/20, step: 80680, training_loss: 2.22368
Epoch: 1/20, step: 80700, training_loss: 2.09438
Epoch: 1/20, step: 80720, training_loss: 2.12067
Epoch: 1/20, step: 80740, training_loss: 2.27207
Epoch: 1/20, step: 80760, training_loss: 2.23521
Epoch: 1/20, step: 80780, training_loss: 2.02356
Epoch: 1/20, step: 80800, training_loss: 2.05854
Epoch: 1/20, step: 80820, training_loss: 2.08952
Epoch: 1/20, step: 80840, training_loss: 2.18627
Epoch: 1/20, step: 80860, training_loss: 2.27531
Epoch: 1/20, step: 80880, training_loss: 2.86170
Epoch: 1/20, step: 80900, training_loss: 3.05130
Epoch: 1/20, step: 80920, training_loss: 2.35786
Epoch: 1/20, step: 80940, training_loss: 3.21758
Epoch: 1/20, step: 80960, training_loss: 2.36423
Epoch: 1/20, step: 80980, training_loss: 1.76513
Epoch: 1/20, step: 81000, training_loss: 1.96168
accuracy: 0.36, validation_loss: 2.0810515880584717, num_samples: 100
Epoch: 1/20, step: 81020, training_loss: 1.93158
Epoch: 1/20, step: 81040, training_loss: 2.71918
Epoch: 1/20, step: 81060, training_loss: 1.67208
Epoch: 1/20, step: 81080, training_loss: 2.69709
Epoch: 1/20, step: 81100, training_loss: 2.43079
Epoch: 1/20, step: 81120, training_loss: 1.37507
Epoch: 1/20, step: 81140, training_loss: 2.16918
Epoch: 1/20, step: 81160, training_loss: 2.09315
Epoch: 1/20, step: 81180, training_loss: 2.95869
Epoch: 1/20, step: 81200, training_loss: 1.92655
Epoch: 1/20, step: 81220, training_loss: 1.93090
Epoch: 1/20, step: 81240, training_loss: 1.92586
Epoch: 1/20, step: 81260, training_loss: 2.66614
Epoch: 1/20, step: 81280, training_loss: 2.45751
Epoch: 1/20, step: 81300, training_loss: 1.96167
Epoch: 1/20, step: 81320, training_loss: 2.42970
Epoch: 1/20, step: 81340, training_loss: 1.49948
Epoch: 1/20, step: 81360, training_loss: 2.00848
Epoch: 1/20, step: 81380, training_loss: 2.01832
Epoch: 1/20, step: 81400, training_loss: 0.88305
Epoch: 1/20, step: 81420, training_loss: 2.78893
Epoch: 1/20, step: 81440, training_loss: 2.04645
Epoch: 1/20, step: 81460, training_loss: 3.29354
Epoch: 1/20, step: 81480, training_loss: 1.83751
Epoch: 1/20, step: 81500, training_loss: 1.81200
Epoch: 1/20, step: 81520, training_loss: 1.68704
Epoch: 1/20, step: 81540, training_loss: 2.08280
Epoch: 1/20, step: 81560, training_loss: 2.32814
Epoch: 1/20, step: 81580, training_loss: 1.58506
Epoch: 1/20, step: 81600, training_loss: 1.99945
Epoch: 1/20, step: 81620, training_loss: 1.56513
Epoch: 1/20, step: 81640, training_loss: 2.12974
Epoch: 1/20, step: 81660, training_loss: 2.59294
Epoch: 1/20, step: 81680, training_loss: 2.01220
Epoch: 1/20, step: 81700, training_loss: 2.24693
Epoch: 1/20, step: 81720, training_loss: 1.77659
Epoch: 1/20, step: 81740, training_loss: 1.42986
Epoch: 1/20, step: 81760, training_loss: 1.83177
Epoch: 1/20, step: 81780, training_loss: 1.64396
Epoch: 1/20, step: 81800, training_loss: 2.53165
Epoch: 1/20, step: 81820, training_loss: 2.20250
Epoch: 1/20, step: 81840, training_loss: 1.99627
Epoch: 1/20, step: 81860, training_loss: 1.45782
Epoch: 1/20, step: 81880, training_loss: 2.23791
Epoch: 1/20, step: 81900, training_loss: 2.15858
Epoch: 1/20, step: 81920, training_loss: 2.15467
Epoch: 1/20, step: 81940, training_loss: 2.43935
Epoch: 1/20, step: 81960, training_loss: 2.01606
Epoch: 1/20, step: 81980, training_loss: 2.10520
Epoch: 1/20, step: 82000, training_loss: 1.83472
accuracy: 0.42, validation_loss: 2.082371473312378, num_samples: 100
Epoch: 1/20, step: 82020, training_loss: 2.44945
Epoch: 1/20, step: 82040, training_loss: 2.75542
Epoch: 1/20, step: 82060, training_loss: 2.22893
Epoch: 1/20, step: 82080, training_loss: 1.80644
Epoch: 1/20, step: 82100, training_loss: 1.45578
Epoch: 1/20, step: 82120, training_loss: 1.52472
Epoch: 1/20, step: 82140, training_loss: 2.27309
Epoch: 1/20, step: 82160, training_loss: 1.77770
Epoch: 1/20, step: 82180, training_loss: 1.68809
Epoch: 1/20, step: 82200, training_loss: 2.40188
Epoch: 1/20, step: 82220, training_loss: 3.09418
Epoch: 1/20, step: 82240, training_loss: 2.33104
Epoch: 1/20, step: 82260, training_loss: 3.18355
Epoch: 1/20, step: 82280, training_loss: 2.20785
Epoch: 1/20, step: 82300, training_loss: 2.95795
Epoch: 1/20, step: 82320, training_loss: 2.73046
Epoch: 1/20, step: 82340, training_loss: 2.08353
Epoch: 1/20, step: 82360, training_loss: 2.46162
Epoch: 1/20, step: 82380, training_loss: 1.28697
Epoch: 1/20, step: 82400, training_loss: 2.52315
Epoch: 1/20, step: 82420, training_loss: 1.76891
Epoch: 1/20, step: 82440, training_loss: 2.80166
Epoch: 1/20, step: 82460, training_loss: 2.01363
Epoch: 1/20, step: 82480, training_loss: 2.79361
Epoch: 1/20, step: 82500, training_loss: 2.75246
Epoch: 1/20, step: 82520, training_loss: 2.52816
Epoch: 1/20, step: 82540, training_loss: 2.50491
Epoch: 1/20, step: 82560, training_loss: 2.60609
Epoch: 1/20, step: 82580, training_loss: 1.96103
Epoch: 1/20, step: 82600, training_loss: 2.28528
Epoch: 1/20, step: 82620, training_loss: 2.12904
Epoch: 1/20, step: 82640, training_loss: 2.68913
Epoch: 1/20, step: 82660, training_loss: 1.73946
Epoch: 1/20, step: 82680, training_loss: 2.19505
Epoch: 1/20, step: 82700, training_loss: 2.70287
Epoch: 1/20, step: 82720, training_loss: 1.32596
Epoch: 1/20, step: 82740, training_loss: 2.21110
Epoch: 1/20, step: 82760, training_loss: 2.49499
Epoch: 1/20, step: 82780, training_loss: 2.10659
Epoch: 1/20, step: 82800, training_loss: 2.69765
Epoch: 1/20, step: 82820, training_loss: 2.45903
Epoch: 1/20, step: 82840, training_loss: 1.44301
Epoch: 1/20, step: 82860, training_loss: 2.09512
Epoch: 1/20, step: 82880, training_loss: 2.01210
Epoch: 1/20, step: 82900, training_loss: 2.31704
Epoch: 1/20, step: 82920, training_loss: 1.65794
Epoch: 1/20, step: 82940, training_loss: 2.23521
Epoch: 1/20, step: 82960, training_loss: 1.82248
Epoch: 1/20, step: 82980, training_loss: 2.59408
Epoch: 1/20, step: 83000, training_loss: 2.64510
accuracy: 0.34, validation_loss: 2.5106964111328125, num_samples: 100
Epoch: 1/20, step: 83020, training_loss: 2.18336
Epoch: 1/20, step: 83040, training_loss: 1.89486
Epoch: 1/20, step: 83060, training_loss: 2.18059
Epoch: 1/20, step: 83080, training_loss: 2.78666
Epoch: 1/20, step: 83100, training_loss: 2.08287
Epoch: 1/20, step: 83120, training_loss: 3.10602
Epoch: 1/20, step: 83140, training_loss: 2.26705
Epoch: 1/20, step: 83160, training_loss: 2.08010
Epoch: 1/20, step: 83180, training_loss: 2.15085
Epoch: 1/20, step: 83200, training_loss: 1.56994
Epoch: 1/20, step: 83220, training_loss: 2.21500
Epoch: 1/20, step: 83240, training_loss: 1.86327
Epoch: 1/20, step: 83260, training_loss: 2.10877
Epoch: 1/20, step: 83280, training_loss: 2.69890
Epoch: 1/20, step: 83300, training_loss: 2.13435
Epoch: 1/20, step: 83320, training_loss: 2.16715
Epoch: 1/20, step: 83340, training_loss: 2.38267
Epoch: 1/20, step: 83360, training_loss: 1.52036
Epoch: 1/20, step: 83380, training_loss: 2.47114
Epoch: 1/20, step: 83400, training_loss: 2.39756
Epoch: 1/20, step: 83420, training_loss: 3.26861
Epoch: 1/20, step: 83440, training_loss: 3.01160
Epoch: 1/20, step: 83460, training_loss: 2.22529
Epoch: 1/20, step: 83480, training_loss: 1.22402
Epoch: 1/20, step: 83500, training_loss: 2.51964
Epoch: 1/20, step: 83520, training_loss: 3.21831
Epoch: 1/20, step: 83540, training_loss: 1.84036
Epoch: 1/20, step: 83560, training_loss: 1.98166
Epoch: 1/20, step: 83580, training_loss: 2.64126
Epoch: 1/20, step: 83600, training_loss: 2.44739
Epoch: 1/20, step: 83620, training_loss: 1.93233
Epoch: 1/20, step: 83640, training_loss: 2.11224
Epoch: 1/20, step: 83660, training_loss: 2.88104
Epoch: 1/20, step: 83680, training_loss: 2.04771
Epoch: 1/20, step: 83700, training_loss: 1.32359
Epoch: 1/20, step: 83720, training_loss: 2.10725
Epoch: 1/20, step: 83740, training_loss: 1.33250
Epoch: 1/20, step: 83760, training_loss: 2.05796
Epoch: 1/20, step: 83780, training_loss: 2.11474
Epoch: 1/20, step: 83800, training_loss: 2.18091
Epoch: 1/20, step: 83820, training_loss: 1.98898
Epoch: 1/20, step: 83840, training_loss: 1.86800
Epoch: 1/20, step: 83860, training_loss: 1.91570
Epoch: 1/20, step: 83880, training_loss: 1.87822
Epoch: 1/20, step: 83900, training_loss: 1.30799
Epoch: 1/20, step: 83920, training_loss: 2.88630
Epoch: 1/20, step: 83940, training_loss: 3.10526
Epoch: 1/20, step: 83960, training_loss: 1.88640
Epoch: 1/20, step: 83980, training_loss: 2.34944
Epoch: 1/20, step: 84000, training_loss: 2.42126
accuracy: 0.36, validation_loss: 2.4746336936950684, num_samples: 100
Epoch: 1/20, step: 84020, training_loss: 2.83428
Epoch: 1/20, step: 84040, training_loss: 2.74411
Epoch: 1/20, step: 84060, training_loss: 1.83723
Epoch: 1/20, step: 84080, training_loss: 2.64542
Epoch: 1/20, step: 84100, training_loss: 2.68350
Epoch: 1/20, step: 84120, training_loss: 2.51305
Epoch: 1/20, step: 84140, training_loss: 2.56766
Epoch: 1/20, step: 84160, training_loss: 2.45607
Epoch: 1/20, step: 84180, training_loss: 2.60805
Epoch: 1/20, step: 84200, training_loss: 2.22295
Epoch: 1/20, step: 84220, training_loss: 1.65902
Epoch: 1/20, step: 84240, training_loss: 2.01846
Epoch: 1/20, step: 84260, training_loss: 3.50845
Epoch: 1/20, step: 84280, training_loss: 1.86793
Epoch: 1/20, step: 84300, training_loss: 2.12021
Epoch: 1/20, step: 84320, training_loss: 1.59276
Epoch: 1/20, step: 84340, training_loss: 2.50659
Epoch: 1/20, step: 84360, training_loss: 2.07143
Epoch: 1/20, step: 84380, training_loss: 1.96052
Epoch: 1/20, step: 84400, training_loss: 2.66494
Epoch: 1/20, step: 84420, training_loss: 1.70190
Epoch: 1/20, step: 84440, training_loss: 1.11329
Epoch: 1/20, step: 84460, training_loss: 1.49376
Epoch: 1/20, step: 84480, training_loss: 3.06010
Epoch: 1/20, step: 84500, training_loss: 2.21549
Epoch: 1/20, step: 84520, training_loss: 2.11270
Epoch: 1/20, step: 84540, training_loss: 2.00304
Epoch: 1/20, step: 84560, training_loss: 2.47485
Epoch: 1/20, step: 84580, training_loss: 1.98608
Epoch: 1/20, step: 84600, training_loss: 2.61244
Epoch: 1/20, step: 84620, training_loss: 1.62267
Epoch: 1/20, step: 84640, training_loss: 3.85897
Epoch: 1/20, step: 84660, training_loss: 2.09071
Epoch: 1/20, step: 84680, training_loss: 1.87900
Epoch: 1/20, step: 84700, training_loss: 1.96684
Epoch: 1/20, step: 84720, training_loss: 2.08753
Epoch: 1/20, step: 84740, training_loss: 2.63569
Epoch: 1/20, step: 84760, training_loss: 2.33776
Epoch: 1/20, step: 84780, training_loss: 2.01360
Epoch: 1/20, step: 84800, training_loss: 1.94269
Epoch: 1/20, step: 84820, training_loss: 2.20933
Epoch: 1/20, step: 84840, training_loss: 2.64017
Epoch: 1/20, step: 84860, training_loss: 2.88119
Epoch: 1/20, step: 84880, training_loss: 3.40976
Epoch: 1/20, step: 84900, training_loss: 2.03440
Epoch: 1/20, step: 84920, training_loss: 2.07054
Epoch: 1/20, step: 84940, training_loss: 1.66839
Epoch: 1/20, step: 84960, training_loss: 1.90445
Epoch: 1/20, step: 84980, training_loss: 1.82419
Epoch: 1/20, step: 85000, training_loss: 2.45898
accuracy: 0.42, validation_loss: 2.0768821239471436, num_samples: 100
Epoch: 1/20, step: 85020, training_loss: 2.99145
Epoch: 1/20, step: 85040, training_loss: 2.22207
Epoch: 1/20, step: 85060, training_loss: 2.57648
Epoch: 1/20, step: 85080, training_loss: 2.72465
Epoch: 1/20, step: 85100, training_loss: 1.81312
Epoch: 1/20, step: 85120, training_loss: 2.62924
Epoch: 1/20, step: 85140, training_loss: 1.81238
Epoch: 1/20, step: 85160, training_loss: 3.03725
Epoch: 1/20, step: 85180, training_loss: 2.15497
Epoch: 1/20, step: 85200, training_loss: 2.17094
Epoch: 1/20, step: 85220, training_loss: 2.08131
Epoch: 1/20, step: 85240, training_loss: 2.61624
Epoch: 1/20, step: 85260, training_loss: 2.82847
Epoch: 1/20, step: 85280, training_loss: 2.31076
Epoch: 1/20, step: 85300, training_loss: 1.70036
Epoch: 1/20, step: 85320, training_loss: 2.39714
Epoch: 1/20, step: 85340, training_loss: 1.78850
Epoch: 1/20, step: 85360, training_loss: 2.95139
Epoch: 1/20, step: 85380, training_loss: 2.60718
Epoch: 1/20, step: 85400, training_loss: 2.79487
Epoch: 1/20, step: 85420, training_loss: 2.39214
Epoch: 1/20, step: 85440, training_loss: 2.66467
Epoch: 1/20, step: 85460, training_loss: 2.93969
Epoch: 1/20, step: 85480, training_loss: 1.95184
Epoch: 1/20, step: 85500, training_loss: 2.50145
Epoch: 1/20, step: 85520, training_loss: 3.58491
Epoch: 1/20, step: 85540, training_loss: 3.31522
Epoch: 1/20, step: 85560, training_loss: 1.59170
Epoch: 1/20, step: 85580, training_loss: 2.55465
Epoch: 1/20, step: 85600, training_loss: 1.90007
Epoch: 1/20, step: 85620, training_loss: 2.95534
Epoch: 1/20, step: 85640, training_loss: 3.29076
Epoch: 1/20, step: 85660, training_loss: 2.49286
Epoch: 1/20, step: 85680, training_loss: 2.55153
Epoch: 1/20, step: 85700, training_loss: 1.66832
Epoch: 1/20, step: 85720, training_loss: 1.79308
Epoch: 1/20, step: 85740, training_loss: 2.58111
Epoch: 1/20, step: 85760, training_loss: 2.30395
Epoch: 1/20, step: 85780, training_loss: 2.44031
Epoch: 1/20, step: 85800, training_loss: 1.57292
Epoch: 1/20, step: 85820, training_loss: 2.60162
Epoch: 1/20, step: 85840, training_loss: 1.53148
Epoch: 1/20, step: 85860, training_loss: 1.63214
Epoch: 1/20, step: 85880, training_loss: 1.58744
Epoch: 1/20, step: 85900, training_loss: 2.72331
Epoch: 1/20, step: 85920, training_loss: 2.60849
Epoch: 1/20, step: 85940, training_loss: 2.06971
Epoch: 1/20, step: 85960, training_loss: 1.98518
Epoch: 1/20, step: 85980, training_loss: 1.93546
Epoch: 1/20, step: 86000, training_loss: 2.41739
accuracy: 0.46, validation_loss: 1.9881963729858398, num_samples: 100
Epoch: 1/20, step: 86020, training_loss: 2.21604
Epoch: 1/20, step: 86040, training_loss: 2.31922
Epoch: 1/20, step: 86060, training_loss: 2.26774
Epoch: 1/20, step: 86080, training_loss: 2.03577
Epoch: 1/20, step: 86100, training_loss: 2.07178
Epoch: 1/20, step: 86120, training_loss: 2.36657
Epoch: 1/20, step: 86140, training_loss: 1.49146
Epoch: 1/20, step: 86160, training_loss: 2.39713
Epoch: 1/20, step: 86180, training_loss: 1.89626
Epoch: 1/20, step: 86200, training_loss: 1.95090
Epoch: 1/20, step: 86220, training_loss: 1.77568
Epoch: 1/20, step: 86240, training_loss: 3.25277
Epoch: 1/20, step: 86260, training_loss: 2.11769
Epoch: 1/20, step: 86280, training_loss: 2.16292
Epoch: 1/20, step: 86300, training_loss: 1.64946
Epoch: 1/20, step: 86320, training_loss: 2.24564
Epoch: 1/20, step: 86340, training_loss: 2.78802
Epoch: 1/20, step: 86360, training_loss: 3.13698
Epoch: 1/20, step: 86380, training_loss: 2.16127
Epoch: 1/20, step: 86400, training_loss: 2.20412
Epoch: 1/20, step: 86420, training_loss: 2.61309
Epoch: 1/20, step: 86440, training_loss: 1.88523
Epoch: 1/20, step: 86460, training_loss: 2.29933
Epoch: 1/20, step: 86480, training_loss: 1.71916
Epoch: 1/20, step: 86500, training_loss: 2.56827
Epoch: 1/20, step: 86520, training_loss: 2.40558
Epoch: 1/20, step: 86540, training_loss: 1.58267
Epoch: 1/20, step: 86560, training_loss: 1.97079
Epoch: 1/20, step: 86580, training_loss: 2.89673
Epoch: 1/20, step: 86600, training_loss: 2.04965
Epoch: 1/20, step: 86620, training_loss: 2.90957
Epoch: 1/20, step: 86640, training_loss: 2.22770
Epoch: 1/20, step: 86660, training_loss: 3.11047
Epoch: 1/20, step: 86680, training_loss: 2.09461
Epoch: 1/20, step: 86700, training_loss: 2.60762
Epoch: 1/20, step: 86720, training_loss: 2.25674
Epoch: 1/20, step: 86740, training_loss: 2.54868
Epoch: 1/20, step: 86760, training_loss: 2.31130
Epoch: 1/20, step: 86780, training_loss: 2.89882
Epoch: 1/20, step: 86800, training_loss: 2.59241
Epoch: 1/20, step: 86820, training_loss: 3.34661
Epoch: 1/20, step: 86840, training_loss: 2.51789
Epoch: 1/20, step: 86860, training_loss: 1.98014
Epoch: 1/20, step: 86880, training_loss: 2.15846
Epoch: 1/20, step: 86900, training_loss: 2.59806
Epoch: 1/20, step: 86920, training_loss: 1.63710
Epoch: 1/20, step: 86940, training_loss: 3.11674
Epoch: 1/20, step: 86960, training_loss: 2.47819
Epoch: 1/20, step: 86980, training_loss: 2.55179
Epoch: 1/20, step: 87000, training_loss: 2.23724
accuracy: 0.36, validation_loss: 2.4340550899505615, num_samples: 100
Epoch: 1/20, step: 87020, training_loss: 1.90936
Epoch: 1/20, step: 87040, training_loss: 1.91283
Epoch: 1/20, step: 87060, training_loss: 2.12431
Epoch: 1/20, step: 87080, training_loss: 2.65627
Epoch: 1/20, step: 87100, training_loss: 3.69798
Epoch: 1/20, step: 87120, training_loss: 3.15135
Epoch: 1/20, step: 87140, training_loss: 1.96396
Epoch: 1/20, step: 87160, training_loss: 1.39090
Epoch: 1/20, step: 87180, training_loss: 2.34291
Epoch: 1/20, step: 87200, training_loss: 1.88482
Epoch: 1/20, step: 87220, training_loss: 1.90243
Epoch: 1/20, step: 87240, training_loss: 1.78573
Epoch: 1/20, step: 87260, training_loss: 2.24084
Epoch: 1/20, step: 87280, training_loss: 2.39064
Epoch: 1/20, step: 87300, training_loss: 2.49645
Epoch: 1/20, step: 87320, training_loss: 2.08999
Epoch: 1/20, step: 87340, training_loss: 1.98684
Epoch: 1/20, step: 87360, training_loss: 2.08259
Epoch: 1/20, step: 87380, training_loss: 1.98566
Epoch: 1/20, step: 87400, training_loss: 1.71336
Epoch: 1/20, step: 87420, training_loss: 1.97685
Epoch: 1/20, step: 87440, training_loss: 2.52138
Epoch: 1/20, step: 87460, training_loss: 1.37575
Epoch: 1/20, step: 87480, training_loss: 2.19072
Epoch: 1/20, step: 87500, training_loss: 2.87672
Epoch: 1/20, step: 87520, training_loss: 2.07042
Epoch: 1/20, step: 87540, training_loss: 2.01863
Epoch: 1/20, step: 87560, training_loss: 2.61029
Epoch: 1/20, step: 87580, training_loss: 2.52587
Epoch: 1/20, step: 87600, training_loss: 1.92953
Epoch: 1/20, step: 87620, training_loss: 1.99706
Epoch: 1/20, step: 87640, training_loss: 2.29226
Epoch: 1/20, step: 87660, training_loss: 2.32710
Epoch: 1/20, step: 87680, training_loss: 1.83945
Epoch: 1/20, step: 87700, training_loss: 2.49077
Epoch: 1/20, step: 87720, training_loss: 2.31984
Epoch: 1/20, step: 87740, training_loss: 2.48274
Epoch: 1/20, step: 87760, training_loss: 2.80645
Epoch: 1/20, step: 87780, training_loss: 2.12248
Epoch: 1/20, step: 87800, training_loss: 2.26798
Epoch: 1/20, step: 87820, training_loss: 1.39216
Epoch: 1/20, step: 87840, training_loss: 2.70939
Epoch: 1/20, step: 87860, training_loss: 2.16835
Epoch: 1/20, step: 87880, training_loss: 2.35786
Epoch: 1/20, step: 87900, training_loss: 1.87332
Epoch: 1/20, step: 87920, training_loss: 2.74610
Epoch: 1/20, step: 87940, training_loss: 2.19387
Epoch: 1/20, step: 87960, training_loss: 2.67986
Epoch: 1/20, step: 87980, training_loss: 2.66995
Epoch: 1/20, step: 88000, training_loss: 1.59246
accuracy: 0.36, validation_loss: 2.3459932804107666, num_samples: 100
Epoch: 1/20, step: 88020, training_loss: 1.87121
Epoch: 1/20, step: 88040, training_loss: 2.50066
Epoch: 1/20, step: 88060, training_loss: 3.42991
Epoch: 1/20, step: 88080, training_loss: 2.32772
Epoch: 1/20, step: 88100, training_loss: 1.74958
Epoch: 1/20, step: 88120, training_loss: 2.92477
Epoch: 1/20, step: 88140, training_loss: 2.17089
Epoch: 1/20, step: 88160, training_loss: 1.71924
Epoch: 1/20, step: 88180, training_loss: 2.28847
Epoch: 1/20, step: 88200, training_loss: 2.62746
Epoch: 1/20, step: 88220, training_loss: 2.05630
Epoch: 1/20, step: 88240, training_loss: 2.36546
Epoch: 1/20, step: 88260, training_loss: 2.53781
Epoch: 1/20, step: 88280, training_loss: 1.98787
Epoch: 1/20, step: 88300, training_loss: 1.94223
Epoch: 1/20, step: 88320, training_loss: 1.66424
Epoch: 1/20, step: 88340, training_loss: 2.26216
Epoch: 1/20, step: 88360, training_loss: 2.12521
Epoch: 1/20, step: 88380, training_loss: 2.37478
Epoch: 1/20, step: 88400, training_loss: 2.68935
Epoch: 1/20, step: 88420, training_loss: 2.03923
Epoch: 1/20, step: 88440, training_loss: 1.36467
Epoch: 1/20, step: 88460, training_loss: 1.96903
Epoch: 1/20, step: 88480, training_loss: 1.77645
Epoch: 1/20, step: 88500, training_loss: 2.55840
Epoch: 1/20, step: 88520, training_loss: 2.25469
Epoch: 1/20, step: 88540, training_loss: 2.08151
Epoch: 1/20, step: 88560, training_loss: 2.32806
Epoch: 1/20, step: 88580, training_loss: 1.78035
Epoch: 1/20, step: 88600, training_loss: 2.65699
Epoch: 1/20, step: 88620, training_loss: 1.74452
Epoch: 1/20, step: 88640, training_loss: 2.49757
Epoch: 1/20, step: 88660, training_loss: 2.42526
Epoch: 1/20, step: 88680, training_loss: 1.85349
Epoch: 1/20, step: 88700, training_loss: 2.37704
Epoch: 1/20, step: 88720, training_loss: 2.66690
Epoch: 1/20, step: 88740, training_loss: 2.65744
Epoch: 1/20, step: 88760, training_loss: 1.68920
Epoch: 1/20, step: 88780, training_loss: 2.30975
Epoch: 1/20, step: 88800, training_loss: 2.37147
Epoch: 1/20, step: 88820, training_loss: 2.74525
Epoch: 1/20, step: 88840, training_loss: 2.45492
Epoch: 1/20, step: 88860, training_loss: 1.58559
Epoch: 1/20, step: 88880, training_loss: 2.03292
Epoch: 1/20, step: 88900, training_loss: 2.04314
Epoch: 1/20, step: 88920, training_loss: 2.40023
Epoch: 1/20, step: 88940, training_loss: 1.56577
Epoch: 1/20, step: 88960, training_loss: 2.13754
Epoch: 1/20, step: 88980, training_loss: 3.43023
Epoch: 1/20, step: 89000, training_loss: 2.61743
accuracy: 0.4, validation_loss: 2.342540740966797, num_samples: 100
Epoch: 1/20, step: 89020, training_loss: 2.47360
Epoch: 1/20, step: 89040, training_loss: 2.37243
Epoch: 1/20, step: 89060, training_loss: 2.75016
Epoch: 1/20, step: 89080, training_loss: 2.15281
Epoch: 1/20, step: 89100, training_loss: 2.15269
Epoch: 1/20, step: 89120, training_loss: 2.95517
Epoch: 1/20, step: 89140, training_loss: 1.53227
Epoch: 1/20, step: 89160, training_loss: 2.74961
Epoch: 1/20, step: 89180, training_loss: 2.25047
Epoch: 1/20, step: 89200, training_loss: 1.84547
Epoch: 1/20, step: 89220, training_loss: 1.71846
Epoch: 1/20, step: 89240, training_loss: 2.34957
Epoch: 1/20, step: 89260, training_loss: 1.71903
Epoch: 1/20, step: 89280, training_loss: 1.79343
Epoch: 1/20, step: 89300, training_loss: 2.57789
Epoch: 1/20, step: 89320, training_loss: 2.58393
Epoch: 1/20, step: 89340, training_loss: 2.29146
Epoch: 1/20, step: 89360, training_loss: 2.03926
Epoch: 1/20, step: 89380, training_loss: 2.18276
Epoch: 1/20, step: 89400, training_loss: 1.58976
Epoch: 1/20, step: 89420, training_loss: 2.95971
Epoch: 1/20, step: 89440, training_loss: 2.18145
Epoch: 1/20, step: 89460, training_loss: 2.50120
Epoch: 1/20, step: 89480, training_loss: 2.79818
Epoch: 1/20, step: 89500, training_loss: 2.45715
Epoch: 1/20, step: 89520, training_loss: 2.79844
Epoch: 1/20, step: 89540, training_loss: 1.45709
Epoch: 1/20, step: 89560, training_loss: 2.71045
Epoch: 1/20, step: 89580, training_loss: 2.72727
Epoch: 1/20, step: 89600, training_loss: 2.27242
Epoch: 1/20, step: 89620, training_loss: 2.72567
Epoch: 1/20, step: 89640, training_loss: 2.14829
Epoch: 1/20, step: 89660, training_loss: 3.12183
Epoch: 1/20, step: 89680, training_loss: 3.17847
Epoch: 1/20, step: 89700, training_loss: 2.42365
Epoch: 1/20, step: 89720, training_loss: 1.59123
Epoch: 1/20, step: 89740, training_loss: 2.43744
Epoch: 1/20, step: 89760, training_loss: 2.28749
Epoch: 1/20, step: 89780, training_loss: 1.96264
Epoch: 1/20, step: 89800, training_loss: 2.88671
Epoch: 1/20, step: 89820, training_loss: 2.25132
Epoch: 1/20, step: 89840, training_loss: 3.00933
Epoch: 1/20, step: 89860, training_loss: 1.55273
Epoch: 1/20, step: 89880, training_loss: 1.58458
Epoch: 1/20, step: 89900, training_loss: 2.90428
Epoch: 1/20, step: 89920, training_loss: 2.80292
Epoch: 1/20, step: 89940, training_loss: 2.30345
Epoch: 1/20, step: 89960, training_loss: 2.44100
Epoch: 1/20, step: 89980, training_loss: 2.83609
Epoch: 1/20, step: 90000, training_loss: 1.56480
accuracy: 0.36, validation_loss: 2.4358534812927246, num_samples: 100
Epoch: 1/20, step: 90020, training_loss: 3.09195
Epoch: 1/20, step: 90040, training_loss: 2.44170
Epoch: 1/20, step: 90060, training_loss: 1.70199
Epoch: 1/20, step: 90080, training_loss: 1.20846
Epoch: 1/20, step: 90100, training_loss: 2.22868
Epoch: 1/20, step: 90120, training_loss: 2.75158
Epoch: 1/20, step: 90140, training_loss: 2.27264
Epoch: 1/20, step: 90160, training_loss: 2.31952
Epoch: 1/20, step: 90180, training_loss: 2.09130
Epoch: 1/20, step: 90200, training_loss: 1.78441
Epoch: 1/20, step: 90220, training_loss: 1.92792
Epoch: 1/20, step: 90240, training_loss: 1.96064
Epoch: 1/20, step: 90260, training_loss: 2.50200
Epoch: 1/20, step: 90280, training_loss: 2.44886
Epoch: 1/20, step: 90300, training_loss: 2.81576
Epoch: 1/20, step: 90320, training_loss: 2.71541
Epoch: 1/20, step: 90340, training_loss: 1.66525
Epoch: 1/20, step: 90360, training_loss: 2.79758
Epoch: 1/20, step: 90380, training_loss: 2.50776
Epoch: 1/20, step: 90400, training_loss: 1.61849
Epoch: 1/20, step: 90420, training_loss: 2.17401
Epoch: 1/20, step: 90440, training_loss: 1.73713
Epoch: 1/20, step: 90460, training_loss: 2.84006
Epoch: 1/20, step: 90480, training_loss: 2.06786
Epoch: 1/20, step: 90500, training_loss: 2.45978
Epoch: 1/20, step: 90520, training_loss: 1.80661
Epoch: 1/20, step: 90540, training_loss: 1.81523
Epoch: 1/20, step: 90560, training_loss: 1.11671
Epoch: 1/20, step: 90580, training_loss: 3.30580
Epoch: 1/20, step: 90600, training_loss: 2.82419
Epoch: 1/20, step: 90620, training_loss: 1.50666
Epoch: 1/20, step: 90640, training_loss: 2.23906
Epoch: 1/20, step: 90660, training_loss: 2.29291
Epoch: 1/20, step: 90680, training_loss: 1.80144
Epoch: 1/20, step: 90700, training_loss: 2.33199
Epoch: 1/20, step: 90720, training_loss: 2.30335
Epoch: 1/20, step: 90740, training_loss: 2.91457
Epoch: 1/20, step: 90760, training_loss: 1.96516
Epoch: 1/20, step: 90780, training_loss: 2.42760
Epoch: 1/20, step: 90800, training_loss: 2.09944
Epoch: 1/20, step: 90820, training_loss: 2.25047
Epoch: 1/20, step: 90840, training_loss: 2.93956
Epoch: 1/20, step: 90860, training_loss: 2.61955
Epoch: 1/20, step: 90880, training_loss: 2.07170
Epoch: 1/20, step: 90900, training_loss: 2.74175
Epoch: 1/20, step: 90920, training_loss: 1.93923
Epoch: 1/20, step: 90940, training_loss: 2.01577
Epoch: 1/20, step: 90960, training_loss: 2.13134
Epoch: 1/20, step: 90980, training_loss: 2.36882
Epoch: 1/20, step: 91000, training_loss: 1.74796
accuracy: 0.33, validation_loss: 2.3112380504608154, num_samples: 100
Epoch: 1/20, step: 91020, training_loss: 2.43405
Epoch: 1/20, step: 91040, training_loss: 2.13567
Epoch: 1/20, step: 91060, training_loss: 3.12506
Epoch: 1/20, step: 91080, training_loss: 1.82846
Epoch: 1/20, step: 91100, training_loss: 2.67822
Epoch: 1/20, step: 91120, training_loss: 2.20551
Epoch: 1/20, step: 91140, training_loss: 2.20133
Epoch: 1/20, step: 91160, training_loss: 1.98897
Epoch: 1/20, step: 91180, training_loss: 2.14384
Epoch: 1/20, step: 91200, training_loss: 2.05492
Epoch: 1/20, step: 91220, training_loss: 1.81196
Epoch: 1/20, step: 91240, training_loss: 2.14228
Epoch: 1/20, step: 91260, training_loss: 2.46082
Epoch: 1/20, step: 91280, training_loss: 2.43272
Epoch: 1/20, step: 91300, training_loss: 2.00109
Epoch: 1/20, step: 91320, training_loss: 2.21240
Epoch: 1/20, step: 91340, training_loss: 2.72516
Epoch: 1/20, step: 91360, training_loss: 1.89299
Epoch: 1/20, step: 91380, training_loss: 2.82810
Epoch: 1/20, step: 91400, training_loss: 1.94213
Epoch: 1/20, step: 91420, training_loss: 1.35275
Epoch: 1/20, step: 91440, training_loss: 2.20556
Epoch: 1/20, step: 91460, training_loss: 2.85571
Epoch: 1/20, step: 91480, training_loss: 2.23973
Epoch: 1/20, step: 91500, training_loss: 2.36867
Epoch: 1/20, step: 91520, training_loss: 2.61156
Epoch: 1/20, step: 91540, training_loss: 2.64327
Epoch: 1/20, step: 91560, training_loss: 2.83791
Epoch: 1/20, step: 91580, training_loss: 2.17287
Epoch: 1/20, step: 91600, training_loss: 2.62215
Epoch: 1/20, step: 91620, training_loss: 1.56493
Epoch: 1/20, step: 91640, training_loss: 2.14917
Epoch: 1/20, step: 91660, training_loss: 2.45332
Epoch: 1/20, step: 91680, training_loss: 2.63879
Epoch: 1/20, step: 91700, training_loss: 2.17857
Epoch: 1/20, step: 91720, training_loss: 2.93342
Epoch: 1/20, step: 91740, training_loss: 2.26598
Epoch: 1/20, step: 91760, training_loss: 1.84319
Epoch: 1/20, step: 91780, training_loss: 2.16690
Epoch: 1/20, step: 91800, training_loss: 1.95335
Epoch: 1/20, step: 91820, training_loss: 1.42586
Epoch: 1/20, step: 91840, training_loss: 2.72589
Epoch: 1/20, step: 91860, training_loss: 2.27734
Epoch: 1/20, step: 91880, training_loss: 1.84479
Epoch: 1/20, step: 91900, training_loss: 2.61900
Epoch: 1/20, step: 91920, training_loss: 3.10843
Epoch: 1/20, step: 91940, training_loss: 2.06088
Epoch: 1/20, step: 91960, training_loss: 2.70062
Epoch: 1/20, step: 91980, training_loss: 2.68616
Epoch: 1/20, step: 92000, training_loss: 2.30937
accuracy: 0.37, validation_loss: 2.494375705718994, num_samples: 100
Epoch: 1/20, step: 92020, training_loss: 2.02682
Epoch: 1/20, step: 92040, training_loss: 2.14384
Epoch: 1/20, step: 92060, training_loss: 1.95719
Epoch: 1/20, step: 92080, training_loss: 1.64861
Epoch: 1/20, step: 92100, training_loss: 1.72131
Epoch: 1/20, step: 92120, training_loss: 1.90003
Epoch: 1/20, step: 92140, training_loss: 2.22985
Epoch: 1/20, step: 92160, training_loss: 2.78179
Epoch: 1/20, step: 92180, training_loss: 1.72872
Epoch: 1/20, step: 92200, training_loss: 1.86596
Epoch: 1/20, step: 92220, training_loss: 2.14467
Epoch: 1/20, step: 92240, training_loss: 2.68614
Epoch: 1/20, step: 92260, training_loss: 2.48938
Epoch: 1/20, step: 92280, training_loss: 1.92730
Epoch: 1/20, step: 92300, training_loss: 1.86243
Epoch: 1/20, step: 92320, training_loss: 3.59522
Epoch: 1/20, step: 92340, training_loss: 2.01239
Epoch: 1/20, step: 92360, training_loss: 1.75282
Epoch: 1/20, step: 92380, training_loss: 1.76468
Epoch: 1/20, step: 92400, training_loss: 2.59688
Epoch: 1/20, step: 92420, training_loss: 2.36608
Epoch: 1/20, step: 92440, training_loss: 2.80463
Epoch: 1/20, step: 92460, training_loss: 2.38157
Epoch: 1/20, step: 92480, training_loss: 2.73935
Epoch: 1/20, step: 92500, training_loss: 1.61923
Epoch: 1/20, step: 92520, training_loss: 2.94138
Epoch: 1/20, step: 92540, training_loss: 2.84923
Epoch: 1/20, step: 92560, training_loss: 2.12739
Epoch: 1/20, step: 92580, training_loss: 2.21957
Epoch: 1/20, step: 92600, training_loss: 2.45161
Epoch: 1/20, step: 92620, training_loss: 1.81196
Epoch: 1/20, step: 92640, training_loss: 1.82025
Epoch: 1/20, step: 92660, training_loss: 2.28118
Epoch: 1/20, step: 92680, training_loss: 2.05511
Epoch: 1/20, step: 92700, training_loss: 3.46005
Epoch: 1/20, step: 92720, training_loss: 2.39766
Epoch: 1/20, step: 92740, training_loss: 2.44334
Epoch: 1/20, step: 92760, training_loss: 2.20738
Epoch: 1/20, step: 92780, training_loss: 1.91909
Epoch: 1/20, step: 92800, training_loss: 2.66217
Epoch: 1/20, step: 92820, training_loss: 2.32609
Epoch: 1/20, step: 92840, training_loss: 2.12497
Epoch: 1/20, step: 92860, training_loss: 2.61621
Epoch: 1/20, step: 92880, training_loss: 2.72174
Epoch: 1/20, step: 92900, training_loss: 3.23720
Epoch: 1/20, step: 92920, training_loss: 1.67092
Epoch: 1/20, step: 92940, training_loss: 2.67504
Epoch: 1/20, step: 92960, training_loss: 2.04761
Epoch: 1/20, step: 92980, training_loss: 2.42152
Epoch: 1/20, step: 93000, training_loss: 2.26696
accuracy: 0.25, validation_loss: 2.651171922683716, num_samples: 100
Epoch: 1/20, step: 93020, training_loss: 1.77799
Epoch: 1/20, step: 93040, training_loss: 1.80412
Epoch: 1/20, step: 93060, training_loss: 2.13441
Epoch: 1/20, step: 93080, training_loss: 2.36839
Epoch: 1/20, step: 93100, training_loss: 1.53247
Epoch: 1/20, step: 93120, training_loss: 2.63084
Epoch: 1/20, step: 93140, training_loss: 1.80574
Epoch: 1/20, step: 93160, training_loss: 2.08543
Epoch: 1/20, step: 93180, training_loss: 2.16937
Epoch: 1/20, step: 93200, training_loss: 2.44574
Epoch: 1/20, step: 93220, training_loss: 2.57446
Epoch: 1/20, step: 93240, training_loss: 1.87773
Epoch: 1/20, step: 93260, training_loss: 2.36018
Epoch: 1/20, step: 93280, training_loss: 1.37828
Epoch: 1/20, step: 93300, training_loss: 2.25314
Epoch: 1/20, step: 93320, training_loss: 1.75208
Epoch: 1/20, step: 93340, training_loss: 2.29506
Epoch: 1/20, step: 93360, training_loss: 2.54405
Epoch: 1/20, step: 93380, training_loss: 2.50170
Epoch: 1/20, step: 93400, training_loss: 2.50503
Epoch: 1/20, step: 93420, training_loss: 2.55674
Epoch: 1/20, step: 93440, training_loss: 2.27634
Epoch: 1/20, step: 93460, training_loss: 1.96420
Epoch: 1/20, step: 93480, training_loss: 1.95262
Epoch: 1/20, step: 93500, training_loss: 2.63141
Epoch: 1/20, step: 93520, training_loss: 1.38776
Epoch: 1/20, step: 93540, training_loss: 2.04179
Epoch: 1/20, step: 93560, training_loss: 1.97325
Epoch: 1/20, step: 93580, training_loss: 0.85371
Epoch: 1/20, step: 93600, training_loss: 2.26696
Epoch: 1/20, step: 93620, training_loss: 1.64071
Epoch: 1/20, step: 93640, training_loss: 1.86584
Epoch: 1/20, step: 93660, training_loss: 2.61402
Epoch: 1/20, step: 93680, training_loss: 1.34357
Epoch: 1/20, step: 93700, training_loss: 2.64689
Epoch: 1/20, step: 93720, training_loss: 1.84913
Epoch: 1/20, step: 93740, training_loss: 2.85720
Epoch: 1/20, step: 93760, training_loss: 1.99173
Epoch: 1/20, step: 93780, training_loss: 2.79656
Epoch: 1/20, step: 93800, training_loss: 1.40209
Epoch: 1/20, step: 93820, training_loss: 2.01201
Epoch: 1/20, step: 93840, training_loss: 2.29067
Epoch: 1/20, step: 93860, training_loss: 2.61903
Epoch: 1/20, step: 93880, training_loss: 2.39365
Epoch: 1/20, step: 93900, training_loss: 2.18966
Epoch: 1/20, step: 93920, training_loss: 2.18038
Epoch: 1/20, step: 93940, training_loss: 2.39272
Epoch: 1/20, step: 93960, training_loss: 2.03087
Epoch: 1/20, step: 93980, training_loss: 1.71288
Epoch: 1/20, step: 94000, training_loss: 2.39094
accuracy: 0.44, validation_loss: 1.985897183418274, num_samples: 100
Epoch: 1/20, step: 94020, training_loss: 2.21626
Epoch: 1/20, step: 94040, training_loss: 2.50080
Epoch: 1/20, step: 94060, training_loss: 2.08850
Epoch: 1/20, step: 94080, training_loss: 2.05410
Epoch: 1/20, step: 94100, training_loss: 1.73897
Epoch: 1/20, step: 94120, training_loss: 1.57457
Epoch: 1/20, step: 94140, training_loss: 3.17702
Epoch: 1/20, step: 94160, training_loss: 1.60184
Epoch: 1/20, step: 94180, training_loss: 2.68105
Epoch: 1/20, step: 94200, training_loss: 2.15561
Epoch: 1/20, step: 94220, training_loss: 1.55113
Epoch: 1/20, step: 94240, training_loss: 1.99862
Epoch: 1/20, step: 94260, training_loss: 2.59572
Epoch: 1/20, step: 94280, training_loss: 3.15237
Epoch: 1/20, step: 94300, training_loss: 2.23607
Epoch: 1/20, step: 94320, training_loss: 2.59301
Epoch: 1/20, step: 94340, training_loss: 2.24973
Epoch: 1/20, step: 94360, training_loss: 1.90716
Epoch: 1/20, step: 94380, training_loss: 2.03311
Epoch: 1/20, step: 94400, training_loss: 2.01423
Epoch: 1/20, step: 94420, training_loss: 2.02447
Epoch: 1/20, step: 94440, training_loss: 1.97031
Epoch: 1/20, step: 94460, training_loss: 2.37741
Epoch: 1/20, step: 94480, training_loss: 1.19669
Epoch: 1/20, step: 94500, training_loss: 2.24655
Epoch: 1/20, step: 94520, training_loss: 2.10780
Epoch: 1/20, step: 94540, training_loss: 2.28313
Epoch: 1/20, step: 94560, training_loss: 1.79342
Epoch: 1/20, step: 94580, training_loss: 1.80187
Epoch: 1/20, step: 94600, training_loss: 1.36566
Epoch: 1/20, step: 94620, training_loss: 2.87917
Epoch: 1/20, step: 94640, training_loss: 2.04935
Epoch: 1/20, step: 94660, training_loss: 2.47645
Epoch: 1/20, step: 94680, training_loss: 3.23031
Epoch: 1/20, step: 94700, training_loss: 1.42872
Epoch: 1/20, step: 94720, training_loss: 2.60669
Epoch: 1/20, step: 94740, training_loss: 2.44885
Epoch: 1/20, step: 94760, training_loss: 2.81631
Epoch: 1/20, step: 94780, training_loss: 1.75583
Epoch: 1/20, step: 94800, training_loss: 2.33713
Epoch: 1/20, step: 94820, training_loss: 2.22530
Epoch: 1/20, step: 94840, training_loss: 1.95166
Epoch: 1/20, step: 94860, training_loss: 1.73621
Epoch: 1/20, step: 94880, training_loss: 2.31157
Epoch: 1/20, step: 94900, training_loss: 2.29760
Epoch: 1/20, step: 94920, training_loss: 2.38791
Epoch: 1/20, step: 94940, training_loss: 1.43740
Epoch: 1/20, step: 94960, training_loss: 2.28080
Epoch: 1/20, step: 94980, training_loss: 1.62021
Epoch: 1/20, step: 95000, training_loss: 2.69820
accuracy: 0.37, validation_loss: 2.074204683303833, num_samples: 100
Epoch: 1/20, step: 95020, training_loss: 2.97692
Epoch: 1/20, step: 95040, training_loss: 1.50353
Epoch: 1/20, step: 95060, training_loss: 2.02197
Epoch: 1/20, step: 95080, training_loss: 2.36997
Epoch: 1/20, step: 95100, training_loss: 2.55316
Epoch: 1/20, step: 95120, training_loss: 1.60387
Epoch: 1/20, step: 95140, training_loss: 2.52402
Epoch: 1/20, step: 95160, training_loss: 1.64751
Epoch: 1/20, step: 95180, training_loss: 2.56426
Epoch: 1/20, step: 95200, training_loss: 2.50804
Epoch: 1/20, step: 95220, training_loss: 2.36125
Epoch: 1/20, step: 95240, training_loss: 2.29499
Epoch: 1/20, step: 95260, training_loss: 2.34587
Epoch: 1/20, step: 95280, training_loss: 3.01472
Epoch: 1/20, step: 95300, training_loss: 2.33463
Epoch: 1/20, step: 95320, training_loss: 1.38628
Epoch: 1/20, step: 95340, training_loss: 2.23905
Epoch: 1/20, step: 95360, training_loss: 2.01282
Epoch: 1/20, step: 95380, training_loss: 2.31467
Epoch: 1/20, step: 95400, training_loss: 2.40557
Epoch: 1/20, step: 95420, training_loss: 1.53434
Epoch: 1/20, step: 95440, training_loss: 1.88324
Epoch: 1/20, step: 95460, training_loss: 2.64295
Epoch: 1/20, step: 95480, training_loss: 1.75927
Epoch: 1/20, step: 95500, training_loss: 2.38241
Epoch: 1/20, step: 95520, training_loss: 2.20350
Epoch: 1/20, step: 95540, training_loss: 2.43914
Epoch: 1/20, step: 95560, training_loss: 2.26180
Epoch: 1/20, step: 95580, training_loss: 1.55239
Epoch: 1/20, step: 95600, training_loss: 2.07514
Epoch: 1/20, step: 95620, training_loss: 2.34233
Epoch: 1/20, step: 95640, training_loss: 1.82179
Epoch: 1/20, step: 95660, training_loss: 1.74473
Epoch: 1/20, step: 95680, training_loss: 2.56108
Epoch: 1/20, step: 95700, training_loss: 1.68175
Epoch: 1/20, step: 95720, training_loss: 2.43801
Epoch: 1/20, step: 95740, training_loss: 2.61994
Epoch: 1/20, step: 95760, training_loss: 2.26837
Epoch: 1/20, step: 95780, training_loss: 2.77918
Epoch: 1/20, step: 95800, training_loss: 1.37517
Epoch: 1/20, step: 95820, training_loss: 2.79391
Epoch: 1/20, step: 95840, training_loss: 2.19259
Epoch: 1/20, step: 95860, training_loss: 1.94197
Epoch: 1/20, step: 95880, training_loss: 2.50832
Epoch: 1/20, step: 95900, training_loss: 3.08342
Epoch: 1/20, step: 95920, training_loss: 2.05487
Epoch: 1/20, step: 95940, training_loss: 1.72216
Epoch: 1/20, step: 95960, training_loss: 2.02436
Epoch: 1/20, step: 95980, training_loss: 2.45602
Epoch: 1/20, step: 96000, training_loss: 2.39585
accuracy: 0.32, validation_loss: 2.2452919483184814, num_samples: 100
Epoch: 1/20, step: 96020, training_loss: 2.75968
Epoch: 1/20, step: 96040, training_loss: 2.27513
Epoch: 1/20, step: 96060, training_loss: 2.59541
Epoch: 1/20, step: 96080, training_loss: 2.39236
Epoch: 1/20, step: 96100, training_loss: 1.84485
Epoch: 1/20, step: 96120, training_loss: 2.85782
Epoch: 1/20, step: 96140, training_loss: 1.08893
Epoch: 1/20, step: 96160, training_loss: 2.00200
Epoch: 1/20, step: 96180, training_loss: 2.48247
Epoch: 1/20, step: 96200, training_loss: 1.88424
Epoch: 1/20, step: 96220, training_loss: 2.70038
Epoch: 1/20, step: 96240, training_loss: 1.53939
Epoch: 1/20, step: 96260, training_loss: 3.26311
Epoch: 1/20, step: 96280, training_loss: 2.39082
Epoch: 1/20, step: 96300, training_loss: 2.96893
Epoch: 1/20, step: 96320, training_loss: 2.31851
Epoch: 1/20, step: 96340, training_loss: 2.44189
Epoch: 1/20, step: 96360, training_loss: 2.49570
Epoch: 1/20, step: 96380, training_loss: 2.61530
Epoch: 1/20, step: 96400, training_loss: 2.05168
Epoch: 1/20, step: 96420, training_loss: 2.28450
Epoch: 1/20, step: 96440, training_loss: 2.16841
Epoch: 1/20, step: 96460, training_loss: 2.38267
Epoch: 1/20, step: 96480, training_loss: 1.51384
Epoch: 1/20, step: 96500, training_loss: 1.98791
Epoch: 1/20, step: 96520, training_loss: 2.08314
Epoch: 1/20, step: 96540, training_loss: 2.90728
Epoch: 1/20, step: 96560, training_loss: 2.29540
Epoch: 1/20, step: 96580, training_loss: 2.04006
Epoch: 1/20, step: 96600, training_loss: 2.80177
Epoch: 1/20, step: 96620, training_loss: 2.65376
Epoch: 1/20, step: 96640, training_loss: 2.61332
Epoch: 1/20, step: 96660, training_loss: 1.77119
Epoch: 1/20, step: 96680, training_loss: 2.60445
Epoch: 1/20, step: 96700, training_loss: 2.55508
Epoch: 1/20, step: 96720, training_loss: 2.31917
Epoch: 1/20, step: 96740, training_loss: 2.22659
Epoch: 1/20, step: 96760, training_loss: 2.62828
Epoch: 1/20, step: 96780, training_loss: 1.68818
Epoch: 1/20, step: 96800, training_loss: 2.94281
Epoch: 1/20, step: 96820, training_loss: 1.76771
Epoch: 1/20, step: 96840, training_loss: 3.51639
Epoch: 1/20, step: 96860, training_loss: 3.21233
Epoch: 1/20, step: 96880, training_loss: 2.12515
Epoch: 1/20, step: 96900, training_loss: 2.09580
Epoch: 1/20, step: 96920, training_loss: 2.83275
Epoch: 1/20, step: 96940, training_loss: 1.58096
Epoch: 1/20, step: 96960, training_loss: 1.35020
Epoch: 1/20, step: 96980, training_loss: 1.84565
Epoch: 1/20, step: 97000, training_loss: 1.96472
accuracy: 0.31, validation_loss: 2.5029959678649902, num_samples: 100
Epoch: 1/20, step: 97020, training_loss: 1.86371
Epoch: 1/20, step: 97040, training_loss: 1.81656
Epoch: 1/20, step: 97060, training_loss: 2.35053
Epoch: 1/20, step: 97080, training_loss: 1.78409
Epoch: 1/20, step: 97100, training_loss: 1.50405
Epoch: 1/20, step: 97120, training_loss: 3.06062
Epoch: 1/20, step: 97140, training_loss: 2.09131
Epoch: 1/20, step: 97160, training_loss: 2.44003
Epoch: 1/20, step: 97180, training_loss: 2.14015
Epoch: 1/20, step: 97200, training_loss: 3.01515
Epoch: 1/20, step: 97220, training_loss: 1.89842
Epoch: 1/20, step: 97240, training_loss: 2.45027
Epoch: 1/20, step: 97260, training_loss: 2.75769
Epoch: 1/20, step: 97280, training_loss: 1.51227
Epoch: 1/20, step: 97300, training_loss: 2.48091
Epoch: 1/20, step: 97320, training_loss: 2.31066
Epoch: 1/20, step: 97340, training_loss: 2.42825
Epoch: 1/20, step: 97360, training_loss: 1.21372
Epoch: 1/20, step: 97380, training_loss: 2.27963
Epoch: 1/20, step: 97400, training_loss: 1.97435
Epoch: 1/20, step: 97420, training_loss: 2.62060
Epoch: 1/20, step: 97440, training_loss: 1.66976
Epoch: 1/20, step: 97460, training_loss: 2.05924
Epoch: 1/20, step: 97480, training_loss: 1.78407
Epoch: 1/20, step: 97500, training_loss: 2.69863
Epoch: 1/20, step: 97520, training_loss: 2.08669
Epoch: 1/20, step: 97540, training_loss: 2.89509
Epoch: 1/20, step: 97560, training_loss: 2.58526
Epoch: 1/20, step: 97580, training_loss: 2.27292
Epoch: 1/20, step: 97600, training_loss: 1.83747
Epoch: 1/20, step: 97620, training_loss: 2.12439
Epoch: 1/20, step: 97640, training_loss: 1.80192
Epoch: 1/20, step: 97660, training_loss: 2.74176
Epoch: 1/20, step: 97680, training_loss: 1.90809
Epoch: 1/20, step: 97700, training_loss: 2.56766
Epoch: 1/20, step: 97720, training_loss: 2.64292
Epoch: 1/20, step: 97740, training_loss: 2.86532
Epoch: 1/20, step: 97760, training_loss: 1.89925
Epoch: 1/20, step: 97780, training_loss: 2.22880
Epoch: 1/20, step: 97800, training_loss: 1.65002
Epoch: 1/20, step: 97820, training_loss: 2.34745
Epoch: 1/20, step: 97840, training_loss: 1.80544
Epoch: 1/20, step: 97860, training_loss: 2.94342
Epoch: 1/20, step: 97880, training_loss: 2.43894
Epoch: 1/20, step: 97900, training_loss: 2.42321
Epoch: 1/20, step: 97920, training_loss: 2.00206
Epoch: 1/20, step: 97940, training_loss: 2.50059
Epoch: 1/20, step: 97960, training_loss: 1.92777
Epoch: 1/20, step: 97980, training_loss: 1.56635
Epoch: 1/20, step: 98000, training_loss: 1.12541
accuracy: 0.41, validation_loss: 2.2958779335021973, num_samples: 100
Epoch: 1/20, step: 98020, training_loss: 2.90602
Epoch: 1/20, step: 98040, training_loss: 2.44426
Epoch: 1/20, step: 98060, training_loss: 2.10758
Epoch: 1/20, step: 98080, training_loss: 1.81644
Epoch: 1/20, step: 98100, training_loss: 2.16295
Epoch: 1/20, step: 98120, training_loss: 2.92874
Epoch: 1/20, step: 98140, training_loss: 2.11444
Epoch: 1/20, step: 98160, training_loss: 1.26662
Epoch: 1/20, step: 98180, training_loss: 1.77670
Epoch: 1/20, step: 98200, training_loss: 1.87566
Epoch: 1/20, step: 98220, training_loss: 2.58557
Epoch: 1/20, step: 98240, training_loss: 2.37007
Epoch: 1/20, step: 98260, training_loss: 2.22210
Epoch: 1/20, step: 98280, training_loss: 1.92958
Epoch: 1/20, step: 98300, training_loss: 2.72387
Epoch: 1/20, step: 98320, training_loss: 2.62836
Epoch: 1/20, step: 98340, training_loss: 1.67505
Epoch: 1/20, step: 98360, training_loss: 2.60060
Epoch: 1/20, step: 98380, training_loss: 2.57120
Epoch: 1/20, step: 98400, training_loss: 2.01115
Epoch: 1/20, step: 98420, training_loss: 1.74427
Epoch: 1/20, step: 98440, training_loss: 2.06481
Epoch: 1/20, step: 98460, training_loss: 2.15320
Epoch: 1/20, step: 98480, training_loss: 1.83557
Epoch: 1/20, step: 98500, training_loss: 1.80785
Epoch: 1/20, step: 98520, training_loss: 2.54494
Epoch: 1/20, step: 98540, training_loss: 1.80682
Epoch: 1/20, step: 98560, training_loss: 2.06899
Epoch: 1/20, step: 98580, training_loss: 1.75107
Epoch: 1/20, step: 98600, training_loss: 2.32454
Epoch: 1/20, step: 98620, training_loss: 1.54754
Epoch: 1/20, step: 98640, training_loss: 1.59552
Epoch: 1/20, step: 98660, training_loss: 2.37429
Epoch: 1/20, step: 98680, training_loss: 2.03606
Epoch: 1/20, step: 98700, training_loss: 2.50467
Epoch: 1/20, step: 98720, training_loss: 2.47042
Epoch: 1/20, step: 98740, training_loss: 2.41937
Epoch: 1/20, step: 98760, training_loss: 2.47933
Epoch: 1/20, step: 98780, training_loss: 1.63784
Epoch: 1/20, step: 98800, training_loss: 2.31327
Epoch: 1/20, step: 98820, training_loss: 3.02372
Epoch: 1/20, step: 98840, training_loss: 2.24507
Epoch: 1/20, step: 98860, training_loss: 2.23509
Epoch: 1/20, step: 98880, training_loss: 2.49631
Epoch: 1/20, step: 98900, training_loss: 1.87191
Epoch: 1/20, step: 98920, training_loss: 1.27591
Epoch: 1/20, step: 98940, training_loss: 1.20134
Epoch: 1/20, step: 98960, training_loss: 2.23448
Epoch: 1/20, step: 98980, training_loss: 1.94087
Epoch: 1/20, step: 99000, training_loss: 2.59239
accuracy: 0.43, validation_loss: 2.22761607170105, num_samples: 100
Epoch: 1/20, step: 99020, training_loss: 2.17174
Epoch: 1/20, step: 99040, training_loss: 1.90463
Epoch: 1/20, step: 99060, training_loss: 1.98859
Epoch: 1/20, step: 99080, training_loss: 3.09359
Epoch: 1/20, step: 99100, training_loss: 1.92103
Epoch: 1/20, step: 99120, training_loss: 2.33203
Epoch: 1/20, step: 99140, training_loss: 2.58169
Epoch: 1/20, step: 99160, training_loss: 1.94049
Epoch: 1/20, step: 99180, training_loss: 2.64336
Epoch: 1/20, step: 99200, training_loss: 2.79228
Epoch: 1/20, step: 99220, training_loss: 1.92461
Epoch: 1/20, step: 99240, training_loss: 2.52185
Epoch: 1/20, step: 99260, training_loss: 2.69949
Epoch: 1/20, step: 99280, training_loss: 2.39950
Epoch: 1/20, step: 99300, training_loss: 2.17083
Epoch: 1/20, step: 99320, training_loss: 3.19805
Epoch: 1/20, step: 99340, training_loss: 2.46189
Epoch: 1/20, step: 99360, training_loss: 2.12328
Epoch: 1/20, step: 99380, training_loss: 1.91414
Epoch: 1/20, step: 99400, training_loss: 2.69742
Epoch: 1/20, step: 99420, training_loss: 2.33490
Epoch: 1/20, step: 99440, training_loss: 2.89479
Epoch: 1/20, step: 99460, training_loss: 2.69545
Epoch: 1/20, step: 99480, training_loss: 3.12581
Epoch: 1/20, step: 99500, training_loss: 2.98752
Epoch: 1/20, step: 99520, training_loss: 2.61599
Epoch: 1/20, step: 99540, training_loss: 1.87017
Epoch: 1/20, step: 99560, training_loss: 2.10251
Epoch: 1/20, step: 99580, training_loss: 1.69208
Epoch: 1/20, step: 99600, training_loss: 1.59223
Epoch: 1/20, step: 99620, training_loss: 2.32386
Epoch: 1/20, step: 99640, training_loss: 2.43449
Epoch: 1/20, step: 99660, training_loss: 2.07863
Epoch: 1/20, step: 99680, training_loss: 1.85662
Epoch: 1/20, step: 99700, training_loss: 2.17168
Epoch: 1/20, step: 99720, training_loss: 2.49670
Epoch: 1/20, step: 99740, training_loss: 2.48564
Epoch: 1/20, step: 99760, training_loss: 2.14263
Epoch: 1/20, step: 99780, training_loss: 2.58602
Epoch: 1/20, step: 99800, training_loss: 2.51746
Epoch: 1/20, step: 99820, training_loss: 2.37129
Epoch: 1/20, step: 99840, training_loss: 2.42815
Epoch: 1/20, step: 99860, training_loss: 2.80174
Epoch: 1/20, step: 99880, training_loss: 2.20721
Epoch: 1/20, step: 99900, training_loss: 1.85904
Epoch: 1/20, step: 99920, training_loss: 2.55614
Epoch: 1/20, step: 99940, training_loss: 3.19342
Epoch: 1/20, step: 99960, training_loss: 2.48717
Epoch: 1/20, step: 99980, training_loss: 2.25782
Epoch: 1/20, step: 100000, training_loss: 2.17845
accuracy: 0.45, validation_loss: 1.933475375175476, num_samples: 100
Epoch: 1/20, step: 100020, training_loss: 2.17480
Epoch: 1/20, step: 100040, training_loss: 2.03121
Epoch: 1/20, step: 100060, training_loss: 2.75312
Epoch: 1/20, step: 100080, training_loss: 1.63937
Epoch: 1/20, step: 100100, training_loss: 2.43671
Epoch: 1/20, step: 100120, training_loss: 2.30730
Epoch: 1/20, step: 100140, training_loss: 1.71593
Epoch: 1/20, step: 100160, training_loss: 1.86365
Epoch: 1/20, step: 100180, training_loss: 1.60313
Epoch: 1/20, step: 100200, training_loss: 1.73308
Epoch: 1/20, step: 100220, training_loss: 1.65143
Epoch: 1/20, step: 100240, training_loss: 1.38250
Epoch: 1/20, step: 100260, training_loss: 1.99384
Epoch: 1/20, step: 100280, training_loss: 2.15903
Epoch: 1/20, step: 100300, training_loss: 1.55650
Epoch: 1/20, step: 100320, training_loss: 1.83661
Epoch: 1/20, step: 100340, training_loss: 1.84127
Epoch: 1/20, step: 100360, training_loss: 1.82963
Epoch: 1/20, step: 100380, training_loss: 2.50044
Epoch: 1/20, step: 100400, training_loss: 1.89206
Epoch: 1/20, step: 100420, training_loss: 2.63959
Epoch: 1/20, step: 100440, training_loss: 1.86003
Epoch: 1/20, step: 100460, training_loss: 1.39887
Epoch: 1/20, step: 100480, training_loss: 1.63430
Epoch: 1/20, step: 100500, training_loss: 2.50738
Epoch: 1/20, step: 100520, training_loss: 2.49251
Epoch: 1/20, step: 100540, training_loss: 1.50549
Epoch: 1/20, step: 100560, training_loss: 2.11379
Epoch: 1/20, step: 100580, training_loss: 2.21664
Epoch: 1/20, step: 100600, training_loss: 3.05057
Epoch: 1/20, step: 100620, training_loss: 2.48778
Epoch: 1/20, step: 100640, training_loss: 2.48111
Epoch: 1/20, step: 100660, training_loss: 1.44529
Epoch: 1/20, step: 100680, training_loss: 3.02610
Epoch: 1/20, step: 100700, training_loss: 2.47605
Epoch: 1/20, step: 100720, training_loss: 2.36253
Epoch: 1/20, step: 100740, training_loss: 2.18832
Epoch: 1/20, step: 100760, training_loss: 2.62220
Epoch: 1/20, step: 100780, training_loss: 2.44639
Epoch: 1/20, step: 100800, training_loss: 2.10089
Epoch: 1/20, step: 100820, training_loss: 2.69753
Epoch: 1/20, step: 100840, training_loss: 2.10734
Epoch: 1/20, step: 100860, training_loss: 2.72878
Epoch: 1/20, step: 100880, training_loss: 2.07357
Epoch: 1/20, step: 100900, training_loss: 2.44009
Epoch: 1/20, step: 100920, training_loss: 2.42520
Epoch: 1/20, step: 100940, training_loss: 2.51875
Epoch: 1/20, step: 100960, training_loss: 1.42576
Epoch: 1/20, step: 100980, training_loss: 2.69696
Epoch: 1/20, step: 101000, training_loss: 2.18162
accuracy: 0.41, validation_loss: 2.2606563568115234, num_samples: 100
Epoch: 1/20, step: 101020, training_loss: 2.68496
Epoch: 1/20, step: 101040, training_loss: 2.19921
Epoch: 1/20, step: 101060, training_loss: 2.64605
Epoch: 1/20, step: 101080, training_loss: 2.39401
Epoch: 1/20, step: 101100, training_loss: 1.98674
Epoch: 1/20, step: 101120, training_loss: 1.90997
Epoch: 1/20, step: 101140, training_loss: 1.52668
Epoch: 1/20, step: 101160, training_loss: 2.28195
Epoch: 1/20, step: 101180, training_loss: 2.42946
Epoch: 1/20, step: 101200, training_loss: 1.78863
Epoch: 1/20, step: 101220, training_loss: 2.35842
Epoch: 1/20, step: 101240, training_loss: 2.06044
Epoch: 1/20, step: 101260, training_loss: 1.55833
Epoch: 1/20, step: 101280, training_loss: 2.02662
Epoch: 1/20, step: 101300, training_loss: 1.38737
Epoch: 1/20, step: 101320, training_loss: 1.71649
Epoch: 1/20, step: 101340, training_loss: 2.27152
Epoch: 1/20, step: 101360, training_loss: 2.87814
Epoch: 1/20, step: 101380, training_loss: 2.28966
Epoch: 1/20, step: 101400, training_loss: 2.03639
Epoch: 1/20, step: 101420, training_loss: 1.85661
Epoch: 1/20, step: 101440, training_loss: 2.45243
Epoch: 1/20, step: 101460, training_loss: 2.56636
Epoch: 1/20, step: 101480, training_loss: 2.49604
Epoch: 1/20, step: 101500, training_loss: 1.84383
Epoch: 1/20, step: 101520, training_loss: 2.18639
Epoch: 1/20, step: 101540, training_loss: 1.55849
Epoch: 1/20, step: 101560, training_loss: 1.93289
Epoch: 1/20, step: 101580, training_loss: 3.11990
Epoch: 1/20, step: 101600, training_loss: 2.53646
Epoch: 1/20, step: 101620, training_loss: 2.31725
Epoch: 1/20, step: 101640, training_loss: 1.93966
Epoch: 1/20, step: 101660, training_loss: 2.42057
Epoch: 1/20, step: 101680, training_loss: 2.12810
Epoch: 1/20, step: 101700, training_loss: 2.40826
Epoch: 1/20, step: 101720, training_loss: 2.64351
Epoch: 1/20, step: 101740, training_loss: 2.21069
Epoch: 1/20, step: 101760, training_loss: 1.53618
Epoch: 1/20, step: 101780, training_loss: 2.67432
Epoch: 1/20, step: 101800, training_loss: 1.77661
Epoch: 1/20, step: 101820, training_loss: 1.29161
Epoch: 1/20, step: 101840, training_loss: 2.64250
Epoch: 1/20, step: 101860, training_loss: 2.96503
Epoch: 1/20, step: 101880, training_loss: 2.97136
Epoch: 1/20, step: 101900, training_loss: 0.97243
Epoch: 1/20, step: 101920, training_loss: 2.63663
Epoch: 1/20, step: 101940, training_loss: 1.88582
Epoch: 1/20, step: 101960, training_loss: 3.10974
Epoch: 1/20, step: 101980, training_loss: 1.94352
Epoch: 1/20, step: 102000, training_loss: 2.59552
accuracy: 0.39, validation_loss: 2.5014967918395996, num_samples: 100
Epoch: 1/20, step: 102020, training_loss: 1.69291
Epoch: 1/20, step: 102040, training_loss: 1.67812
Epoch: 1/20, step: 102060, training_loss: 1.85011
Epoch: 1/20, step: 102080, training_loss: 2.19186
Epoch: 1/20, step: 102100, training_loss: 2.24405
Epoch: 1/20, step: 102120, training_loss: 2.00427
Epoch: 1/20, step: 102140, training_loss: 2.00008
Epoch: 1/20, step: 102160, training_loss: 2.27408
Epoch: 1/20, step: 102180, training_loss: 2.28472
Epoch: 1/20, step: 102200, training_loss: 1.82097
Epoch: 1/20, step: 102220, training_loss: 2.39671
Epoch: 1/20, step: 102240, training_loss: 2.12739
Epoch: 1/20, step: 102260, training_loss: 2.26140
Epoch: 1/20, step: 102280, training_loss: 1.82665
Epoch: 1/20, step: 102300, training_loss: 2.75359
Epoch: 1/20, step: 102320, training_loss: 2.43540
Epoch: 1/20, step: 102340, training_loss: 2.02911
Epoch: 1/20, step: 102360, training_loss: 3.02442
Epoch: 1/20, step: 102380, training_loss: 3.10209
Epoch: 1/20, step: 102400, training_loss: 2.59349
Epoch: 1/20, step: 102420, training_loss: 2.74837
Epoch: 1/20, step: 102440, training_loss: 1.98186
Epoch: 1/20, step: 102460, training_loss: 2.68288
Epoch: 1/20, step: 102480, training_loss: 1.45057
Epoch: 1/20, step: 102500, training_loss: 2.07324
Epoch: 1/20, step: 102520, training_loss: 2.81868
Epoch: 1/20, step: 102540, training_loss: 2.39027
Epoch: 1/20, step: 102560, training_loss: 2.25657
Epoch: 1/20, step: 102580, training_loss: 1.42648
Epoch: 1/20, step: 102600, training_loss: 1.47519
Epoch: 1/20, step: 102620, training_loss: 2.86510
Epoch: 1/20, step: 102640, training_loss: 2.50577
Epoch: 2/20, step: 20, training_loss: 1.56223
Epoch: 2/20, step: 40, training_loss: 1.79584
Epoch: 2/20, step: 60, training_loss: 1.84248
Epoch: 2/20, step: 80, training_loss: 1.82964
Epoch: 2/20, step: 100, training_loss: 2.31721
Epoch: 2/20, step: 120, training_loss: 1.38601
Epoch: 2/20, step: 140, training_loss: 1.93307
Epoch: 2/20, step: 160, training_loss: 2.06316
Epoch: 2/20, step: 180, training_loss: 1.36108
Epoch: 2/20, step: 200, training_loss: 2.00978
Epoch: 2/20, step: 220, training_loss: 1.50696
Epoch: 2/20, step: 240, training_loss: 2.69000
Epoch: 2/20, step: 260, training_loss: 1.51255
Epoch: 2/20, step: 280, training_loss: 2.42125
Epoch: 2/20, step: 300, training_loss: 2.18308
Epoch: 2/20, step: 320, training_loss: 1.65193
Epoch: 2/20, step: 340, training_loss: 1.03385
Epoch: 2/20, step: 360, training_loss: 2.75983
Epoch: 2/20, step: 380, training_loss: 3.13592
Epoch: 2/20, step: 400, training_loss: 2.93780
Epoch: 2/20, step: 420, training_loss: 3.00890
Epoch: 2/20, step: 440, training_loss: 1.75903
Epoch: 2/20, step: 460, training_loss: 2.64045
Epoch: 2/20, step: 480, training_loss: 2.26037
Epoch: 2/20, step: 500, training_loss: 2.77230
Epoch: 2/20, step: 520, training_loss: 2.03528
Epoch: 2/20, step: 540, training_loss: 1.74679
Epoch: 2/20, step: 560, training_loss: 1.98198
Epoch: 2/20, step: 580, training_loss: 2.56515
Epoch: 2/20, step: 600, training_loss: 2.41546
Epoch: 2/20, step: 620, training_loss: 2.21790
Epoch: 2/20, step: 640, training_loss: 1.43319
Epoch: 2/20, step: 660, training_loss: 2.41355
Epoch: 2/20, step: 680, training_loss: 1.96269
Epoch: 2/20, step: 700, training_loss: 3.00094
Epoch: 2/20, step: 720, training_loss: 2.08569
Epoch: 2/20, step: 740, training_loss: 1.43721
Epoch: 2/20, step: 760, training_loss: 2.10313
Epoch: 2/20, step: 780, training_loss: 2.24642
Epoch: 2/20, step: 800, training_loss: 2.61234
Epoch: 2/20, step: 820, training_loss: 1.89817
Epoch: 2/20, step: 840, training_loss: 2.87183
Epoch: 2/20, step: 860, training_loss: 2.77122
Epoch: 2/20, step: 880, training_loss: 2.35677
Epoch: 2/20, step: 900, training_loss: 2.13620
Epoch: 2/20, step: 920, training_loss: 2.33983
Epoch: 2/20, step: 940, training_loss: 2.84774
Epoch: 2/20, step: 960, training_loss: 2.68207
Epoch: 2/20, step: 980, training_loss: 1.89804
Epoch: 2/20, step: 1000, training_loss: 2.66748
accuracy: 0.38, validation_loss: 2.260523796081543, num_samples: 100
Epoch: 2/20, step: 1020, training_loss: 2.24882
Epoch: 2/20, step: 1040, training_loss: 1.35976
Epoch: 2/20, step: 1060, training_loss: 2.44198
Epoch: 2/20, step: 1080, training_loss: 1.67205
Epoch: 2/20, step: 1100, training_loss: 2.37348
Epoch: 2/20, step: 1120, training_loss: 2.50780
Epoch: 2/20, step: 1140, training_loss: 2.66838
Epoch: 2/20, step: 1160, training_loss: 1.90483
Epoch: 2/20, step: 1180, training_loss: 1.22753
Epoch: 2/20, step: 1200, training_loss: 3.36567
Epoch: 2/20, step: 1220, training_loss: 2.28556
Epoch: 2/20, step: 1240, training_loss: 2.09052
Epoch: 2/20, step: 1260, training_loss: 1.76171
Epoch: 2/20, step: 1280, training_loss: 1.78606
Epoch: 2/20, step: 1300, training_loss: 2.43075
Epoch: 2/20, step: 1320, training_loss: 2.69564
Epoch: 2/20, step: 1340, training_loss: 1.46226
Epoch: 2/20, step: 1360, training_loss: 1.77501
Epoch: 2/20, step: 1380, training_loss: 2.07473
Epoch: 2/20, step: 1400, training_loss: 2.05586
Epoch: 2/20, step: 1420, training_loss: 1.50213
Epoch: 2/20, step: 1440, training_loss: 2.81630
Epoch: 2/20, step: 1460, training_loss: 1.95630
Epoch: 2/20, step: 1480, training_loss: 2.31085
Epoch: 2/20, step: 1500, training_loss: 2.25997
Epoch: 2/20, step: 1520, training_loss: 2.34628
Epoch: 2/20, step: 1540, training_loss: 1.73805
Epoch: 2/20, step: 1560, training_loss: 1.67193
Epoch: 2/20, step: 1580, training_loss: 2.44770
Epoch: 2/20, step: 1600, training_loss: 1.77588
Epoch: 2/20, step: 1620, training_loss: 2.30429
Epoch: 2/20, step: 1640, training_loss: 1.97702
Epoch: 2/20, step: 1660, training_loss: 3.18671
Epoch: 2/20, step: 1680, training_loss: 2.68637
Epoch: 2/20, step: 1700, training_loss: 2.84470
Epoch: 2/20, step: 1720, training_loss: 3.31144
Epoch: 2/20, step: 1740, training_loss: 2.52476
Epoch: 2/20, step: 1760, training_loss: 2.72742
Epoch: 2/20, step: 1780, training_loss: 1.49391
Epoch: 2/20, step: 1800, training_loss: 2.16912
Epoch: 2/20, step: 1820, training_loss: 2.28121
Epoch: 2/20, step: 1840, training_loss: 2.49901
Epoch: 2/20, step: 1860, training_loss: 1.76717
Epoch: 2/20, step: 1880, training_loss: 3.26818
Epoch: 2/20, step: 1900, training_loss: 2.36967
Epoch: 2/20, step: 1920, training_loss: 3.20756
Epoch: 2/20, step: 1940, training_loss: 2.37494
Epoch: 2/20, step: 1960, training_loss: 2.53573
Epoch: 2/20, step: 1980, training_loss: 1.61981
Epoch: 2/20, step: 2000, training_loss: 1.84383
accuracy: 0.42, validation_loss: 2.0254967212677, num_samples: 100
Epoch: 2/20, step: 2020, training_loss: 2.02572
Epoch: 2/20, step: 2040, training_loss: 1.78149
Epoch: 2/20, step: 2060, training_loss: 2.60907
Epoch: 2/20, step: 2080, training_loss: 1.96013
Epoch: 2/20, step: 2100, training_loss: 2.42339
Epoch: 2/20, step: 2120, training_loss: 2.54857
Epoch: 2/20, step: 2140, training_loss: 2.03394
Epoch: 2/20, step: 2160, training_loss: 2.06675
Epoch: 2/20, step: 2180, training_loss: 2.00628
Epoch: 2/20, step: 2200, training_loss: 1.47457
Epoch: 2/20, step: 2220, training_loss: 1.94038
Epoch: 2/20, step: 2240, training_loss: 2.69506
Epoch: 2/20, step: 2260, training_loss: 2.40584
Epoch: 2/20, step: 2280, training_loss: 3.03215
Epoch: 2/20, step: 2300, training_loss: 2.30657
Epoch: 2/20, step: 2320, training_loss: 2.08210
Epoch: 2/20, step: 2340, training_loss: 2.24977
Epoch: 2/20, step: 2360, training_loss: 1.38895
Epoch: 2/20, step: 2380, training_loss: 2.63542
Epoch: 2/20, step: 2400, training_loss: 2.19848
Epoch: 2/20, step: 2420, training_loss: 2.87866
Epoch: 2/20, step: 2440, training_loss: 1.78558
Epoch: 2/20, step: 2460, training_loss: 1.74185
Epoch: 2/20, step: 2480, training_loss: 2.24007
Epoch: 2/20, step: 2500, training_loss: 2.50026
Epoch: 2/20, step: 2520, training_loss: 2.82907
Epoch: 2/20, step: 2540, training_loss: 2.49761
Epoch: 2/20, step: 2560, training_loss: 2.53819
Epoch: 2/20, step: 2580, training_loss: 1.62185
Epoch: 2/20, step: 2600, training_loss: 2.38517
Epoch: 2/20, step: 2620, training_loss: 2.51050
Epoch: 2/20, step: 2640, training_loss: 1.78369
Epoch: 2/20, step: 2660, training_loss: 2.56719
Epoch: 2/20, step: 2680, training_loss: 1.77390
Epoch: 2/20, step: 2700, training_loss: 2.59638
Epoch: 2/20, step: 2720, training_loss: 2.18344
Epoch: 2/20, step: 2740, training_loss: 2.15673
Epoch: 2/20, step: 2760, training_loss: 1.71142
Epoch: 2/20, step: 2780, training_loss: 2.62064
Epoch: 2/20, step: 2800, training_loss: 2.11111
Epoch: 2/20, step: 2820, training_loss: 2.49060
Epoch: 2/20, step: 2840, training_loss: 2.59022
Epoch: 2/20, step: 2860, training_loss: 2.21330
Epoch: 2/20, step: 2880, training_loss: 2.58228
Epoch: 2/20, step: 2900, training_loss: 2.81150
Epoch: 2/20, step: 2920, training_loss: 2.17723
Epoch: 2/20, step: 2940, training_loss: 2.56531
Epoch: 2/20, step: 2960, training_loss: 2.19014
Epoch: 2/20, step: 2980, training_loss: 1.68959
Epoch: 2/20, step: 3000, training_loss: 1.82601
accuracy: 0.42, validation_loss: 2.0381691455841064, num_samples: 100
Epoch: 2/20, step: 3020, training_loss: 2.34945
Epoch: 2/20, step: 3040, training_loss: 2.13677
Epoch: 2/20, step: 3060, training_loss: 2.78017
Epoch: 2/20, step: 3080, training_loss: 1.51319
Epoch: 2/20, step: 3100, training_loss: 1.70994
Epoch: 2/20, step: 3120, training_loss: 1.71100
Epoch: 2/20, step: 3140, training_loss: 2.02308
Epoch: 2/20, step: 3160, training_loss: 1.91538
Epoch: 2/20, step: 3180, training_loss: 2.10248
Epoch: 2/20, step: 3200, training_loss: 2.26463
Epoch: 2/20, step: 3220, training_loss: 2.41835
Epoch: 2/20, step: 3240, training_loss: 1.60601
Epoch: 2/20, step: 3260, training_loss: 3.26871
Epoch: 2/20, step: 3280, training_loss: 2.40330
Epoch: 2/20, step: 3300, training_loss: 1.91123
Epoch: 2/20, step: 3320, training_loss: 2.14759
Epoch: 2/20, step: 3340, training_loss: 1.89106
Epoch: 2/20, step: 3360, training_loss: 2.06345
Epoch: 2/20, step: 3380, training_loss: 2.61507
Epoch: 2/20, step: 3400, training_loss: 2.29463
Epoch: 2/20, step: 3420, training_loss: 1.93830
Epoch: 2/20, step: 3440, training_loss: 2.47296
Epoch: 2/20, step: 3460, training_loss: 1.63143
Epoch: 2/20, step: 3480, training_loss: 1.71639
Epoch: 2/20, step: 3500, training_loss: 3.20581
Epoch: 2/20, step: 3520, training_loss: 1.76567
Epoch: 2/20, step: 3540, training_loss: 1.60837
Epoch: 2/20, step: 3560, training_loss: 1.84533
Epoch: 2/20, step: 3580, training_loss: 1.91356
Epoch: 2/20, step: 3600, training_loss: 2.80981
Epoch: 2/20, step: 3620, training_loss: 2.00156
Epoch: 2/20, step: 3640, training_loss: 1.62251
Epoch: 2/20, step: 3660, training_loss: 2.92585
Epoch: 2/20, step: 3680, training_loss: 1.73824
Epoch: 2/20, step: 3700, training_loss: 2.16629
Epoch: 2/20, step: 3720, training_loss: 2.86704
Epoch: 2/20, step: 3740, training_loss: 1.86367
Epoch: 2/20, step: 3760, training_loss: 1.94611
Epoch: 2/20, step: 3780, training_loss: 2.31201
Epoch: 2/20, step: 3800, training_loss: 1.74463
Epoch: 2/20, step: 3820, training_loss: 1.66542
Epoch: 2/20, step: 3840, training_loss: 2.08361
Epoch: 2/20, step: 3860, training_loss: 2.43044
Epoch: 2/20, step: 3880, training_loss: 2.13103
Epoch: 2/20, step: 3900, training_loss: 1.62704
Epoch: 2/20, step: 3920, training_loss: 1.64343
Epoch: 2/20, step: 3940, training_loss: 2.95637
Epoch: 2/20, step: 3960, training_loss: 2.58141
Epoch: 2/20, step: 3980, training_loss: 1.40279
Epoch: 2/20, step: 4000, training_loss: 2.46638
accuracy: 0.37, validation_loss: 2.4355576038360596, num_samples: 100
Epoch: 2/20, step: 4020, training_loss: 2.13978
Epoch: 2/20, step: 4040, training_loss: 1.77302
Epoch: 2/20, step: 4060, training_loss: 1.90731
Epoch: 2/20, step: 4080, training_loss: 2.87549
Epoch: 2/20, step: 4100, training_loss: 2.12974
Epoch: 2/20, step: 4120, training_loss: 1.52388
Epoch: 2/20, step: 4140, training_loss: 2.84284
Epoch: 2/20, step: 4160, training_loss: 0.98996
Epoch: 2/20, step: 4180, training_loss: 1.66407
Epoch: 2/20, step: 4200, training_loss: 2.83669
Epoch: 2/20, step: 4220, training_loss: 1.74509
Epoch: 2/20, step: 4240, training_loss: 1.98752
Epoch: 2/20, step: 4260, training_loss: 2.60904
Epoch: 2/20, step: 4280, training_loss: 1.78079
Epoch: 2/20, step: 4300, training_loss: 2.36008
Epoch: 2/20, step: 4320, training_loss: 2.17662
Epoch: 2/20, step: 4340, training_loss: 1.53859
Epoch: 2/20, step: 4360, training_loss: 2.04404
Epoch: 2/20, step: 4380, training_loss: 2.62217
Epoch: 2/20, step: 4400, training_loss: 1.29616
Epoch: 2/20, step: 4420, training_loss: 2.41646
Epoch: 2/20, step: 4440, training_loss: 2.92780
Epoch: 2/20, step: 4460, training_loss: 2.01316
Epoch: 2/20, step: 4480, training_loss: 2.19896
Epoch: 2/20, step: 4500, training_loss: 2.28430
Epoch: 2/20, step: 4520, training_loss: 3.28177
Epoch: 2/20, step: 4540, training_loss: 2.72942
Epoch: 2/20, step: 4560, training_loss: 2.14262
Epoch: 2/20, step: 4580, training_loss: 1.98963
Epoch: 2/20, step: 4600, training_loss: 2.30161
Epoch: 2/20, step: 4620, training_loss: 2.28218
Epoch: 2/20, step: 4640, training_loss: 2.38215
Epoch: 2/20, step: 4660, training_loss: 2.97466
Epoch: 2/20, step: 4680, training_loss: 2.78540
Epoch: 2/20, step: 4700, training_loss: 1.55447
Epoch: 2/20, step: 4720, training_loss: 1.92884
Epoch: 2/20, step: 4740, training_loss: 2.40746
Epoch: 2/20, step: 4760, training_loss: 2.56468
Epoch: 2/20, step: 4780, training_loss: 2.27711
Epoch: 2/20, step: 4800, training_loss: 2.16075
Epoch: 2/20, step: 4820, training_loss: 2.01258
Epoch: 2/20, step: 4840, training_loss: 2.34360
Epoch: 2/20, step: 4860, training_loss: 2.41334
Epoch: 2/20, step: 4880, training_loss: 1.70367
Epoch: 2/20, step: 4900, training_loss: 2.47244
Epoch: 2/20, step: 4920, training_loss: 2.49181
Epoch: 2/20, step: 4940, training_loss: 2.68887
Epoch: 2/20, step: 4960, training_loss: 1.78031
Epoch: 2/20, step: 4980, training_loss: 0.93384
Epoch: 2/20, step: 5000, training_loss: 1.89991
accuracy: 0.42, validation_loss: 2.255798578262329, num_samples: 100
Epoch: 2/20, step: 5020, training_loss: 1.74210
Epoch: 2/20, step: 5040, training_loss: 2.24412
Epoch: 2/20, step: 5060, training_loss: 2.24756
Epoch: 2/20, step: 5080, training_loss: 2.51732
Epoch: 2/20, step: 5100, training_loss: 2.25456
Epoch: 2/20, step: 5120, training_loss: 2.61936
Epoch: 2/20, step: 5140, training_loss: 2.53543
Epoch: 2/20, step: 5160, training_loss: 2.56473
Epoch: 2/20, step: 5180, training_loss: 1.68338
Epoch: 2/20, step: 5200, training_loss: 1.78532
Epoch: 2/20, step: 5220, training_loss: 2.50161
Epoch: 2/20, step: 5240, training_loss: 2.41590
Epoch: 2/20, step: 5260, training_loss: 2.71956
Epoch: 2/20, step: 5280, training_loss: 2.49636
Epoch: 2/20, step: 5300, training_loss: 2.72768
Epoch: 2/20, step: 5320, training_loss: 2.26130
Epoch: 2/20, step: 5340, training_loss: 2.46610
Epoch: 2/20, step: 5360, training_loss: 2.49910
Epoch: 2/20, step: 5380, training_loss: 1.71093
Epoch: 2/20, step: 5400, training_loss: 2.88866
Epoch: 2/20, step: 5420, training_loss: 1.85955
Epoch: 2/20, step: 5440, training_loss: 2.49244
Epoch: 2/20, step: 5460, training_loss: 2.76022
Epoch: 2/20, step: 5480, training_loss: 1.51546
Epoch: 2/20, step: 5500, training_loss: 1.91356
Epoch: 2/20, step: 5520, training_loss: 2.86040
Epoch: 2/20, step: 5540, training_loss: 3.10232
Epoch: 2/20, step: 5560, training_loss: 2.80500
Epoch: 2/20, step: 5580, training_loss: 1.86289
Epoch: 2/20, step: 5600, training_loss: 1.71495
Epoch: 2/20, step: 5620, training_loss: 2.57525
Epoch: 2/20, step: 5640, training_loss: 0.92382
Epoch: 2/20, step: 5660, training_loss: 2.14494
Epoch: 2/20, step: 5680, training_loss: 2.21207
Epoch: 2/20, step: 5700, training_loss: 1.89100
Epoch: 2/20, step: 5720, training_loss: 2.88351
Epoch: 2/20, step: 5740, training_loss: 2.05035
Epoch: 2/20, step: 5760, training_loss: 2.08099
Epoch: 2/20, step: 5780, training_loss: 2.60568
Epoch: 2/20, step: 5800, training_loss: 2.92936
Epoch: 2/20, step: 5820, training_loss: 1.84138
Epoch: 2/20, step: 5840, training_loss: 2.90764
Epoch: 2/20, step: 5860, training_loss: 2.62176
Epoch: 2/20, step: 5880, training_loss: 1.49825
Epoch: 2/20, step: 5900, training_loss: 2.24586
Epoch: 2/20, step: 5920, training_loss: 1.59739
Epoch: 2/20, step: 5940, training_loss: 1.93758
Epoch: 2/20, step: 5960, training_loss: 2.30602
Epoch: 2/20, step: 5980, training_loss: 2.34163
Epoch: 2/20, step: 6000, training_loss: 2.16359
accuracy: 0.45, validation_loss: 2.181685209274292, num_samples: 100
Epoch: 2/20, step: 6020, training_loss: 1.48278
Epoch: 2/20, step: 6040, training_loss: 2.07447
Epoch: 2/20, step: 6060, training_loss: 1.78082
Epoch: 2/20, step: 6080, training_loss: 1.89380
Epoch: 2/20, step: 6100, training_loss: 2.26574
Epoch: 2/20, step: 6120, training_loss: 2.11126
Epoch: 2/20, step: 6140, training_loss: 1.69180
Epoch: 2/20, step: 6160, training_loss: 2.63177
Epoch: 2/20, step: 6180, training_loss: 2.65348
Epoch: 2/20, step: 6200, training_loss: 2.02696
Epoch: 2/20, step: 6220, training_loss: 2.28658
Epoch: 2/20, step: 6240, training_loss: 2.20197
Epoch: 2/20, step: 6260, training_loss: 2.36392
Epoch: 2/20, step: 6280, training_loss: 1.81298
Epoch: 2/20, step: 6300, training_loss: 1.75414
Epoch: 2/20, step: 6320, training_loss: 2.87403
Epoch: 2/20, step: 6340, training_loss: 1.76409
Epoch: 2/20, step: 6360, training_loss: 2.81041
Epoch: 2/20, step: 6380, training_loss: 2.46998
Epoch: 2/20, step: 6400, training_loss: 1.88365
Epoch: 2/20, step: 6420, training_loss: 1.90856
Epoch: 2/20, step: 6440, training_loss: 1.71466
Epoch: 2/20, step: 6460, training_loss: 2.79980
Epoch: 2/20, step: 6480, training_loss: 1.40520
Epoch: 2/20, step: 6500, training_loss: 1.62112
Epoch: 2/20, step: 6520, training_loss: 1.62117
Epoch: 2/20, step: 6540, training_loss: 2.45053
Epoch: 2/20, step: 6560, training_loss: 2.43490
Epoch: 2/20, step: 6580, training_loss: 1.86000
Epoch: 2/20, step: 6600, training_loss: 2.75008
Epoch: 2/20, step: 6620, training_loss: 2.54556
Epoch: 2/20, step: 6640, training_loss: 2.69035
Epoch: 2/20, step: 6660, training_loss: 1.85798
Epoch: 2/20, step: 6680, training_loss: 1.89197
Epoch: 2/20, step: 6700, training_loss: 1.45591
Epoch: 2/20, step: 6720, training_loss: 1.75843
Epoch: 2/20, step: 6740, training_loss: 2.03096
Epoch: 2/20, step: 6760, training_loss: 2.26438
Epoch: 2/20, step: 6780, training_loss: 1.80164
Epoch: 2/20, step: 6800, training_loss: 1.70762
Epoch: 2/20, step: 6820, training_loss: 2.50029
Epoch: 2/20, step: 6840, training_loss: 2.10643
Epoch: 2/20, step: 6860, training_loss: 1.79628
Epoch: 2/20, step: 6880, training_loss: 2.08662
Epoch: 2/20, step: 6900, training_loss: 2.21354
Epoch: 2/20, step: 6920, training_loss: 2.09258
Epoch: 2/20, step: 6940, training_loss: 2.71404
Epoch: 2/20, step: 6960, training_loss: 1.62641
Epoch: 2/20, step: 6980, training_loss: 1.90430
Epoch: 2/20, step: 7000, training_loss: 2.14293
accuracy: 0.46, validation_loss: 2.0039291381835938, num_samples: 100
Epoch: 2/20, step: 7020, training_loss: 2.14168
Epoch: 2/20, step: 7040, training_loss: 2.69621
Epoch: 2/20, step: 7060, training_loss: 2.20127
Epoch: 2/20, step: 7080, training_loss: 2.74677
Epoch: 2/20, step: 7100, training_loss: 2.04075
Epoch: 2/20, step: 7120, training_loss: 2.58550
Epoch: 2/20, step: 7140, training_loss: 2.10662
Epoch: 2/20, step: 7160, training_loss: 1.83176
Epoch: 2/20, step: 7180, training_loss: 2.60723
Epoch: 2/20, step: 7200, training_loss: 1.91311
Epoch: 2/20, step: 7220, training_loss: 2.21083
Epoch: 2/20, step: 7240, training_loss: 2.19483
Epoch: 2/20, step: 7260, training_loss: 2.33667
Epoch: 2/20, step: 7280, training_loss: 2.70960
Epoch: 2/20, step: 7300, training_loss: 2.56365
Epoch: 2/20, step: 7320, training_loss: 2.08128
Epoch: 2/20, step: 7340, training_loss: 2.73273
Epoch: 2/20, step: 7360, training_loss: 2.29737
Epoch: 2/20, step: 7380, training_loss: 2.05068
Epoch: 2/20, step: 7400, training_loss: 1.77538
Epoch: 2/20, step: 7420, training_loss: 2.00747
Epoch: 2/20, step: 7440, training_loss: 2.32483
Epoch: 2/20, step: 7460, training_loss: 2.50356
Epoch: 2/20, step: 7480, training_loss: 2.04078
Epoch: 2/20, step: 7500, training_loss: 2.12284
Epoch: 2/20, step: 7520, training_loss: 2.26049
Epoch: 2/20, step: 7540, training_loss: 2.07127
Epoch: 2/20, step: 7560, training_loss: 1.70867
Epoch: 2/20, step: 7580, training_loss: 2.53022
Epoch: 2/20, step: 7600, training_loss: 2.76470
Epoch: 2/20, step: 7620, training_loss: 2.37293
Epoch: 2/20, step: 7640, training_loss: 2.13447
Epoch: 2/20, step: 7660, training_loss: 1.76884
Epoch: 2/20, step: 7680, training_loss: 2.05145
Epoch: 2/20, step: 7700, training_loss: 2.24328
Epoch: 2/20, step: 7720, training_loss: 2.81738
Epoch: 2/20, step: 7740, training_loss: 2.34825
Epoch: 2/20, step: 7760, training_loss: 2.00608
Epoch: 2/20, step: 7780, training_loss: 2.10623
Epoch: 2/20, step: 7800, training_loss: 2.18412
Epoch: 2/20, step: 7820, training_loss: 1.95229
Epoch: 2/20, step: 7840, training_loss: 1.90167
Epoch: 2/20, step: 7860, training_loss: 2.32095
Epoch: 2/20, step: 7880, training_loss: 1.91002
Epoch: 2/20, step: 7900, training_loss: 2.07456
Epoch: 2/20, step: 7920, training_loss: 3.11259
Epoch: 2/20, step: 7940, training_loss: 2.12609
Epoch: 2/20, step: 7960, training_loss: 2.29273
Epoch: 2/20, step: 7980, training_loss: 2.54802
Epoch: 2/20, step: 8000, training_loss: 2.88197
accuracy: 0.35, validation_loss: 2.295844316482544, num_samples: 100
Epoch: 2/20, step: 8020, training_loss: 1.84723
Epoch: 2/20, step: 8040, training_loss: 2.57458
Epoch: 2/20, step: 8060, training_loss: 2.11340
Epoch: 2/20, step: 8080, training_loss: 2.81165
Epoch: 2/20, step: 8100, training_loss: 3.17840
Epoch: 2/20, step: 8120, training_loss: 2.26567
Epoch: 2/20, step: 8140, training_loss: 2.46850
Epoch: 2/20, step: 8160, training_loss: 1.98200
Epoch: 2/20, step: 8180, training_loss: 3.11637
Epoch: 2/20, step: 8200, training_loss: 2.16681
Epoch: 2/20, step: 8220, training_loss: 2.59912
Epoch: 2/20, step: 8240, training_loss: 2.75968
Epoch: 2/20, step: 8260, training_loss: 2.67697
Epoch: 2/20, step: 8280, training_loss: 1.79384
Epoch: 2/20, step: 8300, training_loss: 2.37635
Epoch: 2/20, step: 8320, training_loss: 2.43068
Epoch: 2/20, step: 8340, training_loss: 2.42994
Epoch: 2/20, step: 8360, training_loss: 2.79134
Epoch: 2/20, step: 8380, training_loss: 1.87539
Epoch: 2/20, step: 8400, training_loss: 2.03303
Epoch: 2/20, step: 8420, training_loss: 2.42982
Epoch: 2/20, step: 8440, training_loss: 1.82694
Epoch: 2/20, step: 8460, training_loss: 2.42601
Epoch: 2/20, step: 8480, training_loss: 1.52115
Epoch: 2/20, step: 8500, training_loss: 1.86142
Epoch: 2/20, step: 8520, training_loss: 2.32928
Epoch: 2/20, step: 8540, training_loss: 2.41522
Epoch: 2/20, step: 8560, training_loss: 2.05160
Epoch: 2/20, step: 8580, training_loss: 2.32709
Epoch: 2/20, step: 8600, training_loss: 1.41214
Epoch: 2/20, step: 8620, training_loss: 1.95561
Epoch: 2/20, step: 8640, training_loss: 2.19960
Epoch: 2/20, step: 8660, training_loss: 1.92008
Epoch: 2/20, step: 8680, training_loss: 1.46885
Epoch: 2/20, step: 8700, training_loss: 1.82722
Epoch: 2/20, step: 8720, training_loss: 2.49376
Epoch: 2/20, step: 8740, training_loss: 2.70579
Epoch: 2/20, step: 8760, training_loss: 2.07707
Epoch: 2/20, step: 8780, training_loss: 1.78433
Epoch: 2/20, step: 8800, training_loss: 2.47829
Epoch: 2/20, step: 8820, training_loss: 2.79561
Epoch: 2/20, step: 8840, training_loss: 2.38714
Epoch: 2/20, step: 8860, training_loss: 1.88672
Epoch: 2/20, step: 8880, training_loss: 1.40604
Epoch: 2/20, step: 8900, training_loss: 2.87075
Epoch: 2/20, step: 8920, training_loss: 1.82820
Epoch: 2/20, step: 8940, training_loss: 2.27558
Epoch: 2/20, step: 8960, training_loss: 2.20416
Epoch: 2/20, step: 8980, training_loss: 1.96328
Epoch: 2/20, step: 9000, training_loss: 2.09343
accuracy: 0.4, validation_loss: 2.21163010597229, num_samples: 100
Epoch: 2/20, step: 9020, training_loss: 2.15086
Epoch: 2/20, step: 9040, training_loss: 2.09732
Epoch: 2/20, step: 9060, training_loss: 2.60180
Epoch: 2/20, step: 9080, training_loss: 2.02493
Epoch: 2/20, step: 9100, training_loss: 2.57678
Epoch: 2/20, step: 9120, training_loss: 1.85475
Epoch: 2/20, step: 9140, training_loss: 2.68944
Epoch: 2/20, step: 9160, training_loss: 1.77122
Epoch: 2/20, step: 9180, training_loss: 2.28745
Epoch: 2/20, step: 9200, training_loss: 2.38076
Epoch: 2/20, step: 9220, training_loss: 2.41088
Epoch: 2/20, step: 9240, training_loss: 2.12020
Epoch: 2/20, step: 9260, training_loss: 2.12983
Epoch: 2/20, step: 9280, training_loss: 1.72274
Epoch: 2/20, step: 9300, training_loss: 2.14612
Epoch: 2/20, step: 9320, training_loss: 1.75787
Epoch: 2/20, step: 9340, training_loss: 2.71128
Epoch: 2/20, step: 9360, training_loss: 2.28830
Epoch: 2/20, step: 9380, training_loss: 2.63721
Epoch: 2/20, step: 9400, training_loss: 2.00894
Epoch: 2/20, step: 9420, training_loss: 1.87902
Epoch: 2/20, step: 9440, training_loss: 1.88675
Epoch: 2/20, step: 9460, training_loss: 2.11893
Epoch: 2/20, step: 9480, training_loss: 2.45863
Epoch: 2/20, step: 9500, training_loss: 1.90511
Epoch: 2/20, step: 9520, training_loss: 2.58500
Epoch: 2/20, step: 9540, training_loss: 2.48967
Epoch: 2/20, step: 9560, training_loss: 1.78484
Epoch: 2/20, step: 9580, training_loss: 2.48050
Epoch: 2/20, step: 9600, training_loss: 2.31763
Epoch: 2/20, step: 9620, training_loss: 2.35000
Epoch: 2/20, step: 9640, training_loss: 2.38806
Epoch: 2/20, step: 9660, training_loss: 1.51769
Epoch: 2/20, step: 9680, training_loss: 2.18765
Epoch: 2/20, step: 9700, training_loss: 2.77535
Epoch: 2/20, step: 9720, training_loss: 2.26955
Epoch: 2/20, step: 9740, training_loss: 2.52351
Epoch: 2/20, step: 9760, training_loss: 2.67110
Epoch: 2/20, step: 9780, training_loss: 2.80209
Epoch: 2/20, step: 9800, training_loss: 1.73049
Epoch: 2/20, step: 9820, training_loss: 3.04728
Epoch: 2/20, step: 9840, training_loss: 2.60868
Epoch: 2/20, step: 9860, training_loss: 2.37600
Epoch: 2/20, step: 9880, training_loss: 2.31256
Epoch: 2/20, step: 9900, training_loss: 2.51416
Epoch: 2/20, step: 9920, training_loss: 2.24791
Epoch: 2/20, step: 9940, training_loss: 1.63425
Epoch: 2/20, step: 9960, training_loss: 1.31230
Epoch: 2/20, step: 9980, training_loss: 2.23418
Epoch: 2/20, step: 10000, training_loss: 2.29101
accuracy: 0.33, validation_loss: 2.312404155731201, num_samples: 100
Epoch: 2/20, step: 10020, training_loss: 0.69181
Epoch: 2/20, step: 10040, training_loss: 2.22632
Epoch: 2/20, step: 10060, training_loss: 1.49847
Epoch: 2/20, step: 10080, training_loss: 3.03024
Epoch: 2/20, step: 10100, training_loss: 1.41755
Epoch: 2/20, step: 10120, training_loss: 2.59904
Epoch: 2/20, step: 10140, training_loss: 2.02927
Epoch: 2/20, step: 10160, training_loss: 2.17584
Epoch: 2/20, step: 10180, training_loss: 2.71865
Epoch: 2/20, step: 10200, training_loss: 2.16112
Epoch: 2/20, step: 10220, training_loss: 2.39133
Epoch: 2/20, step: 10240, training_loss: 1.93120
Epoch: 2/20, step: 10260, training_loss: 2.17372
Epoch: 2/20, step: 10280, training_loss: 1.20032
Epoch: 2/20, step: 10300, training_loss: 2.04910
Epoch: 2/20, step: 10320, training_loss: 2.27539
Epoch: 2/20, step: 10340, training_loss: 1.81836
Epoch: 2/20, step: 10360, training_loss: 1.72614
Epoch: 2/20, step: 10380, training_loss: 1.61180
Epoch: 2/20, step: 10400, training_loss: 1.75543
Epoch: 2/20, step: 10420, training_loss: 2.24391
Epoch: 2/20, step: 10440, training_loss: 1.98573
Epoch: 2/20, step: 10460, training_loss: 2.82261
Epoch: 2/20, step: 10480, training_loss: 1.93971
Epoch: 2/20, step: 10500, training_loss: 2.44425
Epoch: 2/20, step: 10520, training_loss: 2.44506
Epoch: 2/20, step: 10540, training_loss: 2.67250
Epoch: 2/20, step: 10560, training_loss: 2.65880
Epoch: 2/20, step: 10580, training_loss: 2.09830
Epoch: 2/20, step: 10600, training_loss: 2.30357
Epoch: 2/20, step: 10620, training_loss: 2.42812
Epoch: 2/20, step: 10640, training_loss: 2.18545
Epoch: 2/20, step: 10660, training_loss: 1.96036
Epoch: 2/20, step: 10680, training_loss: 2.14824
Epoch: 2/20, step: 10700, training_loss: 2.10811
Epoch: 2/20, step: 10720, training_loss: 2.08542
Epoch: 2/20, step: 10740, training_loss: 2.37802
Epoch: 2/20, step: 10760, training_loss: 2.23124
Epoch: 2/20, step: 10780, training_loss: 2.10581
Epoch: 2/20, step: 10800, training_loss: 2.45494
Epoch: 2/20, step: 10820, training_loss: 2.53499
Epoch: 2/20, step: 10840, training_loss: 2.25496
Epoch: 2/20, step: 10860, training_loss: 2.03896
Epoch: 2/20, step: 10880, training_loss: 2.66271
Epoch: 2/20, step: 10900, training_loss: 1.83079
Epoch: 2/20, step: 10920, training_loss: 2.48816
Epoch: 2/20, step: 10940, training_loss: 2.08162
Epoch: 2/20, step: 10960, training_loss: 2.19363
Epoch: 2/20, step: 10980, training_loss: 2.88896
Epoch: 2/20, step: 11000, training_loss: 2.68200
accuracy: 0.5, validation_loss: 1.7572866678237915, num_samples: 100
Epoch: 2/20, step: 11020, training_loss: 2.52205
Epoch: 2/20, step: 11040, training_loss: 2.40079
Epoch: 2/20, step: 11060, training_loss: 1.51289
Epoch: 2/20, step: 11080, training_loss: 2.49591
Epoch: 2/20, step: 11100, training_loss: 1.76251
Epoch: 2/20, step: 11120, training_loss: 2.45218
Epoch: 2/20, step: 11140, training_loss: 1.90990
Epoch: 2/20, step: 11160, training_loss: 1.78904
Epoch: 2/20, step: 11180, training_loss: 2.84794
Epoch: 2/20, step: 11200, training_loss: 2.60898
Epoch: 2/20, step: 11220, training_loss: 2.44246
Epoch: 2/20, step: 11240, training_loss: 2.77365
Epoch: 2/20, step: 11260, training_loss: 1.60916
Epoch: 2/20, step: 11280, training_loss: 2.33506
Epoch: 2/20, step: 11300, training_loss: 2.50047
Epoch: 2/20, step: 11320, training_loss: 2.24283
Epoch: 2/20, step: 11340, training_loss: 3.01929
Epoch: 2/20, step: 11360, training_loss: 1.94132
Epoch: 2/20, step: 11380, training_loss: 1.91110
Epoch: 2/20, step: 11400, training_loss: 1.25190
Epoch: 2/20, step: 11420, training_loss: 1.45922
Epoch: 2/20, step: 11440, training_loss: 2.25881
Epoch: 2/20, step: 11460, training_loss: 2.18520
Epoch: 2/20, step: 11480, training_loss: 1.76181
Epoch: 2/20, step: 11500, training_loss: 2.38950
Epoch: 2/20, step: 11520, training_loss: 2.57330
Epoch: 2/20, step: 11540, training_loss: 1.40263
Epoch: 2/20, step: 11560, training_loss: 1.56051
Epoch: 2/20, step: 11580, training_loss: 2.11224
Epoch: 2/20, step: 11600, training_loss: 2.87015
Epoch: 2/20, step: 11620, training_loss: 2.10585
Epoch: 2/20, step: 11640, training_loss: 2.42899
Epoch: 2/20, step: 11660, training_loss: 1.89598
Epoch: 2/20, step: 11680, training_loss: 1.95221
Epoch: 2/20, step: 11700, training_loss: 2.45874
Epoch: 2/20, step: 11720, training_loss: 2.29831
Epoch: 2/20, step: 11740, training_loss: 2.41916
Epoch: 2/20, step: 11760, training_loss: 2.53086
Epoch: 2/20, step: 11780, training_loss: 1.31236
Epoch: 2/20, step: 11800, training_loss: 2.20716
Epoch: 2/20, step: 11820, training_loss: 2.72248
Epoch: 2/20, step: 11840, training_loss: 1.80111
Epoch: 2/20, step: 11860, training_loss: 2.09505
Epoch: 2/20, step: 11880, training_loss: 2.81085
Epoch: 2/20, step: 11900, training_loss: 1.89742
Epoch: 2/20, step: 11920, training_loss: 1.62925
Epoch: 2/20, step: 11940, training_loss: 2.17567
Epoch: 2/20, step: 11960, training_loss: 2.25891
Epoch: 2/20, step: 11980, training_loss: 2.31839
Epoch: 2/20, step: 12000, training_loss: 1.29994
accuracy: 0.37, validation_loss: 2.1948206424713135, num_samples: 100
Epoch: 2/20, step: 12020, training_loss: 2.27386
Epoch: 2/20, step: 12040, training_loss: 1.99810
Epoch: 2/20, step: 12060, training_loss: 1.58799
Epoch: 2/20, step: 12080, training_loss: 2.70791
Epoch: 2/20, step: 12100, training_loss: 2.68536
Epoch: 2/20, step: 12120, training_loss: 1.44711
Epoch: 2/20, step: 12140, training_loss: 1.38784
Epoch: 2/20, step: 12160, training_loss: 1.56213
Epoch: 2/20, step: 12180, training_loss: 2.13341
Epoch: 2/20, step: 12200, training_loss: 2.06040
Epoch: 2/20, step: 12220, training_loss: 2.07735
Epoch: 2/20, step: 12240, training_loss: 2.44804
Epoch: 2/20, step: 12260, training_loss: 2.81228
Epoch: 2/20, step: 12280, training_loss: 2.27811
Epoch: 2/20, step: 12300, training_loss: 2.05506
Epoch: 2/20, step: 12320, training_loss: 2.31543
Epoch: 2/20, step: 12340, training_loss: 2.83048
Epoch: 2/20, step: 12360, training_loss: 2.18396
Epoch: 2/20, step: 12380, training_loss: 2.58933
Epoch: 2/20, step: 12400, training_loss: 2.91686
Epoch: 2/20, step: 12420, training_loss: 2.46737
Epoch: 2/20, step: 12440, training_loss: 2.94254
Epoch: 2/20, step: 12460, training_loss: 2.92996
Epoch: 2/20, step: 12480, training_loss: 2.36047
Epoch: 2/20, step: 12500, training_loss: 3.13457
Epoch: 2/20, step: 12520, training_loss: 1.95282
Epoch: 2/20, step: 12540, training_loss: 2.29401
Epoch: 2/20, step: 12560, training_loss: 1.30663
Epoch: 2/20, step: 12580, training_loss: 2.02584
Epoch: 2/20, step: 12600, training_loss: 1.92985
Epoch: 2/20, step: 12620, training_loss: 2.32329
Epoch: 2/20, step: 12640, training_loss: 2.46461
Epoch: 2/20, step: 12660, training_loss: 1.99416
Epoch: 2/20, step: 12680, training_loss: 2.09509
Epoch: 2/20, step: 12700, training_loss: 1.78666
Epoch: 2/20, step: 12720, training_loss: 2.10608
Epoch: 2/20, step: 12740, training_loss: 2.34970
Epoch: 2/20, step: 12760, training_loss: 1.90838
Epoch: 2/20, step: 12780, training_loss: 2.33848
Epoch: 2/20, step: 12800, training_loss: 1.94892
Epoch: 2/20, step: 12820, training_loss: 2.33338
Epoch: 2/20, step: 12840, training_loss: 1.82961
Epoch: 2/20, step: 12860, training_loss: 2.25943
Epoch: 2/20, step: 12880, training_loss: 2.15538
Epoch: 2/20, step: 12900, training_loss: 2.06174
Epoch: 2/20, step: 12920, training_loss: 2.53358
Epoch: 2/20, step: 12940, training_loss: 2.96950
Epoch: 2/20, step: 12960, training_loss: 2.11586
Epoch: 2/20, step: 12980, training_loss: 2.41298
Epoch: 2/20, step: 13000, training_loss: 1.80501
accuracy: 0.3, validation_loss: 2.433281898498535, num_samples: 100
Epoch: 2/20, step: 13020, training_loss: 2.32112
Epoch: 2/20, step: 13040, training_loss: 2.03240
Epoch: 2/20, step: 13060, training_loss: 3.17315
Epoch: 2/20, step: 13080, training_loss: 1.91029
Epoch: 2/20, step: 13100, training_loss: 1.95584
Epoch: 2/20, step: 13120, training_loss: 2.20782
Epoch: 2/20, step: 13140, training_loss: 2.23780
Epoch: 2/20, step: 13160, training_loss: 2.15902
Epoch: 2/20, step: 13180, training_loss: 2.39611
Epoch: 2/20, step: 13200, training_loss: 1.61550
Epoch: 2/20, step: 13220, training_loss: 1.93304
Epoch: 2/20, step: 13240, training_loss: 2.18487
Epoch: 2/20, step: 13260, training_loss: 1.97420
Epoch: 2/20, step: 13280, training_loss: 3.32091
Epoch: 2/20, step: 13300, training_loss: 3.08315
Epoch: 2/20, step: 13320, training_loss: 3.03474
Epoch: 2/20, step: 13340, training_loss: 1.46685
Epoch: 2/20, step: 13360, training_loss: 2.71891
Epoch: 2/20, step: 13380, training_loss: 2.16076
Epoch: 2/20, step: 13400, training_loss: 2.11670
Epoch: 2/20, step: 13420, training_loss: 1.37064
Epoch: 2/20, step: 13440, training_loss: 2.28633
Epoch: 2/20, step: 13460, training_loss: 2.25514
Epoch: 2/20, step: 13480, training_loss: 1.71303
Epoch: 2/20, step: 13500, training_loss: 2.06698
Epoch: 2/20, step: 13520, training_loss: 1.67830
Epoch: 2/20, step: 13540, training_loss: 2.37310
Epoch: 2/20, step: 13560, training_loss: 2.12507
Epoch: 2/20, step: 13580, training_loss: 1.79896
Epoch: 2/20, step: 13600, training_loss: 1.65062
Epoch: 2/20, step: 13620, training_loss: 2.60021
Epoch: 2/20, step: 13640, training_loss: 2.54057
Epoch: 2/20, step: 13660, training_loss: 1.77788
Epoch: 2/20, step: 13680, training_loss: 2.65259
Epoch: 2/20, step: 13700, training_loss: 2.59913
Epoch: 2/20, step: 13720, training_loss: 1.69829
Epoch: 2/20, step: 13740, training_loss: 2.03815
Epoch: 2/20, step: 13760, training_loss: 2.19405
Epoch: 2/20, step: 13780, training_loss: 2.20195
Epoch: 2/20, step: 13800, training_loss: 2.07895
Epoch: 2/20, step: 13820, training_loss: 2.68499
Epoch: 2/20, step: 13840, training_loss: 1.32962
Epoch: 2/20, step: 13860, training_loss: 1.97191
Epoch: 2/20, step: 13880, training_loss: 2.28672
Epoch: 2/20, step: 13900, training_loss: 2.58141
Epoch: 2/20, step: 13920, training_loss: 2.46148
Epoch: 2/20, step: 13940, training_loss: 2.55292
Epoch: 2/20, step: 13960, training_loss: 1.60722
Epoch: 2/20, step: 13980, training_loss: 3.13887
Epoch: 2/20, step: 14000, training_loss: 1.82880
accuracy: 0.41, validation_loss: 2.1768031120300293, num_samples: 100
Epoch: 2/20, step: 14020, training_loss: 2.86501
Epoch: 2/20, step: 14040, training_loss: 2.77915
Epoch: 2/20, step: 14060, training_loss: 2.29122
Epoch: 2/20, step: 14080, training_loss: 2.19903
Epoch: 2/20, step: 14100, training_loss: 2.42099
Epoch: 2/20, step: 14120, training_loss: 1.77932
Epoch: 2/20, step: 14140, training_loss: 2.33663
Epoch: 2/20, step: 14160, training_loss: 2.14382
Epoch: 2/20, step: 14180, training_loss: 2.53862
Epoch: 2/20, step: 14200, training_loss: 2.21920
Epoch: 2/20, step: 14220, training_loss: 1.56767
Epoch: 2/20, step: 14240, training_loss: 2.97947
Epoch: 2/20, step: 14260, training_loss: 2.48956
Epoch: 2/20, step: 14280, training_loss: 2.49275
Epoch: 2/20, step: 14300, training_loss: 1.46074
Epoch: 2/20, step: 14320, training_loss: 2.22268
Epoch: 2/20, step: 14340, training_loss: 2.40776
Epoch: 2/20, step: 14360, training_loss: 1.72858
Epoch: 2/20, step: 14380, training_loss: 1.92650
Epoch: 2/20, step: 14400, training_loss: 2.69316
Epoch: 2/20, step: 14420, training_loss: 2.73952
Epoch: 2/20, step: 14440, training_loss: 2.73257
Epoch: 2/20, step: 14460, training_loss: 1.93317
Epoch: 2/20, step: 14480, training_loss: 2.92085
Epoch: 2/20, step: 14500, training_loss: 2.10758
Epoch: 2/20, step: 14520, training_loss: 1.44361
Epoch: 2/20, step: 14540, training_loss: 2.03209
Epoch: 2/20, step: 14560, training_loss: 3.05924
Epoch: 2/20, step: 14580, training_loss: 1.16472
Epoch: 2/20, step: 14600, training_loss: 1.90685
Epoch: 2/20, step: 14620, training_loss: 2.67714
Epoch: 2/20, step: 14640, training_loss: 1.95304
Epoch: 2/20, step: 14660, training_loss: 1.97314
Epoch: 2/20, step: 14680, training_loss: 2.38636
Epoch: 2/20, step: 14700, training_loss: 2.02277
Epoch: 2/20, step: 14720, training_loss: 2.27049
Epoch: 2/20, step: 14740, training_loss: 2.35024
Epoch: 2/20, step: 14760, training_loss: 2.54751
Epoch: 2/20, step: 14780, training_loss: 1.92250
Epoch: 2/20, step: 14800, training_loss: 2.69310
Epoch: 2/20, step: 14820, training_loss: 2.23064
Epoch: 2/20, step: 14840, training_loss: 2.20935
Epoch: 2/20, step: 14860, training_loss: 1.35156
Epoch: 2/20, step: 14880, training_loss: 1.55066
Epoch: 2/20, step: 14900, training_loss: 2.16407
Epoch: 2/20, step: 14920, training_loss: 2.06957
Epoch: 2/20, step: 14940, training_loss: 2.30809
Epoch: 2/20, step: 14960, training_loss: 2.34480
Epoch: 2/20, step: 14980, training_loss: 1.94558
Epoch: 2/20, step: 15000, training_loss: 1.23237
accuracy: 0.44, validation_loss: 2.069190263748169, num_samples: 100
Epoch: 2/20, step: 15020, training_loss: 1.73527
Epoch: 2/20, step: 15040, training_loss: 1.75488
Epoch: 2/20, step: 15060, training_loss: 2.03532
Epoch: 2/20, step: 15080, training_loss: 1.69610
Epoch: 2/20, step: 15100, training_loss: 2.64530
Epoch: 2/20, step: 15120, training_loss: 2.37236
Epoch: 2/20, step: 15140, training_loss: 1.80460
Epoch: 2/20, step: 15160, training_loss: 2.74294
Epoch: 2/20, step: 15180, training_loss: 2.31734
Epoch: 2/20, step: 15200, training_loss: 2.29534
Epoch: 2/20, step: 15220, training_loss: 2.04882
Epoch: 2/20, step: 15240, training_loss: 1.69357
Epoch: 2/20, step: 15260, training_loss: 2.36865
Epoch: 2/20, step: 15280, training_loss: 2.01269
Epoch: 2/20, step: 15300, training_loss: 2.09468
Epoch: 2/20, step: 15320, training_loss: 1.90572
Epoch: 2/20, step: 15340, training_loss: 1.48806
Epoch: 2/20, step: 15360, training_loss: 2.20354
Epoch: 2/20, step: 15380, training_loss: 2.66316
Epoch: 2/20, step: 15400, training_loss: 2.26464
Epoch: 2/20, step: 15420, training_loss: 2.96653
Epoch: 2/20, step: 15440, training_loss: 2.48127
Epoch: 2/20, step: 15460, training_loss: 2.37680
Epoch: 2/20, step: 15480, training_loss: 1.38137
Epoch: 2/20, step: 15500, training_loss: 1.40375
Epoch: 2/20, step: 15520, training_loss: 2.77271
Epoch: 2/20, step: 15540, training_loss: 2.20698
Epoch: 2/20, step: 15560, training_loss: 1.38644
Epoch: 2/20, step: 15580, training_loss: 2.33690
Epoch: 2/20, step: 15600, training_loss: 2.70253
Epoch: 2/20, step: 15620, training_loss: 2.01125
Epoch: 2/20, step: 15640, training_loss: 2.21446
Epoch: 2/20, step: 15660, training_loss: 2.71110
Epoch: 2/20, step: 15680, training_loss: 2.12223
Epoch: 2/20, step: 15700, training_loss: 2.47483
Epoch: 2/20, step: 15720, training_loss: 2.28841
Epoch: 2/20, step: 15740, training_loss: 2.69088
Epoch: 2/20, step: 15760, training_loss: 2.30369
Epoch: 2/20, step: 15780, training_loss: 1.34161
Epoch: 2/20, step: 15800, training_loss: 2.40420
Epoch: 2/20, step: 15820, training_loss: 1.76221
Epoch: 2/20, step: 15840, training_loss: 1.84945
Epoch: 2/20, step: 15860, training_loss: 2.09736
Epoch: 2/20, step: 15880, training_loss: 1.42236
Epoch: 2/20, step: 15900, training_loss: 2.01865
Epoch: 2/20, step: 15920, training_loss: 2.46887
Epoch: 2/20, step: 15940, training_loss: 2.58265
Epoch: 2/20, step: 15960, training_loss: 1.80355
Epoch: 2/20, step: 15980, training_loss: 2.30527
Epoch: 2/20, step: 16000, training_loss: 1.73072
accuracy: 0.31, validation_loss: 2.4839906692504883, num_samples: 100
Epoch: 2/20, step: 16020, training_loss: 2.27821
Epoch: 2/20, step: 16040, training_loss: 1.65553
Epoch: 2/20, step: 16060, training_loss: 1.23074
Epoch: 2/20, step: 16080, training_loss: 2.00450
Epoch: 2/20, step: 16100, training_loss: 2.07345
Epoch: 2/20, step: 16120, training_loss: 2.04704
Epoch: 2/20, step: 16140, training_loss: 1.73906
Epoch: 2/20, step: 16160, training_loss: 2.16823
Epoch: 2/20, step: 16180, training_loss: 1.75002
Epoch: 2/20, step: 16200, training_loss: 1.26339
Epoch: 2/20, step: 16220, training_loss: 2.57530
Epoch: 2/20, step: 16240, training_loss: 2.71435
Epoch: 2/20, step: 16260, training_loss: 2.03550
Epoch: 2/20, step: 16280, training_loss: 2.89182
Epoch: 2/20, step: 16300, training_loss: 1.75807
Epoch: 2/20, step: 16320, training_loss: 1.53202
Epoch: 2/20, step: 16340, training_loss: 1.62312
Epoch: 2/20, step: 16360, training_loss: 2.13682
Epoch: 2/20, step: 16380, training_loss: 2.28143
Epoch: 2/20, step: 16400, training_loss: 2.06860
Epoch: 2/20, step: 16420, training_loss: 2.50862
Epoch: 2/20, step: 16440, training_loss: 2.20339
Epoch: 2/20, step: 16460, training_loss: 1.92180
Epoch: 2/20, step: 16480, training_loss: 3.02872
Epoch: 2/20, step: 16500, training_loss: 2.47772
Epoch: 2/20, step: 16520, training_loss: 1.81510
Epoch: 2/20, step: 16540, training_loss: 1.81728
Epoch: 2/20, step: 16560, training_loss: 2.09291
Epoch: 2/20, step: 16580, training_loss: 2.56992
Epoch: 2/20, step: 16600, training_loss: 3.02605
Epoch: 2/20, step: 16620, training_loss: 1.40348
Epoch: 2/20, step: 16640, training_loss: 2.43780
Epoch: 2/20, step: 16660, training_loss: 2.11620
Epoch: 2/20, step: 16680, training_loss: 1.87987
Epoch: 2/20, step: 16700, training_loss: 2.11957
Epoch: 2/20, step: 16720, training_loss: 1.98111
Epoch: 2/20, step: 16740, training_loss: 2.39454
Epoch: 2/20, step: 16760, training_loss: 1.86902
Epoch: 2/20, step: 16780, training_loss: 2.26165
Epoch: 2/20, step: 16800, training_loss: 3.46996
Epoch: 2/20, step: 16820, training_loss: 2.68688
Epoch: 2/20, step: 16840, training_loss: 1.98157
Epoch: 2/20, step: 16860, training_loss: 1.96628
Epoch: 2/20, step: 16880, training_loss: 2.32453
Epoch: 2/20, step: 16900, training_loss: 2.71348
Epoch: 2/20, step: 16920, training_loss: 2.13140
Epoch: 2/20, step: 16940, training_loss: 3.06353
Epoch: 2/20, step: 16960, training_loss: 2.76903
Epoch: 2/20, step: 16980, training_loss: 2.42823
Epoch: 2/20, step: 17000, training_loss: 2.38082
accuracy: 0.44, validation_loss: 2.1640584468841553, num_samples: 100
Epoch: 2/20, step: 17020, training_loss: 2.71440
Epoch: 2/20, step: 17040, training_loss: 1.95703
Epoch: 2/20, step: 17060, training_loss: 2.70509
Epoch: 2/20, step: 17080, training_loss: 2.04659
Epoch: 2/20, step: 17100, training_loss: 2.43974
Epoch: 2/20, step: 17120, training_loss: 1.41959
Epoch: 2/20, step: 17140, training_loss: 2.40229
Epoch: 2/20, step: 17160, training_loss: 2.74777
Epoch: 2/20, step: 17180, training_loss: 3.10413
Epoch: 2/20, step: 17200, training_loss: 2.29130
Epoch: 2/20, step: 17220, training_loss: 1.85781
Epoch: 2/20, step: 17240, training_loss: 1.66537
Epoch: 2/20, step: 17260, training_loss: 2.41702
Epoch: 2/20, step: 17280, training_loss: 2.14039
Epoch: 2/20, step: 17300, training_loss: 2.10058
Epoch: 2/20, step: 17320, training_loss: 2.56188
Epoch: 2/20, step: 17340, training_loss: 2.25350
Epoch: 2/20, step: 17360, training_loss: 2.19830
Epoch: 2/20, step: 17380, training_loss: 2.60052
Epoch: 2/20, step: 17400, training_loss: 2.10333
Epoch: 2/20, step: 17420, training_loss: 2.56280
Epoch: 2/20, step: 17440, training_loss: 2.90570
Epoch: 2/20, step: 17460, training_loss: 2.39281
Epoch: 2/20, step: 17480, training_loss: 2.41263
Epoch: 2/20, step: 17500, training_loss: 2.99078
Epoch: 2/20, step: 17520, training_loss: 2.23600
Epoch: 2/20, step: 17540, training_loss: 1.34112
Epoch: 2/20, step: 17560, training_loss: 2.59962
Epoch: 2/20, step: 17580, training_loss: 2.75835
Epoch: 2/20, step: 17600, training_loss: 1.37202
Epoch: 2/20, step: 17620, training_loss: 2.24468
Epoch: 2/20, step: 17640, training_loss: 2.88818
Epoch: 2/20, step: 17660, training_loss: 2.00384
Epoch: 2/20, step: 17680, training_loss: 1.39795
Epoch: 2/20, step: 17700, training_loss: 2.61674
Epoch: 2/20, step: 17720, training_loss: 2.70966
Epoch: 2/20, step: 17740, training_loss: 2.32548
Epoch: 2/20, step: 17760, training_loss: 2.22184
Epoch: 2/20, step: 17780, training_loss: 1.76424
Epoch: 2/20, step: 17800, training_loss: 3.27143
Epoch: 2/20, step: 17820, training_loss: 2.46611
Epoch: 2/20, step: 17840, training_loss: 1.35462
Epoch: 2/20, step: 17860, training_loss: 2.41285
Epoch: 2/20, step: 17880, training_loss: 3.07284
Epoch: 2/20, step: 17900, training_loss: 2.10822
Epoch: 2/20, step: 17920, training_loss: 1.75302
Epoch: 2/20, step: 17940, training_loss: 2.48014
Epoch: 2/20, step: 17960, training_loss: 2.08944
Epoch: 2/20, step: 17980, training_loss: 2.70930
Epoch: 2/20, step: 18000, training_loss: 3.03615
accuracy: 0.38, validation_loss: 2.2337493896484375, num_samples: 100
Epoch: 2/20, step: 18020, training_loss: 2.26768
Epoch: 2/20, step: 18040, training_loss: 1.89741
Epoch: 2/20, step: 18060, training_loss: 1.76869
Epoch: 2/20, step: 18080, training_loss: 1.69440
Epoch: 2/20, step: 18100, training_loss: 2.26787
Epoch: 2/20, step: 18120, training_loss: 2.62439
Epoch: 2/20, step: 18140, training_loss: 1.97540
Epoch: 2/20, step: 18160, training_loss: 2.45276
Epoch: 2/20, step: 18180, training_loss: 2.02262
Epoch: 2/20, step: 18200, training_loss: 2.54301
Epoch: 2/20, step: 18220, training_loss: 2.14095
Epoch: 2/20, step: 18240, training_loss: 2.97186
Epoch: 2/20, step: 18260, training_loss: 2.17988
Epoch: 2/20, step: 18280, training_loss: 1.96705
Epoch: 2/20, step: 18300, training_loss: 2.13001
Epoch: 2/20, step: 18320, training_loss: 2.14338
Epoch: 2/20, step: 18340, training_loss: 1.84223
Epoch: 2/20, step: 18360, training_loss: 2.68851
Epoch: 2/20, step: 18380, training_loss: 3.03200
Epoch: 2/20, step: 18400, training_loss: 2.68637
Epoch: 2/20, step: 18420, training_loss: 1.88458
Epoch: 2/20, step: 18440, training_loss: 1.80618
Epoch: 2/20, step: 18460, training_loss: 1.94743
Epoch: 2/20, step: 18480, training_loss: 2.58914
Epoch: 2/20, step: 18500, training_loss: 2.63950
Epoch: 2/20, step: 18520, training_loss: 1.82258
Epoch: 2/20, step: 18540, training_loss: 1.27506
Epoch: 2/20, step: 18560, training_loss: 2.51319
Epoch: 2/20, step: 18580, training_loss: 2.80830
Epoch: 2/20, step: 18600, training_loss: 1.23283
Epoch: 2/20, step: 18620, training_loss: 2.83676
Epoch: 2/20, step: 18640, training_loss: 2.46234
Epoch: 2/20, step: 18660, training_loss: 2.11385
Epoch: 2/20, step: 18680, training_loss: 2.14589
Epoch: 2/20, step: 18700, training_loss: 2.46252
Epoch: 2/20, step: 18720, training_loss: 2.07039
Epoch: 2/20, step: 18740, training_loss: 2.60276
Epoch: 2/20, step: 18760, training_loss: 2.74728
Epoch: 2/20, step: 18780, training_loss: 1.73613
Epoch: 2/20, step: 18800, training_loss: 2.50563
Epoch: 2/20, step: 18820, training_loss: 2.51306
Epoch: 2/20, step: 18840, training_loss: 2.52630
Epoch: 2/20, step: 18860, training_loss: 2.28646
Epoch: 2/20, step: 18880, training_loss: 1.82251
Epoch: 2/20, step: 18900, training_loss: 2.32969
Epoch: 2/20, step: 18920, training_loss: 2.45189
Epoch: 2/20, step: 18940, training_loss: 2.70304
Epoch: 2/20, step: 18960, training_loss: 1.61644
Epoch: 2/20, step: 18980, training_loss: 2.85096
Epoch: 2/20, step: 19000, training_loss: 2.71754
accuracy: 0.39, validation_loss: 2.173285484313965, num_samples: 100
Epoch: 2/20, step: 19020, training_loss: 2.21963
Epoch: 2/20, step: 19040, training_loss: 2.41957
Epoch: 2/20, step: 19060, training_loss: 2.83097
Epoch: 2/20, step: 19080, training_loss: 3.05890
Epoch: 2/20, step: 19100, training_loss: 1.84245
Epoch: 2/20, step: 19120, training_loss: 2.62829
Epoch: 2/20, step: 19140, training_loss: 2.39166
Epoch: 2/20, step: 19160, training_loss: 2.11123
Epoch: 2/20, step: 19180, training_loss: 1.82545
Epoch: 2/20, step: 19200, training_loss: 1.93216
Epoch: 2/20, step: 19220, training_loss: 1.89865
Epoch: 2/20, step: 19240, training_loss: 1.61778
Epoch: 2/20, step: 19260, training_loss: 2.18796
Epoch: 2/20, step: 19280, training_loss: 1.92150
Epoch: 2/20, step: 19300, training_loss: 2.35762
Epoch: 2/20, step: 19320, training_loss: 2.44033
Epoch: 2/20, step: 19340, training_loss: 1.78974
Epoch: 2/20, step: 19360, training_loss: 2.26323
Epoch: 2/20, step: 19380, training_loss: 3.00518
Epoch: 2/20, step: 19400, training_loss: 2.59331
Epoch: 2/20, step: 19420, training_loss: 2.14897
Epoch: 2/20, step: 19440, training_loss: 2.37774
Epoch: 2/20, step: 19460, training_loss: 2.17950
Epoch: 2/20, step: 19480, training_loss: 1.67622
Epoch: 2/20, step: 19500, training_loss: 2.33218
Epoch: 2/20, step: 19520, training_loss: 1.96184
Epoch: 2/20, step: 19540, training_loss: 2.95601
Epoch: 2/20, step: 19560, training_loss: 1.89992
Epoch: 2/20, step: 19580, training_loss: 1.59304
Epoch: 2/20, step: 19600, training_loss: 1.78933
Epoch: 2/20, step: 19620, training_loss: 2.18533
Epoch: 2/20, step: 19640, training_loss: 2.92100
Epoch: 2/20, step: 19660, training_loss: 2.45800
Epoch: 2/20, step: 19680, training_loss: 2.22847
Epoch: 2/20, step: 19700, training_loss: 2.30195
Epoch: 2/20, step: 19720, training_loss: 1.74702
Epoch: 2/20, step: 19740, training_loss: 2.50991
Epoch: 2/20, step: 19760, training_loss: 2.77390
Epoch: 2/20, step: 19780, training_loss: 2.69988
Epoch: 2/20, step: 19800, training_loss: 2.44317
Epoch: 2/20, step: 19820, training_loss: 2.02983
Epoch: 2/20, step: 19840, training_loss: 1.53763
Epoch: 2/20, step: 19860, training_loss: 2.45557
Epoch: 2/20, step: 19880, training_loss: 2.13081
Epoch: 2/20, step: 19900, training_loss: 2.02019
Epoch: 2/20, step: 19920, training_loss: 2.14830
Epoch: 2/20, step: 19940, training_loss: 2.23989
Epoch: 2/20, step: 19960, training_loss: 1.96505
Epoch: 2/20, step: 19980, training_loss: 3.02333
Epoch: 2/20, step: 20000, training_loss: 2.38472
accuracy: 0.4, validation_loss: 2.1821887493133545, num_samples: 100
Epoch: 2/20, step: 20020, training_loss: 3.33180
Epoch: 2/20, step: 20040, training_loss: 2.27963
Epoch: 2/20, step: 20060, training_loss: 2.10914
Epoch: 2/20, step: 20080, training_loss: 1.70229
Epoch: 2/20, step: 20100, training_loss: 1.82855
Epoch: 2/20, step: 20120, training_loss: 2.19524
Epoch: 2/20, step: 20140, training_loss: 2.33463
Epoch: 2/20, step: 20160, training_loss: 2.52481
Epoch: 2/20, step: 20180, training_loss: 2.57591
Epoch: 2/20, step: 20200, training_loss: 2.47673
Epoch: 2/20, step: 20220, training_loss: 2.22528
Epoch: 2/20, step: 20240, training_loss: 2.05875
Epoch: 2/20, step: 20260, training_loss: 1.81886
Epoch: 2/20, step: 20280, training_loss: 3.24640
Epoch: 2/20, step: 20300, training_loss: 2.12937
Epoch: 2/20, step: 20320, training_loss: 2.40455
Epoch: 2/20, step: 20340, training_loss: 2.24234
Epoch: 2/20, step: 20360, training_loss: 1.72024
Epoch: 2/20, step: 20380, training_loss: 1.97161
Epoch: 2/20, step: 20400, training_loss: 2.60129
Epoch: 2/20, step: 20420, training_loss: 2.04340
Epoch: 2/20, step: 20440, training_loss: 1.78609
Epoch: 2/20, step: 20460, training_loss: 1.70439
Epoch: 2/20, step: 20480, training_loss: 2.27068
Epoch: 2/20, step: 20500, training_loss: 2.24360
Epoch: 2/20, step: 20520, training_loss: 2.04783
Epoch: 2/20, step: 20540, training_loss: 1.70513
Epoch: 2/20, step: 20560, training_loss: 2.22321
Epoch: 2/20, step: 20580, training_loss: 2.17150
Epoch: 2/20, step: 20600, training_loss: 1.78979
Epoch: 2/20, step: 20620, training_loss: 2.00466
Epoch: 2/20, step: 20640, training_loss: 2.45234
Epoch: 2/20, step: 20660, training_loss: 1.80271
Epoch: 2/20, step: 20680, training_loss: 2.47964
Epoch: 2/20, step: 20700, training_loss: 3.09140
Epoch: 2/20, step: 20720, training_loss: 1.81096
Epoch: 2/20, step: 20740, training_loss: 2.64520
Epoch: 2/20, step: 20760, training_loss: 2.28698
Epoch: 2/20, step: 20780, training_loss: 1.93627
Epoch: 2/20, step: 20800, training_loss: 2.44948
Epoch: 2/20, step: 20820, training_loss: 1.77943
Epoch: 2/20, step: 20840, training_loss: 1.79329
Epoch: 2/20, step: 20860, training_loss: 2.05992
Epoch: 2/20, step: 20880, training_loss: 2.68964
Epoch: 2/20, step: 20900, training_loss: 1.97665
Epoch: 2/20, step: 20920, training_loss: 2.45988
Epoch: 2/20, step: 20940, training_loss: 1.97519
Epoch: 2/20, step: 20960, training_loss: 2.15240
Epoch: 2/20, step: 20980, training_loss: 2.55781
Epoch: 2/20, step: 21000, training_loss: 1.55570
accuracy: 0.44, validation_loss: 2.254929304122925, num_samples: 100
Epoch: 2/20, step: 21020, training_loss: 2.20494
Epoch: 2/20, step: 21040, training_loss: 2.45602
Epoch: 2/20, step: 21060, training_loss: 2.35436
Epoch: 2/20, step: 21080, training_loss: 1.53374
Epoch: 2/20, step: 21100, training_loss: 2.82629
Epoch: 2/20, step: 21120, training_loss: 2.17264
Epoch: 2/20, step: 21140, training_loss: 2.41714
Epoch: 2/20, step: 21160, training_loss: 1.99402
Epoch: 2/20, step: 21180, training_loss: 1.97493
Epoch: 2/20, step: 21200, training_loss: 1.99212
Epoch: 2/20, step: 21220, training_loss: 2.12987
Epoch: 2/20, step: 21240, training_loss: 1.99428
Epoch: 2/20, step: 21260, training_loss: 2.42664
Epoch: 2/20, step: 21280, training_loss: 2.18880
Epoch: 2/20, step: 21300, training_loss: 1.51122
Epoch: 2/20, step: 21320, training_loss: 2.00905
Epoch: 2/20, step: 21340, training_loss: 2.63264
Epoch: 2/20, step: 21360, training_loss: 2.66571
Epoch: 2/20, step: 21380, training_loss: 2.16791
Epoch: 2/20, step: 21400, training_loss: 2.06709
Epoch: 2/20, step: 21420, training_loss: 2.29460
Epoch: 2/20, step: 21440, training_loss: 2.13411
Epoch: 2/20, step: 21460, training_loss: 1.99899
Epoch: 2/20, step: 21480, training_loss: 3.15966
Epoch: 2/20, step: 21500, training_loss: 2.19495
Epoch: 2/20, step: 21520, training_loss: 2.52839
Epoch: 2/20, step: 21540, training_loss: 2.14950
Epoch: 2/20, step: 21560, training_loss: 1.87351
Epoch: 2/20, step: 21580, training_loss: 2.73336
Epoch: 2/20, step: 21600, training_loss: 2.07164
Epoch: 2/20, step: 21620, training_loss: 2.34268
Epoch: 2/20, step: 21640, training_loss: 1.82983
Epoch: 2/20, step: 21660, training_loss: 1.55717
Epoch: 2/20, step: 21680, training_loss: 2.36382
Epoch: 2/20, step: 21700, training_loss: 2.85251
Epoch: 2/20, step: 21720, training_loss: 2.49351
Epoch: 2/20, step: 21740, training_loss: 1.84821
Epoch: 2/20, step: 21760, training_loss: 2.12559
Epoch: 2/20, step: 21780, training_loss: 2.04390
Epoch: 2/20, step: 21800, training_loss: 2.53177
Epoch: 2/20, step: 21820, training_loss: 1.49527
Epoch: 2/20, step: 21840, training_loss: 2.38854
Epoch: 2/20, step: 21860, training_loss: 1.82563
Epoch: 2/20, step: 21880, training_loss: 3.02988
Epoch: 2/20, step: 21900, training_loss: 2.48926
Epoch: 2/20, step: 21920, training_loss: 1.71381
Epoch: 2/20, step: 21940, training_loss: 1.42287
Epoch: 2/20, step: 21960, training_loss: 2.45583
Epoch: 2/20, step: 21980, training_loss: 1.79369
Epoch: 2/20, step: 22000, training_loss: 2.38912
accuracy: 0.5, validation_loss: 1.7080820798873901, num_samples: 100
Epoch: 2/20, step: 22020, training_loss: 2.36226
Epoch: 2/20, step: 22040, training_loss: 2.16310
Epoch: 2/20, step: 22060, training_loss: 2.42478
Epoch: 2/20, step: 22080, training_loss: 2.25960
Epoch: 2/20, step: 22100, training_loss: 2.78286
Epoch: 2/20, step: 22120, training_loss: 2.33575
Epoch: 2/20, step: 22140, training_loss: 2.55541
Epoch: 2/20, step: 22160, training_loss: 2.49181
Epoch: 2/20, step: 22180, training_loss: 2.33638
Epoch: 2/20, step: 22200, training_loss: 1.71594
Epoch: 2/20, step: 22220, training_loss: 2.52441
Epoch: 2/20, step: 22240, training_loss: 3.22841
Epoch: 2/20, step: 22260, training_loss: 2.56362
Epoch: 2/20, step: 22280, training_loss: 1.90370
Epoch: 2/20, step: 22300, training_loss: 2.66749
Epoch: 2/20, step: 22320, training_loss: 2.28076
Epoch: 2/20, step: 22340, training_loss: 3.07638
Epoch: 2/20, step: 22360, training_loss: 1.92939
Epoch: 2/20, step: 22380, training_loss: 1.78271
Epoch: 2/20, step: 22400, training_loss: 1.68352
Epoch: 2/20, step: 22420, training_loss: 1.70127
Epoch: 2/20, step: 22440, training_loss: 1.87065
Epoch: 2/20, step: 22460, training_loss: 2.56823
Epoch: 2/20, step: 22480, training_loss: 1.82954
Epoch: 2/20, step: 22500, training_loss: 2.75484
Epoch: 2/20, step: 22520, training_loss: 1.90520
Epoch: 2/20, step: 22540, training_loss: 3.09488
Epoch: 2/20, step: 22560, training_loss: 2.16011
Epoch: 2/20, step: 22580, training_loss: 1.54298
Epoch: 2/20, step: 22600, training_loss: 1.98332
Epoch: 2/20, step: 22620, training_loss: 1.96346
Epoch: 2/20, step: 22640, training_loss: 2.02814
Epoch: 2/20, step: 22660, training_loss: 2.34456
Epoch: 2/20, step: 22680, training_loss: 1.99039
Epoch: 2/20, step: 22700, training_loss: 1.79736
Epoch: 2/20, step: 22720, training_loss: 1.87401
Epoch: 2/20, step: 22740, training_loss: 2.92318
Epoch: 2/20, step: 22760, training_loss: 2.58339
Epoch: 2/20, step: 22780, training_loss: 1.84026
Epoch: 2/20, step: 22800, training_loss: 2.09502
Epoch: 2/20, step: 22820, training_loss: 2.53455
Epoch: 2/20, step: 22840, training_loss: 2.16622
Epoch: 2/20, step: 22860, training_loss: 2.02699
Epoch: 2/20, step: 22880, training_loss: 2.69591
Epoch: 2/20, step: 22900, training_loss: 2.23534
Epoch: 2/20, step: 22920, training_loss: 2.17206
Epoch: 2/20, step: 22940, training_loss: 2.72630
Epoch: 2/20, step: 22960, training_loss: 2.54511
Epoch: 2/20, step: 22980, training_loss: 1.83785
Epoch: 2/20, step: 23000, training_loss: 1.68145
accuracy: 0.4, validation_loss: 2.220029830932617, num_samples: 100
Epoch: 2/20, step: 23020, training_loss: 2.07340
Epoch: 2/20, step: 23040, training_loss: 2.11280
Epoch: 2/20, step: 23060, training_loss: 2.44386
Epoch: 2/20, step: 23080, training_loss: 2.01901
Epoch: 2/20, step: 23100, training_loss: 1.99036
Epoch: 2/20, step: 23120, training_loss: 2.63146
Epoch: 2/20, step: 23140, training_loss: 1.97482
Epoch: 2/20, step: 23160, training_loss: 1.42339
Epoch: 2/20, step: 23180, training_loss: 2.08125
Epoch: 2/20, step: 23200, training_loss: 2.71103
Epoch: 2/20, step: 23220, training_loss: 2.18449
Epoch: 2/20, step: 23240, training_loss: 3.01859
Epoch: 2/20, step: 23260, training_loss: 2.53433
Epoch: 2/20, step: 23280, training_loss: 1.48941
Epoch: 2/20, step: 23300, training_loss: 1.37097
Epoch: 2/20, step: 23320, training_loss: 2.63198
Epoch: 2/20, step: 23340, training_loss: 2.15939
Epoch: 2/20, step: 23360, training_loss: 2.39389
Epoch: 2/20, step: 23380, training_loss: 1.73224
Epoch: 2/20, step: 23400, training_loss: 2.66580
Epoch: 2/20, step: 23420, training_loss: 1.98839
Epoch: 2/20, step: 23440, training_loss: 2.04664
Epoch: 2/20, step: 23460, training_loss: 2.62496
Epoch: 2/20, step: 23480, training_loss: 2.20992
Epoch: 2/20, step: 23500, training_loss: 2.28821
Epoch: 2/20, step: 23520, training_loss: 2.40957
Epoch: 2/20, step: 23540, training_loss: 2.73635
Epoch: 2/20, step: 23560, training_loss: 1.95254
Epoch: 2/20, step: 23580, training_loss: 2.90435
Epoch: 2/20, step: 23600, training_loss: 2.14525
Epoch: 2/20, step: 23620, training_loss: 1.38830
Epoch: 2/20, step: 23640, training_loss: 2.31313
Epoch: 2/20, step: 23660, training_loss: 2.31578
Epoch: 2/20, step: 23680, training_loss: 2.14109
Epoch: 2/20, step: 23700, training_loss: 1.53260
Epoch: 2/20, step: 23720, training_loss: 2.56766
Epoch: 2/20, step: 23740, training_loss: 1.74564
Epoch: 2/20, step: 23760, training_loss: 2.06075
Epoch: 2/20, step: 23780, training_loss: 1.74363
Epoch: 2/20, step: 23800, training_loss: 2.60639
Epoch: 2/20, step: 23820, training_loss: 1.93240
Epoch: 2/20, step: 23840, training_loss: 1.80228
Epoch: 2/20, step: 23860, training_loss: 2.51037
Epoch: 2/20, step: 23880, training_loss: 2.92477
Epoch: 2/20, step: 23900, training_loss: 2.15266
Epoch: 2/20, step: 23920, training_loss: 1.69441
Epoch: 2/20, step: 23940, training_loss: 2.07611
Epoch: 2/20, step: 23960, training_loss: 2.09188
Epoch: 2/20, step: 23980, training_loss: 2.09038
Epoch: 2/20, step: 24000, training_loss: 2.39025
accuracy: 0.39, validation_loss: 2.2748873233795166, num_samples: 100
Epoch: 2/20, step: 24020, training_loss: 2.43756
Epoch: 2/20, step: 24040, training_loss: 2.90470
Epoch: 2/20, step: 24060, training_loss: 2.81872
Epoch: 2/20, step: 24080, training_loss: 1.44206
Epoch: 2/20, step: 24100, training_loss: 2.22166
Epoch: 2/20, step: 24120, training_loss: 2.24628
Epoch: 2/20, step: 24140, training_loss: 2.29365
Epoch: 2/20, step: 24160, training_loss: 2.46242
Epoch: 2/20, step: 24180, training_loss: 2.04984
Epoch: 2/20, step: 24200, training_loss: 2.47587
Epoch: 2/20, step: 24220, training_loss: 2.74206
Epoch: 2/20, step: 24240, training_loss: 2.59838
Epoch: 2/20, step: 24260, training_loss: 2.42381
Epoch: 2/20, step: 24280, training_loss: 2.57168
Epoch: 2/20, step: 24300, training_loss: 2.26345
Epoch: 2/20, step: 24320, training_loss: 1.22681
Epoch: 2/20, step: 24340, training_loss: 2.69920
Epoch: 2/20, step: 24360, training_loss: 2.07298
Epoch: 2/20, step: 24380, training_loss: 2.12981
Epoch: 2/20, step: 24400, training_loss: 2.38171
Epoch: 2/20, step: 24420, training_loss: 1.79546
Epoch: 2/20, step: 24440, training_loss: 2.21259
Epoch: 2/20, step: 24460, training_loss: 1.97220
Epoch: 2/20, step: 24480, training_loss: 2.01831
Epoch: 2/20, step: 24500, training_loss: 1.90256
Epoch: 2/20, step: 24520, training_loss: 2.70038
Epoch: 2/20, step: 24540, training_loss: 2.63523
Epoch: 2/20, step: 24560, training_loss: 2.64986
Epoch: 2/20, step: 24580, training_loss: 2.06682
Epoch: 2/20, step: 24600, training_loss: 1.70634
Epoch: 2/20, step: 24620, training_loss: 2.46501
Epoch: 2/20, step: 24640, training_loss: 2.39815
Epoch: 2/20, step: 24660, training_loss: 1.70692
Epoch: 2/20, step: 24680, training_loss: 2.71305
Epoch: 2/20, step: 24700, training_loss: 2.14488
Epoch: 2/20, step: 24720, training_loss: 1.95944
Epoch: 2/20, step: 24740, training_loss: 2.03355
Epoch: 2/20, step: 24760, training_loss: 2.87670
Epoch: 2/20, step: 24780, training_loss: 2.15491
Epoch: 2/20, step: 24800, training_loss: 2.89750
Epoch: 2/20, step: 24820, training_loss: 2.42131
Epoch: 2/20, step: 24840, training_loss: 1.94569
Epoch: 2/20, step: 24860, training_loss: 2.28866
Epoch: 2/20, step: 24880, training_loss: 2.35256
Epoch: 2/20, step: 24900, training_loss: 1.74152
Epoch: 2/20, step: 24920, training_loss: 1.84984
Epoch: 2/20, step: 24940, training_loss: 2.95902
Epoch: 2/20, step: 24960, training_loss: 2.57537
Epoch: 2/20, step: 24980, training_loss: 3.22278
Epoch: 2/20, step: 25000, training_loss: 1.23387
accuracy: 0.47, validation_loss: 2.154103994369507, num_samples: 100
Epoch: 2/20, step: 25020, training_loss: 1.97495
Epoch: 2/20, step: 25040, training_loss: 2.49290
Epoch: 2/20, step: 25060, training_loss: 1.83676
Epoch: 2/20, step: 25080, training_loss: 1.80405
Epoch: 2/20, step: 25100, training_loss: 2.92319
Epoch: 2/20, step: 25120, training_loss: 2.24322
Epoch: 2/20, step: 25140, training_loss: 2.44706
Epoch: 2/20, step: 25160, training_loss: 2.16953
Epoch: 2/20, step: 25180, training_loss: 2.35537
Epoch: 2/20, step: 25200, training_loss: 1.99825
Epoch: 2/20, step: 25220, training_loss: 2.54089
Epoch: 2/20, step: 25240, training_loss: 2.72703
Epoch: 2/20, step: 25260, training_loss: 1.78324
Epoch: 2/20, step: 25280, training_loss: 1.58970
Epoch: 2/20, step: 25300, training_loss: 1.36433
Epoch: 2/20, step: 25320, training_loss: 2.25490
Epoch: 2/20, step: 25340, training_loss: 2.53262
Epoch: 2/20, step: 25360, training_loss: 1.78241
Epoch: 2/20, step: 25380, training_loss: 2.37796
Epoch: 2/20, step: 25400, training_loss: 2.54355
Epoch: 2/20, step: 25420, training_loss: 1.72378
Epoch: 2/20, step: 25440, training_loss: 2.76198
Epoch: 2/20, step: 25460, training_loss: 2.83226
Epoch: 2/20, step: 25480, training_loss: 2.25122
Epoch: 2/20, step: 25500, training_loss: 2.04200
Epoch: 2/20, step: 25520, training_loss: 2.02397
Epoch: 2/20, step: 25540, training_loss: 1.64092
Epoch: 2/20, step: 25560, training_loss: 2.07068
Epoch: 2/20, step: 25580, training_loss: 2.40336
Epoch: 2/20, step: 25600, training_loss: 2.38943
Epoch: 2/20, step: 25620, training_loss: 2.70882
Epoch: 2/20, step: 25640, training_loss: 2.03090
Epoch: 2/20, step: 25660, training_loss: 1.98128
Epoch: 2/20, step: 25680, training_loss: 2.58354
Epoch: 2/20, step: 25700, training_loss: 1.39543
Epoch: 2/20, step: 25720, training_loss: 2.39803
Epoch: 2/20, step: 25740, training_loss: 2.47254
Epoch: 2/20, step: 25760, training_loss: 2.18966
Epoch: 2/20, step: 25780, training_loss: 1.93343
Epoch: 2/20, step: 25800, training_loss: 2.44242
Epoch: 2/20, step: 25820, training_loss: 2.26765
Epoch: 2/20, step: 25840, training_loss: 3.04703
Epoch: 2/20, step: 25860, training_loss: 2.35335
Epoch: 2/20, step: 25880, training_loss: 2.34074
Epoch: 2/20, step: 25900, training_loss: 2.71126
Epoch: 2/20, step: 25920, training_loss: 1.90992
Epoch: 2/20, step: 25940, training_loss: 2.00572
Epoch: 2/20, step: 25960, training_loss: 2.11968
Epoch: 2/20, step: 25980, training_loss: 3.02341
Epoch: 2/20, step: 26000, training_loss: 2.09136
accuracy: 0.42, validation_loss: 2.2675933837890625, num_samples: 100
Epoch: 2/20, step: 26020, training_loss: 1.68046
Epoch: 2/20, step: 26040, training_loss: 1.96508
Epoch: 2/20, step: 26060, training_loss: 2.85709
Epoch: 2/20, step: 26080, training_loss: 1.01291
Epoch: 2/20, step: 26100, training_loss: 2.30981
Epoch: 2/20, step: 26120, training_loss: 2.36485
Epoch: 2/20, step: 26140, training_loss: 1.40663
Epoch: 2/20, step: 26160, training_loss: 2.12137
Epoch: 2/20, step: 26180, training_loss: 2.60863
Epoch: 2/20, step: 26200, training_loss: 2.63383
Epoch: 2/20, step: 26220, training_loss: 1.91239
Epoch: 2/20, step: 26240, training_loss: 2.09475
Epoch: 2/20, step: 26260, training_loss: 2.49074
Epoch: 2/20, step: 26280, training_loss: 2.29749
Epoch: 2/20, step: 26300, training_loss: 1.72349
Epoch: 2/20, step: 26320, training_loss: 1.80945
Epoch: 2/20, step: 26340, training_loss: 2.56057
Epoch: 2/20, step: 26360, training_loss: 2.31669
Epoch: 2/20, step: 26380, training_loss: 2.22010
Epoch: 2/20, step: 26400, training_loss: 2.34622
Epoch: 2/20, step: 26420, training_loss: 2.52108
Epoch: 2/20, step: 26440, training_loss: 2.34346
Epoch: 2/20, step: 26460, training_loss: 2.37772
Epoch: 2/20, step: 26480, training_loss: 2.23607
Epoch: 2/20, step: 26500, training_loss: 1.85442
Epoch: 2/20, step: 26520, training_loss: 1.68396
Epoch: 2/20, step: 26540, training_loss: 1.60061
Epoch: 2/20, step: 26560, training_loss: 2.39387
Epoch: 2/20, step: 26580, training_loss: 2.80060
Epoch: 2/20, step: 26600, training_loss: 1.82507
Epoch: 2/20, step: 26620, training_loss: 2.91907
Epoch: 2/20, step: 26640, training_loss: 2.96528
Epoch: 2/20, step: 26660, training_loss: 2.49350
Epoch: 2/20, step: 26680, training_loss: 2.37320
Epoch: 2/20, step: 26700, training_loss: 2.43010
Epoch: 2/20, step: 26720, training_loss: 2.41101
Epoch: 2/20, step: 26740, training_loss: 2.13717
Epoch: 2/20, step: 26760, training_loss: 2.24561
Epoch: 2/20, step: 26780, training_loss: 2.22319
Epoch: 2/20, step: 26800, training_loss: 1.89714
Epoch: 2/20, step: 26820, training_loss: 1.86383
Epoch: 2/20, step: 26840, training_loss: 2.96006
Epoch: 2/20, step: 26860, training_loss: 1.52021
Epoch: 2/20, step: 26880, training_loss: 1.99988
Epoch: 2/20, step: 26900, training_loss: 1.97110
Epoch: 2/20, step: 26920, training_loss: 1.60465
Epoch: 2/20, step: 26940, training_loss: 2.04676
Epoch: 2/20, step: 26960, training_loss: 2.31148
Epoch: 2/20, step: 26980, training_loss: 2.87797
Epoch: 2/20, step: 27000, training_loss: 2.15900
accuracy: 0.33, validation_loss: 2.546354293823242, num_samples: 100
Epoch: 2/20, step: 27020, training_loss: 2.29200
Epoch: 2/20, step: 27040, training_loss: 2.65326
Epoch: 2/20, step: 27060, training_loss: 1.28426
Epoch: 2/20, step: 27080, training_loss: 1.98045
Epoch: 2/20, step: 27100, training_loss: 2.30661
Epoch: 2/20, step: 27120, training_loss: 1.57520
Epoch: 2/20, step: 27140, training_loss: 2.10199
Epoch: 2/20, step: 27160, training_loss: 2.50180
Epoch: 2/20, step: 27180, training_loss: 1.79036
Epoch: 2/20, step: 27200, training_loss: 2.50735
Epoch: 2/20, step: 27220, training_loss: 1.67212
Epoch: 2/20, step: 27240, training_loss: 2.51956
Epoch: 2/20, step: 27260, training_loss: 2.25271
Epoch: 2/20, step: 27280, training_loss: 1.67075
Epoch: 2/20, step: 27300, training_loss: 2.81944
Epoch: 2/20, step: 27320, training_loss: 2.15327
Epoch: 2/20, step: 27340, training_loss: 2.50804
Epoch: 2/20, step: 27360, training_loss: 2.06097
Epoch: 2/20, step: 27380, training_loss: 1.71924
Epoch: 2/20, step: 27400, training_loss: 3.11159
Epoch: 2/20, step: 27420, training_loss: 1.49570
Epoch: 2/20, step: 27440, training_loss: 2.69437
Epoch: 2/20, step: 27460, training_loss: 1.93126
Epoch: 2/20, step: 27480, training_loss: 2.39272
Epoch: 2/20, step: 27500, training_loss: 2.44235
Epoch: 2/20, step: 27520, training_loss: 2.14533
Epoch: 2/20, step: 27540, training_loss: 2.79699
Epoch: 2/20, step: 27560, training_loss: 2.33221
Epoch: 2/20, step: 27580, training_loss: 1.35332
Epoch: 2/20, step: 27600, training_loss: 2.51631
Epoch: 2/20, step: 27620, training_loss: 2.06317
Epoch: 2/20, step: 27640, training_loss: 1.96595
Epoch: 2/20, step: 27660, training_loss: 1.83109
Epoch: 2/20, step: 27680, training_loss: 2.28323
Epoch: 2/20, step: 27700, training_loss: 2.26407
Epoch: 2/20, step: 27720, training_loss: 2.07835
Epoch: 2/20, step: 27740, training_loss: 2.58242
Epoch: 2/20, step: 27760, training_loss: 1.39840
Epoch: 2/20, step: 27780, training_loss: 1.89897
Epoch: 2/20, step: 27800, training_loss: 2.11938
Epoch: 2/20, step: 27820, training_loss: 2.12524
Epoch: 2/20, step: 27840, training_loss: 2.81908
Epoch: 2/20, step: 27860, training_loss: 1.96355
Epoch: 2/20, step: 27880, training_loss: 2.36574
Epoch: 2/20, step: 27900, training_loss: 2.23963
Epoch: 2/20, step: 27920, training_loss: 2.32821
Epoch: 2/20, step: 27940, training_loss: 2.80539
Epoch: 2/20, step: 27960, training_loss: 2.12410
Epoch: 2/20, step: 27980, training_loss: 2.40442
Epoch: 2/20, step: 28000, training_loss: 2.48374
accuracy: 0.46, validation_loss: 2.1453568935394287, num_samples: 100
Epoch: 2/20, step: 28020, training_loss: 2.58197
Epoch: 2/20, step: 28040, training_loss: 2.12658
Epoch: 2/20, step: 28060, training_loss: 2.28851
Epoch: 2/20, step: 28080, training_loss: 2.18654
Epoch: 2/20, step: 28100, training_loss: 2.89921
Epoch: 2/20, step: 28120, training_loss: 2.18878
Epoch: 2/20, step: 28140, training_loss: 2.52396
Epoch: 2/20, step: 28160, training_loss: 2.22579
Epoch: 2/20, step: 28180, training_loss: 1.87466
Epoch: 2/20, step: 28200, training_loss: 2.73533
Epoch: 2/20, step: 28220, training_loss: 2.01877
Epoch: 2/20, step: 28240, training_loss: 2.23147
Epoch: 2/20, step: 28260, training_loss: 2.18733
Epoch: 2/20, step: 28280, training_loss: 2.56855
Epoch: 2/20, step: 28300, training_loss: 1.65090
Epoch: 2/20, step: 28320, training_loss: 1.36656
Epoch: 2/20, step: 28340, training_loss: 1.86499
Epoch: 2/20, step: 28360, training_loss: 2.61446
Epoch: 2/20, step: 28380, training_loss: 2.45038
Epoch: 2/20, step: 28400, training_loss: 1.92844
Epoch: 2/20, step: 28420, training_loss: 1.63497
Epoch: 2/20, step: 28440, training_loss: 2.56724
Epoch: 2/20, step: 28460, training_loss: 1.38404
Epoch: 2/20, step: 28480, training_loss: 2.49298
Epoch: 2/20, step: 28500, training_loss: 1.91502
Epoch: 2/20, step: 28520, training_loss: 1.52194
Epoch: 2/20, step: 28540, training_loss: 2.31400
Epoch: 2/20, step: 28560, training_loss: 2.06266
Epoch: 2/20, step: 28580, training_loss: 2.04005
Epoch: 2/20, step: 28600, training_loss: 2.35290
Epoch: 2/20, step: 28620, training_loss: 2.74220
Epoch: 2/20, step: 28640, training_loss: 2.64053
Epoch: 2/20, step: 28660, training_loss: 2.39391
Epoch: 2/20, step: 28680, training_loss: 2.58750
Epoch: 2/20, step: 28700, training_loss: 1.73020
Epoch: 2/20, step: 28720, training_loss: 2.42387
Epoch: 2/20, step: 28740, training_loss: 2.39335
Epoch: 2/20, step: 28760, training_loss: 1.66867
Epoch: 2/20, step: 28780, training_loss: 2.34412
Epoch: 2/20, step: 28800, training_loss: 1.33218
Epoch: 2/20, step: 28820, training_loss: 2.79278
Epoch: 2/20, step: 28840, training_loss: 2.30490
Epoch: 2/20, step: 28860, training_loss: 2.57284
Epoch: 2/20, step: 28880, training_loss: 1.99821
Epoch: 2/20, step: 28900, training_loss: 2.29667
Epoch: 2/20, step: 28920, training_loss: 1.84388
Epoch: 2/20, step: 28940, training_loss: 1.88664
Epoch: 2/20, step: 28960, training_loss: 2.52347
Epoch: 2/20, step: 28980, training_loss: 2.47066
Epoch: 2/20, step: 29000, training_loss: 1.76286
accuracy: 0.38, validation_loss: 2.5982000827789307, num_samples: 100
Epoch: 2/20, step: 29020, training_loss: 2.50900
Epoch: 2/20, step: 29040, training_loss: 2.71965
Epoch: 2/20, step: 29060, training_loss: 3.18470
Epoch: 2/20, step: 29080, training_loss: 2.90418
Epoch: 2/20, step: 29100, training_loss: 1.38186
Epoch: 2/20, step: 29120, training_loss: 1.53041
Epoch: 2/20, step: 29140, training_loss: 1.51730
Epoch: 2/20, step: 29160, training_loss: 1.09894
Epoch: 2/20, step: 29180, training_loss: 2.80436
Epoch: 2/20, step: 29200, training_loss: 2.70795
Epoch: 2/20, step: 29220, training_loss: 2.64655
Epoch: 2/20, step: 29240, training_loss: 2.23531
Epoch: 2/20, step: 29260, training_loss: 2.38062
Epoch: 2/20, step: 29280, training_loss: 1.85169
Epoch: 2/20, step: 29300, training_loss: 2.79781
Epoch: 2/20, step: 29320, training_loss: 2.39733
Epoch: 2/20, step: 29340, training_loss: 3.23035
Epoch: 2/20, step: 29360, training_loss: 2.22996
Epoch: 2/20, step: 29380, training_loss: 2.78098
Epoch: 2/20, step: 29400, training_loss: 1.85072
Epoch: 2/20, step: 29420, training_loss: 3.84824
Epoch: 2/20, step: 29440, training_loss: 2.45254
Epoch: 2/20, step: 29460, training_loss: 2.12156
Epoch: 2/20, step: 29480, training_loss: 3.49641
Epoch: 2/20, step: 29500, training_loss: 1.75783
Epoch: 2/20, step: 29520, training_loss: 1.33236
Epoch: 2/20, step: 29540, training_loss: 1.61144
Epoch: 2/20, step: 29560, training_loss: 1.41434
Epoch: 2/20, step: 29580, training_loss: 2.58465
Epoch: 2/20, step: 29600, training_loss: 2.60394
Epoch: 2/20, step: 29620, training_loss: 1.98743
Epoch: 2/20, step: 29640, training_loss: 2.32215
Epoch: 2/20, step: 29660, training_loss: 1.83484
Epoch: 2/20, step: 29680, training_loss: 3.50762
Epoch: 2/20, step: 29700, training_loss: 1.83489
Epoch: 2/20, step: 29720, training_loss: 2.33840
Epoch: 2/20, step: 29740, training_loss: 2.72155
Epoch: 2/20, step: 29760, training_loss: 2.29286
Epoch: 2/20, step: 29780, training_loss: 1.93308
Epoch: 2/20, step: 29800, training_loss: 2.10324
Epoch: 2/20, step: 29820, training_loss: 1.90469
Epoch: 2/20, step: 29840, training_loss: 3.22842
Epoch: 2/20, step: 29860, training_loss: 2.41544
Epoch: 2/20, step: 29880, training_loss: 1.70135
Epoch: 2/20, step: 29900, training_loss: 2.76118
Epoch: 2/20, step: 29920, training_loss: 1.65107
Epoch: 2/20, step: 29940, training_loss: 2.30386
Epoch: 2/20, step: 29960, training_loss: 2.80263
Epoch: 2/20, step: 29980, training_loss: 1.99628
Epoch: 2/20, step: 30000, training_loss: 1.61606
accuracy: 0.47, validation_loss: 2.0283291339874268, num_samples: 100
Epoch: 2/20, step: 30020, training_loss: 2.11352
Epoch: 2/20, step: 30040, training_loss: 1.71957
Epoch: 2/20, step: 30060, training_loss: 2.14110
Epoch: 2/20, step: 30080, training_loss: 2.04485
Epoch: 2/20, step: 30100, training_loss: 3.09344
Epoch: 2/20, step: 30120, training_loss: 2.50748
Epoch: 2/20, step: 30140, training_loss: 2.61372
Epoch: 2/20, step: 30160, training_loss: 2.14623
Epoch: 2/20, step: 30180, training_loss: 2.13770
Epoch: 2/20, step: 30200, training_loss: 2.16168
Epoch: 2/20, step: 30220, training_loss: 1.18876
Epoch: 2/20, step: 30240, training_loss: 1.95183
Epoch: 2/20, step: 30260, training_loss: 1.78325
Epoch: 2/20, step: 30280, training_loss: 2.04139
Epoch: 2/20, step: 30300, training_loss: 2.13449
Epoch: 2/20, step: 30320, training_loss: 2.27995
Epoch: 2/20, step: 30340, training_loss: 2.60582
Epoch: 2/20, step: 30360, training_loss: 2.20955
Epoch: 2/20, step: 30380, training_loss: 3.00318
Epoch: 2/20, step: 30400, training_loss: 2.13429
Epoch: 2/20, step: 30420, training_loss: 2.98182
Epoch: 2/20, step: 30440, training_loss: 3.00379
Epoch: 2/20, step: 30460, training_loss: 2.17703
Epoch: 2/20, step: 30480, training_loss: 2.48365
Epoch: 2/20, step: 30500, training_loss: 1.98266
Epoch: 2/20, step: 30520, training_loss: 1.63602
Epoch: 2/20, step: 30540, training_loss: 2.34660
Epoch: 2/20, step: 30560, training_loss: 2.13016
Epoch: 2/20, step: 30580, training_loss: 2.15668
Epoch: 2/20, step: 30600, training_loss: 2.57208
Epoch: 2/20, step: 30620, training_loss: 3.31279
Epoch: 2/20, step: 30640, training_loss: 1.65452
Epoch: 2/20, step: 30660, training_loss: 2.45296
Epoch: 2/20, step: 30680, training_loss: 3.28828
Epoch: 2/20, step: 30700, training_loss: 1.88173
Epoch: 2/20, step: 30720, training_loss: 2.41106
Epoch: 2/20, step: 30740, training_loss: 1.79933
Epoch: 2/20, step: 30760, training_loss: 2.30027
Epoch: 2/20, step: 30780, training_loss: 1.87846
Epoch: 2/20, step: 30800, training_loss: 1.44861
Epoch: 2/20, step: 30820, training_loss: 1.68435
Epoch: 2/20, step: 30840, training_loss: 1.88847
Epoch: 2/20, step: 30860, training_loss: 1.43410
Epoch: 2/20, step: 30880, training_loss: 1.61572
Epoch: 2/20, step: 30900, training_loss: 2.96622
Epoch: 2/20, step: 30920, training_loss: 1.87928
Epoch: 2/20, step: 30940, training_loss: 1.93180
Epoch: 2/20, step: 30960, training_loss: 2.30456
Epoch: 2/20, step: 30980, training_loss: 2.20302
Epoch: 2/20, step: 31000, training_loss: 2.25483
accuracy: 0.35, validation_loss: 2.236703872680664, num_samples: 100
Epoch: 2/20, step: 31020, training_loss: 1.75523
Epoch: 2/20, step: 31040, training_loss: 2.60102
Epoch: 2/20, step: 31060, training_loss: 1.40504
Epoch: 2/20, step: 31080, training_loss: 2.83341
Epoch: 2/20, step: 31100, training_loss: 2.42818
Epoch: 2/20, step: 31120, training_loss: 2.77635
Epoch: 2/20, step: 31140, training_loss: 2.30078
Epoch: 2/20, step: 31160, training_loss: 2.16748
Epoch: 2/20, step: 31180, training_loss: 1.76700
Epoch: 2/20, step: 31200, training_loss: 2.55303
Epoch: 2/20, step: 31220, training_loss: 2.57274
Epoch: 2/20, step: 31240, training_loss: 1.90735
Epoch: 2/20, step: 31260, training_loss: 1.54398
Epoch: 2/20, step: 31280, training_loss: 1.96264
Epoch: 2/20, step: 31300, training_loss: 1.50074
Epoch: 2/20, step: 31320, training_loss: 2.34687
Epoch: 2/20, step: 31340, training_loss: 2.05018
Epoch: 2/20, step: 31360, training_loss: 2.32415
Epoch: 2/20, step: 31380, training_loss: 2.94352
Epoch: 2/20, step: 31400, training_loss: 2.20837
Epoch: 2/20, step: 31420, training_loss: 2.91566
Epoch: 2/20, step: 31440, training_loss: 1.51014
Epoch: 2/20, step: 31460, training_loss: 2.36724
Epoch: 2/20, step: 31480, training_loss: 2.34768
Epoch: 2/20, step: 31500, training_loss: 2.12025
Epoch: 2/20, step: 31520, training_loss: 2.02907
Epoch: 2/20, step: 31540, training_loss: 2.30962
Epoch: 2/20, step: 31560, training_loss: 1.89177
Epoch: 2/20, step: 31580, training_loss: 2.66919
Epoch: 2/20, step: 31600, training_loss: 2.54466
Epoch: 2/20, step: 31620, training_loss: 1.72273
Epoch: 2/20, step: 31640, training_loss: 1.77334
Epoch: 2/20, step: 31660, training_loss: 2.85765
Epoch: 2/20, step: 31680, training_loss: 1.89095
Epoch: 2/20, step: 31700, training_loss: 1.65998
Epoch: 2/20, step: 31720, training_loss: 2.35031
Epoch: 2/20, step: 31740, training_loss: 1.80559
Epoch: 2/20, step: 31760, training_loss: 2.05407
Epoch: 2/20, step: 31780, training_loss: 1.81608
Epoch: 2/20, step: 31800, training_loss: 1.58479
Epoch: 2/20, step: 31820, training_loss: 2.25172
Epoch: 2/20, step: 31840, training_loss: 2.43647
Epoch: 2/20, step: 31860, training_loss: 1.95273
Epoch: 2/20, step: 31880, training_loss: 2.72161
Epoch: 2/20, step: 31900, training_loss: 2.26818
Epoch: 2/20, step: 31920, training_loss: 2.18469
Epoch: 2/20, step: 31940, training_loss: 1.85852
Epoch: 2/20, step: 31960, training_loss: 1.85787
Epoch: 2/20, step: 31980, training_loss: 1.51163
Epoch: 2/20, step: 32000, training_loss: 3.12208
accuracy: 0.48, validation_loss: 2.202782392501831, num_samples: 100
Epoch: 2/20, step: 32020, training_loss: 3.03702
Epoch: 2/20, step: 32040, training_loss: 1.76524
Epoch: 2/20, step: 32060, training_loss: 2.45243
Epoch: 2/20, step: 32080, training_loss: 2.17999
Epoch: 2/20, step: 32100, training_loss: 2.22544
Epoch: 2/20, step: 32120, training_loss: 2.57242
Epoch: 2/20, step: 32140, training_loss: 2.94905
Epoch: 2/20, step: 32160, training_loss: 2.65888
Epoch: 2/20, step: 32180, training_loss: 2.45576
Epoch: 2/20, step: 32200, training_loss: 2.12204
Epoch: 2/20, step: 32220, training_loss: 2.20506
Epoch: 2/20, step: 32240, training_loss: 1.96347
Epoch: 2/20, step: 32260, training_loss: 2.64223
Epoch: 2/20, step: 32280, training_loss: 2.54013
Epoch: 2/20, step: 32300, training_loss: 2.14194
Epoch: 2/20, step: 32320, training_loss: 2.51616
Epoch: 2/20, step: 32340, training_loss: 2.53580
Epoch: 2/20, step: 32360, training_loss: 2.15324
Epoch: 2/20, step: 32380, training_loss: 1.82015
Epoch: 2/20, step: 32400, training_loss: 2.95693
Epoch: 2/20, step: 32420, training_loss: 2.65319
Epoch: 2/20, step: 32440, training_loss: 2.38875
Epoch: 2/20, step: 32460, training_loss: 2.04555
Epoch: 2/20, step: 32480, training_loss: 2.09779
Epoch: 2/20, step: 32500, training_loss: 2.11010
Epoch: 2/20, step: 32520, training_loss: 1.62945
Epoch: 2/20, step: 32540, training_loss: 2.03989
Epoch: 2/20, step: 32560, training_loss: 2.99561
Epoch: 2/20, step: 32580, training_loss: 2.98778
Epoch: 2/20, step: 32600, training_loss: 1.74019
Epoch: 2/20, step: 32620, training_loss: 2.69789
Epoch: 2/20, step: 32640, training_loss: 2.43613
Epoch: 2/20, step: 32660, training_loss: 2.11574
Epoch: 2/20, step: 32680, training_loss: 1.90981
Epoch: 2/20, step: 32700, training_loss: 1.57552
Epoch: 2/20, step: 32720, training_loss: 2.34829
Epoch: 2/20, step: 32740, training_loss: 3.07206
Epoch: 2/20, step: 32760, training_loss: 2.40771
Epoch: 2/20, step: 32780, training_loss: 2.64309
Epoch: 2/20, step: 32800, training_loss: 2.36995
Epoch: 2/20, step: 32820, training_loss: 1.64915
Epoch: 2/20, step: 32840, training_loss: 2.65333
Epoch: 2/20, step: 32860, training_loss: 1.99953
Epoch: 2/20, step: 32880, training_loss: 2.16519
Epoch: 2/20, step: 32900, training_loss: 1.30065
Epoch: 2/20, step: 32920, training_loss: 1.81737
Epoch: 2/20, step: 32940, training_loss: 2.77940
Epoch: 2/20, step: 32960, training_loss: 2.25531
Epoch: 2/20, step: 32980, training_loss: 1.98646
Epoch: 2/20, step: 33000, training_loss: 2.32334
accuracy: 0.5, validation_loss: 1.9786410331726074, num_samples: 100
Epoch: 2/20, step: 33020, training_loss: 1.72762
Epoch: 2/20, step: 33040, training_loss: 2.13519
Epoch: 2/20, step: 33060, training_loss: 2.43858
Epoch: 2/20, step: 33080, training_loss: 1.07949
Epoch: 2/20, step: 33100, training_loss: 2.13279
Epoch: 2/20, step: 33120, training_loss: 1.98294
Epoch: 2/20, step: 33140, training_loss: 2.33896
Epoch: 2/20, step: 33160, training_loss: 2.16454
Epoch: 2/20, step: 33180, training_loss: 2.17028
Epoch: 2/20, step: 33200, training_loss: 1.82345
Epoch: 2/20, step: 33220, training_loss: 1.32164
Epoch: 2/20, step: 33240, training_loss: 2.47706
Epoch: 2/20, step: 33260, training_loss: 1.96392
Epoch: 2/20, step: 33280, training_loss: 1.62871
Epoch: 2/20, step: 33300, training_loss: 3.01077
Epoch: 2/20, step: 33320, training_loss: 1.77661
Epoch: 2/20, step: 33340, training_loss: 3.15010
Epoch: 2/20, step: 33360, training_loss: 2.97018
Epoch: 2/20, step: 33380, training_loss: 2.87728
Epoch: 2/20, step: 33400, training_loss: 2.29930
Epoch: 2/20, step: 33420, training_loss: 2.58650
Epoch: 2/20, step: 33440, training_loss: 2.12852
Epoch: 2/20, step: 33460, training_loss: 1.79503
Epoch: 2/20, step: 33480, training_loss: 2.32860
Epoch: 2/20, step: 33500, training_loss: 1.24946
Epoch: 2/20, step: 33520, training_loss: 2.75028
Epoch: 2/20, step: 33540, training_loss: 2.27206
Epoch: 2/20, step: 33560, training_loss: 2.06023
Epoch: 2/20, step: 33580, training_loss: 2.68729
Epoch: 2/20, step: 33600, training_loss: 1.66794
Epoch: 2/20, step: 33620, training_loss: 2.37717
Epoch: 2/20, step: 33640, training_loss: 2.07385
Epoch: 2/20, step: 33660, training_loss: 2.19077
Epoch: 2/20, step: 33680, training_loss: 1.86494
Epoch: 2/20, step: 33700, training_loss: 1.23885
Epoch: 2/20, step: 33720, training_loss: 2.14662
Epoch: 2/20, step: 33740, training_loss: 1.80894
Epoch: 2/20, step: 33760, training_loss: 2.46277
Epoch: 2/20, step: 33780, training_loss: 2.55086
Epoch: 2/20, step: 33800, training_loss: 1.45690
Epoch: 2/20, step: 33820, training_loss: 2.00160
Epoch: 2/20, step: 33840, training_loss: 2.60607
Epoch: 2/20, step: 33860, training_loss: 1.69506
Epoch: 2/20, step: 33880, training_loss: 2.57223
Epoch: 2/20, step: 33900, training_loss: 1.78516
Epoch: 2/20, step: 33920, training_loss: 2.52198
Epoch: 2/20, step: 33940, training_loss: 2.18091
Epoch: 2/20, step: 33960, training_loss: 2.05244
Epoch: 2/20, step: 33980, training_loss: 2.05937
Epoch: 2/20, step: 34000, training_loss: 1.78116
accuracy: 0.38, validation_loss: 2.1955175399780273, num_samples: 100
Epoch: 2/20, step: 34020, training_loss: 2.15317
Epoch: 2/20, step: 34040, training_loss: 1.92130
Epoch: 2/20, step: 34060, training_loss: 2.12465
Epoch: 2/20, step: 34080, training_loss: 2.54863
Epoch: 2/20, step: 34100, training_loss: 2.57920
Epoch: 2/20, step: 34120, training_loss: 1.71320
Epoch: 2/20, step: 34140, training_loss: 2.85028
Epoch: 2/20, step: 34160, training_loss: 2.26528
Epoch: 2/20, step: 34180, training_loss: 1.91664
Epoch: 2/20, step: 34200, training_loss: 2.64755
Epoch: 2/20, step: 34220, training_loss: 2.21985
Epoch: 2/20, step: 34240, training_loss: 1.95865
Epoch: 2/20, step: 34260, training_loss: 1.89787
Epoch: 2/20, step: 34280, training_loss: 2.12550
Epoch: 2/20, step: 34300, training_loss: 2.45710
Epoch: 2/20, step: 34320, training_loss: 2.07594
Epoch: 2/20, step: 34340, training_loss: 2.63924
Epoch: 2/20, step: 34360, training_loss: 1.50352
Epoch: 2/20, step: 34380, training_loss: 1.66437
Epoch: 2/20, step: 34400, training_loss: 2.90550
Epoch: 2/20, step: 34420, training_loss: 2.68398
Epoch: 2/20, step: 34440, training_loss: 1.79045
Epoch: 2/20, step: 34460, training_loss: 2.55756
Epoch: 2/20, step: 34480, training_loss: 2.76588
Epoch: 2/20, step: 34500, training_loss: 2.77118
Epoch: 2/20, step: 34520, training_loss: 2.09008
Epoch: 2/20, step: 34540, training_loss: 1.86764
Epoch: 2/20, step: 34560, training_loss: 2.12173
Epoch: 2/20, step: 34580, training_loss: 1.52010
Epoch: 2/20, step: 34600, training_loss: 3.13260
Epoch: 2/20, step: 34620, training_loss: 1.72820
Epoch: 2/20, step: 34640, training_loss: 2.72995
Epoch: 2/20, step: 34660, training_loss: 2.32264
Epoch: 2/20, step: 34680, training_loss: 2.57804
Epoch: 2/20, step: 34700, training_loss: 2.55002
Epoch: 2/20, step: 34720, training_loss: 2.06343
Epoch: 2/20, step: 34740, training_loss: 2.38674
Epoch: 2/20, step: 34760, training_loss: 1.85697
Epoch: 2/20, step: 34780, training_loss: 2.85052
Epoch: 2/20, step: 34800, training_loss: 1.98826
Epoch: 2/20, step: 34820, training_loss: 1.59001
Epoch: 2/20, step: 34840, training_loss: 2.39939
Epoch: 2/20, step: 34860, training_loss: 2.36964
Epoch: 2/20, step: 34880, training_loss: 1.61183
Epoch: 2/20, step: 34900, training_loss: 2.30785
Epoch: 2/20, step: 34920, training_loss: 3.02439
Epoch: 2/20, step: 34940, training_loss: 2.36214
Epoch: 2/20, step: 34960, training_loss: 1.15541
Epoch: 2/20, step: 34980, training_loss: 2.17946
Epoch: 2/20, step: 35000, training_loss: 2.51133
accuracy: 0.41, validation_loss: 2.0594048500061035, num_samples: 100
Epoch: 2/20, step: 35020, training_loss: 1.95561
Epoch: 2/20, step: 35040, training_loss: 1.46679
Epoch: 2/20, step: 35060, training_loss: 2.22654
Epoch: 2/20, step: 35080, training_loss: 2.14521
Epoch: 2/20, step: 35100, training_loss: 2.46289
Epoch: 2/20, step: 35120, training_loss: 1.27339
Epoch: 2/20, step: 35140, training_loss: 2.65140
Epoch: 2/20, step: 35160, training_loss: 1.81715
Epoch: 2/20, step: 35180, training_loss: 1.45960
Epoch: 2/20, step: 35200, training_loss: 2.14716
Epoch: 2/20, step: 35220, training_loss: 2.38132
Epoch: 2/20, step: 35240, training_loss: 2.86065
Epoch: 2/20, step: 35260, training_loss: 2.31268
Epoch: 2/20, step: 35280, training_loss: 2.58600
Epoch: 2/20, step: 35300, training_loss: 2.36096
Epoch: 2/20, step: 35320, training_loss: 2.47818
Epoch: 2/20, step: 35340, training_loss: 2.63503
Epoch: 2/20, step: 35360, training_loss: 2.49423
Epoch: 2/20, step: 35380, training_loss: 2.48938
Epoch: 2/20, step: 35400, training_loss: 1.10897
Epoch: 2/20, step: 35420, training_loss: 2.29660
Epoch: 2/20, step: 35440, training_loss: 1.98635
Epoch: 2/20, step: 35460, training_loss: 2.90280
Epoch: 2/20, step: 35480, training_loss: 2.76966
Epoch: 2/20, step: 35500, training_loss: 1.85434
Epoch: 2/20, step: 35520, training_loss: 3.12948
Epoch: 2/20, step: 35540, training_loss: 2.55919
Epoch: 2/20, step: 35560, training_loss: 2.44283
Epoch: 2/20, step: 35580, training_loss: 2.56258
Epoch: 2/20, step: 35600, training_loss: 1.88496
Epoch: 2/20, step: 35620, training_loss: 1.84503
Epoch: 2/20, step: 35640, training_loss: 2.54169
Epoch: 2/20, step: 35660, training_loss: 2.58396
Epoch: 2/20, step: 35680, training_loss: 2.12681
Epoch: 2/20, step: 35700, training_loss: 2.21291
Epoch: 2/20, step: 35720, training_loss: 2.49102
Epoch: 2/20, step: 35740, training_loss: 2.33122
Epoch: 2/20, step: 35760, training_loss: 2.28853
Epoch: 2/20, step: 35780, training_loss: 2.30696
Epoch: 2/20, step: 35800, training_loss: 2.96354
Epoch: 2/20, step: 35820, training_loss: 2.45955
Epoch: 2/20, step: 35840, training_loss: 1.75180
Epoch: 2/20, step: 35860, training_loss: 1.94979
Epoch: 2/20, step: 35880, training_loss: 2.64637
Epoch: 2/20, step: 35900, training_loss: 2.07273
Epoch: 2/20, step: 35920, training_loss: 2.76614
Epoch: 2/20, step: 35940, training_loss: 2.65999
Epoch: 2/20, step: 35960, training_loss: 2.17843
Epoch: 2/20, step: 35980, training_loss: 2.64420
Epoch: 2/20, step: 36000, training_loss: 1.27842
accuracy: 0.43, validation_loss: 2.2418720722198486, num_samples: 100
Epoch: 2/20, step: 36020, training_loss: 2.02144
Epoch: 2/20, step: 36040, training_loss: 1.57776
Epoch: 2/20, step: 36060, training_loss: 2.71205
Epoch: 2/20, step: 36080, training_loss: 2.15402
Epoch: 2/20, step: 36100, training_loss: 3.14286
Epoch: 2/20, step: 36120, training_loss: 2.68728
Epoch: 2/20, step: 36140, training_loss: 3.05460
Epoch: 2/20, step: 36160, training_loss: 2.08490
Epoch: 2/20, step: 36180, training_loss: 1.86326
Epoch: 2/20, step: 36200, training_loss: 1.78631
Epoch: 2/20, step: 36220, training_loss: 2.00575
Epoch: 2/20, step: 36240, training_loss: 1.42230
Epoch: 2/20, step: 36260, training_loss: 2.43005
Epoch: 2/20, step: 36280, training_loss: 2.39429
Epoch: 2/20, step: 36300, training_loss: 1.79024
Epoch: 2/20, step: 36320, training_loss: 1.61012
Epoch: 2/20, step: 36340, training_loss: 2.29000
Epoch: 2/20, step: 36360, training_loss: 2.44082
Epoch: 2/20, step: 36380, training_loss: 2.51391
Epoch: 2/20, step: 36400, training_loss: 1.52151
Epoch: 2/20, step: 36420, training_loss: 3.27116
Epoch: 2/20, step: 36440, training_loss: 1.95063
Epoch: 2/20, step: 36460, training_loss: 1.69011
Epoch: 2/20, step: 36480, training_loss: 2.63336
Epoch: 2/20, step: 36500, training_loss: 2.60286
Epoch: 2/20, step: 36520, training_loss: 2.56087
Epoch: 2/20, step: 36540, training_loss: 1.51454
Epoch: 2/20, step: 36560, training_loss: 2.09216
Epoch: 2/20, step: 36580, training_loss: 2.34516
Epoch: 2/20, step: 36600, training_loss: 2.20828
Epoch: 2/20, step: 36620, training_loss: 2.49333
Epoch: 2/20, step: 36640, training_loss: 2.47047
Epoch: 2/20, step: 36660, training_loss: 1.83985
Epoch: 2/20, step: 36680, training_loss: 1.98981
Epoch: 2/20, step: 36700, training_loss: 1.78351
Epoch: 2/20, step: 36720, training_loss: 2.62998
Epoch: 2/20, step: 36740, training_loss: 2.18662
Epoch: 2/20, step: 36760, training_loss: 2.71375
Epoch: 2/20, step: 36780, training_loss: 1.54632
Epoch: 2/20, step: 36800, training_loss: 2.07373
Epoch: 2/20, step: 36820, training_loss: 1.77751
Epoch: 2/20, step: 36840, training_loss: 2.64232
Epoch: 2/20, step: 36860, training_loss: 2.39643
Epoch: 2/20, step: 36880, training_loss: 2.41360
Epoch: 2/20, step: 36900, training_loss: 2.58644
Epoch: 2/20, step: 36920, training_loss: 2.00142
Epoch: 2/20, step: 36940, training_loss: 2.43881
Epoch: 2/20, step: 36960, training_loss: 2.76895
Epoch: 2/20, step: 36980, training_loss: 2.38182
Epoch: 2/20, step: 37000, training_loss: 2.36876
accuracy: 0.48, validation_loss: 2.0326879024505615, num_samples: 100
Epoch: 2/20, step: 37020, training_loss: 1.97330
Epoch: 2/20, step: 37040, training_loss: 2.01366
Epoch: 2/20, step: 37060, training_loss: 2.33279
Epoch: 2/20, step: 37080, training_loss: 3.36491
Epoch: 2/20, step: 37100, training_loss: 1.90445
Epoch: 2/20, step: 37120, training_loss: 3.01644
Epoch: 2/20, step: 37140, training_loss: 1.65217
Epoch: 2/20, step: 37160, training_loss: 2.96640
Epoch: 2/20, step: 37180, training_loss: 2.44279
Epoch: 2/20, step: 37200, training_loss: 2.07598
Epoch: 2/20, step: 37220, training_loss: 2.17040
Epoch: 2/20, step: 37240, training_loss: 1.68068
Epoch: 2/20, step: 37260, training_loss: 1.72154
Epoch: 2/20, step: 37280, training_loss: 1.74575
Epoch: 2/20, step: 37300, training_loss: 1.54873
Epoch: 2/20, step: 37320, training_loss: 2.98458
Epoch: 2/20, step: 37340, training_loss: 2.59720
Epoch: 2/20, step: 37360, training_loss: 1.85125
Epoch: 2/20, step: 37380, training_loss: 3.03469
Epoch: 2/20, step: 37400, training_loss: 2.71459
Epoch: 2/20, step: 37420, training_loss: 2.69194
Epoch: 2/20, step: 37440, training_loss: 2.88905
Epoch: 2/20, step: 37460, training_loss: 2.54987
Epoch: 2/20, step: 37480, training_loss: 2.21558
Epoch: 2/20, step: 37500, training_loss: 2.31755
Epoch: 2/20, step: 37520, training_loss: 1.69037
Epoch: 2/20, step: 37540, training_loss: 2.48071
Epoch: 2/20, step: 37560, training_loss: 1.78665
Epoch: 2/20, step: 37580, training_loss: 2.49985
Epoch: 2/20, step: 37600, training_loss: 1.86243
Epoch: 2/20, step: 37620, training_loss: 1.64758
Epoch: 2/20, step: 37640, training_loss: 2.21576
Epoch: 2/20, step: 37660, training_loss: 3.04623
Epoch: 2/20, step: 37680, training_loss: 2.36476
Epoch: 2/20, step: 37700, training_loss: 2.24814
Epoch: 2/20, step: 37720, training_loss: 2.04112
Epoch: 2/20, step: 37740, training_loss: 1.96231
Epoch: 2/20, step: 37760, training_loss: 2.74965
Epoch: 2/20, step: 37780, training_loss: 1.86757
Epoch: 2/20, step: 37800, training_loss: 2.98493
Epoch: 2/20, step: 37820, training_loss: 2.63082
Epoch: 2/20, step: 37840, training_loss: 2.11007
Epoch: 2/20, step: 37860, training_loss: 2.83609
Epoch: 2/20, step: 37880, training_loss: 2.42851
Epoch: 2/20, step: 37900, training_loss: 1.89088
Epoch: 2/20, step: 37920, training_loss: 1.65977
Epoch: 2/20, step: 37940, training_loss: 2.54716
Epoch: 2/20, step: 37960, training_loss: 2.16882
Epoch: 2/20, step: 37980, training_loss: 2.45740
Epoch: 2/20, step: 38000, training_loss: 2.10939
accuracy: 0.34, validation_loss: 2.323709011077881, num_samples: 100
Epoch: 2/20, step: 38020, training_loss: 2.19404
Epoch: 2/20, step: 38040, training_loss: 1.61695
Epoch: 2/20, step: 38060, training_loss: 2.58468
Epoch: 2/20, step: 38080, training_loss: 1.79224
Epoch: 2/20, step: 38100, training_loss: 2.32522
Epoch: 2/20, step: 38120, training_loss: 2.04540
Epoch: 2/20, step: 38140, training_loss: 2.09868
Epoch: 2/20, step: 38160, training_loss: 1.66740
Epoch: 2/20, step: 38180, training_loss: 2.41476
Epoch: 2/20, step: 38200, training_loss: 2.36321
Epoch: 2/20, step: 38220, training_loss: 2.08374
Epoch: 2/20, step: 38240, training_loss: 2.17308
Epoch: 2/20, step: 38260, training_loss: 1.98574
Epoch: 2/20, step: 38280, training_loss: 2.89148
Epoch: 2/20, step: 38300, training_loss: 1.84852
Epoch: 2/20, step: 38320, training_loss: 3.06033
Epoch: 2/20, step: 38340, training_loss: 2.70564
Epoch: 2/20, step: 38360, training_loss: 2.64721
Epoch: 2/20, step: 38380, training_loss: 2.37178
Epoch: 2/20, step: 38400, training_loss: 2.58436
Epoch: 2/20, step: 38420, training_loss: 2.40072
Epoch: 2/20, step: 38440, training_loss: 2.61909
Epoch: 2/20, step: 38460, training_loss: 2.40820
Epoch: 2/20, step: 38480, training_loss: 2.05050
Epoch: 2/20, step: 38500, training_loss: 2.76718
Epoch: 2/20, step: 38520, training_loss: 2.83093
Epoch: 2/20, step: 38540, training_loss: 1.53401
Epoch: 2/20, step: 38560, training_loss: 2.45203
Epoch: 2/20, step: 38580, training_loss: 2.25989
Epoch: 2/20, step: 38600, training_loss: 2.14713
Epoch: 2/20, step: 38620, training_loss: 2.47125
Epoch: 2/20, step: 38640, training_loss: 2.91108
Epoch: 2/20, step: 38660, training_loss: 1.87275
Epoch: 2/20, step: 38680, training_loss: 2.43160
Epoch: 2/20, step: 38700, training_loss: 2.54179
Epoch: 2/20, step: 38720, training_loss: 2.31472
Epoch: 2/20, step: 38740, training_loss: 2.30418
Epoch: 2/20, step: 38760, training_loss: 2.40242
Epoch: 2/20, step: 38780, training_loss: 1.34351
Epoch: 2/20, step: 38800, training_loss: 2.70467
Epoch: 2/20, step: 38820, training_loss: 2.61540
Epoch: 2/20, step: 38840, training_loss: 1.96125
Epoch: 2/20, step: 38860, training_loss: 2.48172
Epoch: 2/20, step: 38880, training_loss: 2.43610
Epoch: 2/20, step: 38900, training_loss: 2.30125
Epoch: 2/20, step: 38920, training_loss: 2.28377
Epoch: 2/20, step: 38940, training_loss: 2.25241
Epoch: 2/20, step: 38960, training_loss: 2.68258
Epoch: 2/20, step: 38980, training_loss: 2.62730
Epoch: 2/20, step: 39000, training_loss: 2.07256
accuracy: 0.44, validation_loss: 2.0761032104492188, num_samples: 100
Epoch: 2/20, step: 39020, training_loss: 2.35500
Epoch: 2/20, step: 39040, training_loss: 2.38924
Epoch: 2/20, step: 39060, training_loss: 2.61986
Epoch: 2/20, step: 39080, training_loss: 2.25901
Epoch: 2/20, step: 39100, training_loss: 1.89114
Epoch: 2/20, step: 39120, training_loss: 2.81837
Epoch: 2/20, step: 39140, training_loss: 1.62919
Epoch: 2/20, step: 39160, training_loss: 1.85829
Epoch: 2/20, step: 39180, training_loss: 2.26340
Epoch: 2/20, step: 39200, training_loss: 1.39718
Epoch: 2/20, step: 39220, training_loss: 2.00143
Epoch: 2/20, step: 39240, training_loss: 2.34174
Epoch: 2/20, step: 39260, training_loss: 2.86044
Epoch: 2/20, step: 39280, training_loss: 1.38589
Epoch: 2/20, step: 39300, training_loss: 2.30308
Epoch: 2/20, step: 39320, training_loss: 2.14557
Epoch: 2/20, step: 39340, training_loss: 1.94339
Epoch: 2/20, step: 39360, training_loss: 2.92684
Epoch: 2/20, step: 39380, training_loss: 1.80020
Epoch: 2/20, step: 39400, training_loss: 2.62258
Epoch: 2/20, step: 39420, training_loss: 1.57521
Epoch: 2/20, step: 39440, training_loss: 2.81098
Epoch: 2/20, step: 39460, training_loss: 2.30143
Epoch: 2/20, step: 39480, training_loss: 2.45626
Epoch: 2/20, step: 39500, training_loss: 2.51095
Epoch: 2/20, step: 39520, training_loss: 2.20260
Epoch: 2/20, step: 39540, training_loss: 2.58449
Epoch: 2/20, step: 39560, training_loss: 2.03171
Epoch: 2/20, step: 39580, training_loss: 1.59487
Epoch: 2/20, step: 39600, training_loss: 2.13993
Epoch: 2/20, step: 39620, training_loss: 2.21079
Epoch: 2/20, step: 39640, training_loss: 2.09306
Epoch: 2/20, step: 39660, training_loss: 2.12916
Epoch: 2/20, step: 39680, training_loss: 2.49154
Epoch: 2/20, step: 39700, training_loss: 3.02071
Epoch: 2/20, step: 39720, training_loss: 2.08043
Epoch: 2/20, step: 39740, training_loss: 2.17216
Epoch: 2/20, step: 39760, training_loss: 2.09087
Epoch: 2/20, step: 39780, training_loss: 2.59177
Epoch: 2/20, step: 39800, training_loss: 1.96322
Epoch: 2/20, step: 39820, training_loss: 2.35715
Epoch: 2/20, step: 39840, training_loss: 2.77212
Epoch: 2/20, step: 39860, training_loss: 2.02614
Epoch: 2/20, step: 39880, training_loss: 1.76051
Epoch: 2/20, step: 39900, training_loss: 2.03443
Epoch: 2/20, step: 39920, training_loss: 3.32249
Epoch: 2/20, step: 39940, training_loss: 1.96645
Epoch: 2/20, step: 39960, training_loss: 2.44960
Epoch: 2/20, step: 39980, training_loss: 1.54540
Epoch: 2/20, step: 40000, training_loss: 1.36889
accuracy: 0.42, validation_loss: 2.017103910446167, num_samples: 100
Epoch: 2/20, step: 40020, training_loss: 2.27079
Epoch: 2/20, step: 40040, training_loss: 2.30321
Epoch: 2/20, step: 40060, training_loss: 2.23023
Epoch: 2/20, step: 40080, training_loss: 2.64718
Epoch: 2/20, step: 40100, training_loss: 1.94347
Epoch: 2/20, step: 40120, training_loss: 2.54886
Epoch: 2/20, step: 40140, training_loss: 2.74848
Epoch: 2/20, step: 40160, training_loss: 2.39442
Epoch: 2/20, step: 40180, training_loss: 2.92375
Epoch: 2/20, step: 40200, training_loss: 2.43800
Epoch: 2/20, step: 40220, training_loss: 2.13897
Epoch: 2/20, step: 40240, training_loss: 1.95217
Epoch: 2/20, step: 40260, training_loss: 2.28170
Epoch: 2/20, step: 40280, training_loss: 2.14587
Epoch: 2/20, step: 40300, training_loss: 2.12409
Epoch: 2/20, step: 40320, training_loss: 2.67547
Epoch: 2/20, step: 40340, training_loss: 1.00165
Epoch: 2/20, step: 40360, training_loss: 2.07895
Epoch: 2/20, step: 40380, training_loss: 2.77142
Epoch: 2/20, step: 40400, training_loss: 3.16508
Epoch: 2/20, step: 40420, training_loss: 2.75880
Epoch: 2/20, step: 40440, training_loss: 2.75501
Epoch: 2/20, step: 40460, training_loss: 1.87366
Epoch: 2/20, step: 40480, training_loss: 1.28524
Epoch: 2/20, step: 40500, training_loss: 1.98763
Epoch: 2/20, step: 40520, training_loss: 2.12762
Epoch: 2/20, step: 40540, training_loss: 1.44906
Epoch: 2/20, step: 40560, training_loss: 3.21185
Epoch: 2/20, step: 40580, training_loss: 2.49845
Epoch: 2/20, step: 40600, training_loss: 2.05182
Epoch: 2/20, step: 40620, training_loss: 2.40628
Epoch: 2/20, step: 40640, training_loss: 1.93311
Epoch: 2/20, step: 40660, training_loss: 2.25505
Epoch: 2/20, step: 40680, training_loss: 1.91995
Epoch: 2/20, step: 40700, training_loss: 2.15351
Epoch: 2/20, step: 40720, training_loss: 2.46574
Epoch: 2/20, step: 40740, training_loss: 2.64530
Epoch: 2/20, step: 40760, training_loss: 2.23764
Epoch: 2/20, step: 40780, training_loss: 2.70977
Epoch: 2/20, step: 40800, training_loss: 1.63329
Epoch: 2/20, step: 40820, training_loss: 2.81077
Epoch: 2/20, step: 40840, training_loss: 2.11347
Epoch: 2/20, step: 40860, training_loss: 1.90478
Epoch: 2/20, step: 40880, training_loss: 2.61459
Epoch: 2/20, step: 40900, training_loss: 2.08811
Epoch: 2/20, step: 40920, training_loss: 1.71555
Epoch: 2/20, step: 40940, training_loss: 2.58262
Epoch: 2/20, step: 40960, training_loss: 2.35483
Epoch: 2/20, step: 40980, training_loss: 1.81228
Epoch: 2/20, step: 41000, training_loss: 2.46598
accuracy: 0.43, validation_loss: 2.1966559886932373, num_samples: 100
Epoch: 2/20, step: 41020, training_loss: 2.07309
Epoch: 2/20, step: 41040, training_loss: 2.53349
Epoch: 2/20, step: 41060, training_loss: 2.51977
Epoch: 2/20, step: 41080, training_loss: 2.15934
Epoch: 2/20, step: 41100, training_loss: 1.64125
Epoch: 2/20, step: 41120, training_loss: 1.95259
Epoch: 2/20, step: 41140, training_loss: 1.91214
Epoch: 2/20, step: 41160, training_loss: 1.62112
Epoch: 2/20, step: 41180, training_loss: 2.51110
Epoch: 2/20, step: 41200, training_loss: 2.82940
Epoch: 2/20, step: 41220, training_loss: 2.53047
Epoch: 2/20, step: 41240, training_loss: 2.06385
Epoch: 2/20, step: 41260, training_loss: 1.92004
Epoch: 2/20, step: 41280, training_loss: 2.17873
Epoch: 2/20, step: 41300, training_loss: 2.33025
Epoch: 2/20, step: 41320, training_loss: 1.52965
Epoch: 2/20, step: 41340, training_loss: 2.46391
Epoch: 2/20, step: 41360, training_loss: 2.01277
Epoch: 2/20, step: 41380, training_loss: 1.86150
Epoch: 2/20, step: 41400, training_loss: 2.46921
Epoch: 2/20, step: 41420, training_loss: 2.09567
Epoch: 2/20, step: 41440, training_loss: 2.03611
Epoch: 2/20, step: 41460, training_loss: 1.52673
Epoch: 2/20, step: 41480, training_loss: 2.35296
Epoch: 2/20, step: 41500, training_loss: 2.42499
Epoch: 2/20, step: 41520, training_loss: 2.13274
Epoch: 2/20, step: 41540, training_loss: 2.97117
Epoch: 2/20, step: 41560, training_loss: 1.60425
Epoch: 2/20, step: 41580, training_loss: 1.39994
Epoch: 2/20, step: 41600, training_loss: 1.74095
Epoch: 2/20, step: 41620, training_loss: 2.14550
Epoch: 2/20, step: 41640, training_loss: 2.78892
Epoch: 2/20, step: 41660, training_loss: 1.83789
Epoch: 2/20, step: 41680, training_loss: 1.97758
Epoch: 2/20, step: 41700, training_loss: 1.70662
Epoch: 2/20, step: 41720, training_loss: 1.80316
Epoch: 2/20, step: 41740, training_loss: 2.04278
Epoch: 2/20, step: 41760, training_loss: 2.07401
Epoch: 2/20, step: 41780, training_loss: 2.03527
Epoch: 2/20, step: 41800, training_loss: 2.69800
Epoch: 2/20, step: 41820, training_loss: 2.94874
Epoch: 2/20, step: 41840, training_loss: 2.67759
Epoch: 2/20, step: 41860, training_loss: 2.77350
Epoch: 2/20, step: 41880, training_loss: 1.55756
Epoch: 2/20, step: 41900, training_loss: 1.73018
Epoch: 2/20, step: 41920, training_loss: 2.49270
Epoch: 2/20, step: 41940, training_loss: 2.46075
Epoch: 2/20, step: 41960, training_loss: 1.52906
Epoch: 2/20, step: 41980, training_loss: 1.64601
Epoch: 2/20, step: 42000, training_loss: 2.88801
accuracy: 0.46, validation_loss: 2.009369134902954, num_samples: 100
Epoch: 2/20, step: 42020, training_loss: 2.82029
Epoch: 2/20, step: 42040, training_loss: 2.11057
Epoch: 2/20, step: 42060, training_loss: 2.43447
Epoch: 2/20, step: 42080, training_loss: 2.36585
Epoch: 2/20, step: 42100, training_loss: 2.59400
Epoch: 2/20, step: 42120, training_loss: 1.58697
Epoch: 2/20, step: 42140, training_loss: 2.06147
Epoch: 2/20, step: 42160, training_loss: 1.91161
Epoch: 2/20, step: 42180, training_loss: 1.39980
Epoch: 2/20, step: 42200, training_loss: 2.33718
Epoch: 2/20, step: 42220, training_loss: 2.47191
Epoch: 2/20, step: 42240, training_loss: 2.38940
Epoch: 2/20, step: 42260, training_loss: 2.86305
Epoch: 2/20, step: 42280, training_loss: 2.06323
Epoch: 2/20, step: 42300, training_loss: 2.11189
Epoch: 2/20, step: 42320, training_loss: 2.00882
Epoch: 2/20, step: 42340, training_loss: 2.29561
Epoch: 2/20, step: 42360, training_loss: 1.04472
Epoch: 2/20, step: 42380, training_loss: 1.92755
Epoch: 2/20, step: 42400, training_loss: 2.79375
Epoch: 2/20, step: 42420, training_loss: 1.41499
Epoch: 2/20, step: 42440, training_loss: 1.74177
Epoch: 2/20, step: 42460, training_loss: 2.24752
Epoch: 2/20, step: 42480, training_loss: 1.66577
Epoch: 2/20, step: 42500, training_loss: 1.78085
Epoch: 2/20, step: 42520, training_loss: 2.35297
Epoch: 2/20, step: 42540, training_loss: 2.32810
Epoch: 2/20, step: 42560, training_loss: 2.13679
Epoch: 2/20, step: 42580, training_loss: 2.89178
Epoch: 2/20, step: 42600, training_loss: 3.04013
Epoch: 2/20, step: 42620, training_loss: 2.07281
Epoch: 2/20, step: 42640, training_loss: 1.46663
Epoch: 2/20, step: 42660, training_loss: 2.07995
Epoch: 2/20, step: 42680, training_loss: 2.59499
Epoch: 2/20, step: 42700, training_loss: 2.51970
Epoch: 2/20, step: 42720, training_loss: 2.08000
Epoch: 2/20, step: 42740, training_loss: 2.94052
Epoch: 2/20, step: 42760, training_loss: 3.10208
Epoch: 2/20, step: 42780, training_loss: 1.34814
Epoch: 2/20, step: 42800, training_loss: 2.45248
Epoch: 2/20, step: 42820, training_loss: 2.46911
Epoch: 2/20, step: 42840, training_loss: 1.95015
Epoch: 2/20, step: 42860, training_loss: 2.36283
Epoch: 2/20, step: 42880, training_loss: 2.74854
Epoch: 2/20, step: 42900, training_loss: 1.76138
Epoch: 2/20, step: 42920, training_loss: 2.00068
Epoch: 2/20, step: 42940, training_loss: 1.78749
Epoch: 2/20, step: 42960, training_loss: 1.95984
Epoch: 2/20, step: 42980, training_loss: 3.23672
Epoch: 2/20, step: 43000, training_loss: 2.23285
accuracy: 0.39, validation_loss: 2.2934112548828125, num_samples: 100
Epoch: 2/20, step: 43020, training_loss: 2.15976
Epoch: 2/20, step: 43040, training_loss: 2.38532
Epoch: 2/20, step: 43060, training_loss: 1.94848
Epoch: 2/20, step: 43080, training_loss: 1.69731
Epoch: 2/20, step: 43100, training_loss: 2.38613
Epoch: 2/20, step: 43120, training_loss: 2.79733
Epoch: 2/20, step: 43140, training_loss: 2.17503
Epoch: 2/20, step: 43160, training_loss: 1.91639
Epoch: 2/20, step: 43180, training_loss: 2.71418
Epoch: 2/20, step: 43200, training_loss: 1.84294
Epoch: 2/20, step: 43220, training_loss: 1.86252
Epoch: 2/20, step: 43240, training_loss: 2.03705
Epoch: 2/20, step: 43260, training_loss: 2.09958
Epoch: 2/20, step: 43280, training_loss: 2.72095
Epoch: 2/20, step: 43300, training_loss: 2.27817
Epoch: 2/20, step: 43320, training_loss: 1.65505
Epoch: 2/20, step: 43340, training_loss: 2.32923
Epoch: 2/20, step: 43360, training_loss: 2.69686
Epoch: 2/20, step: 43380, training_loss: 2.09413
Epoch: 2/20, step: 43400, training_loss: 1.99228
Epoch: 2/20, step: 43420, training_loss: 1.84891
Epoch: 2/20, step: 43440, training_loss: 3.00881
Epoch: 2/20, step: 43460, training_loss: 2.35528
Epoch: 2/20, step: 43480, training_loss: 2.48633
Epoch: 2/20, step: 43500, training_loss: 2.24903
Epoch: 2/20, step: 43520, training_loss: 2.46043
Epoch: 2/20, step: 43540, training_loss: 1.99396
Epoch: 2/20, step: 43560, training_loss: 2.25526
Epoch: 2/20, step: 43580, training_loss: 1.99932
Epoch: 2/20, step: 43600, training_loss: 1.92772
Epoch: 2/20, step: 43620, training_loss: 2.28015
Epoch: 2/20, step: 43640, training_loss: 3.04249
Epoch: 2/20, step: 43660, training_loss: 2.00296
Epoch: 2/20, step: 43680, training_loss: 2.80593
Epoch: 2/20, step: 43700, training_loss: 1.89740
Epoch: 2/20, step: 43720, training_loss: 1.29482
Epoch: 2/20, step: 43740, training_loss: 2.05039
Epoch: 2/20, step: 43760, training_loss: 1.70598
Epoch: 2/20, step: 43780, training_loss: 2.01146
Epoch: 2/20, step: 43800, training_loss: 0.98417
Epoch: 2/20, step: 43820, training_loss: 1.88424
Epoch: 2/20, step: 43840, training_loss: 1.79053
Epoch: 2/20, step: 43860, training_loss: 2.15693
Epoch: 2/20, step: 43880, training_loss: 2.83089
Epoch: 2/20, step: 43900, training_loss: 2.07858
Epoch: 2/20, step: 43920, training_loss: 2.22612
Epoch: 2/20, step: 43940, training_loss: 2.63054
Epoch: 2/20, step: 43960, training_loss: 2.86102
Epoch: 2/20, step: 43980, training_loss: 2.63800
Epoch: 2/20, step: 44000, training_loss: 2.32468
accuracy: 0.32, validation_loss: 2.580944299697876, num_samples: 100
Epoch: 2/20, step: 44020, training_loss: 1.76381
Epoch: 2/20, step: 44040, training_loss: 2.42801
Epoch: 2/20, step: 44060, training_loss: 2.27083
Epoch: 2/20, step: 44080, training_loss: 2.08856
Epoch: 2/20, step: 44100, training_loss: 2.57706
Epoch: 2/20, step: 44120, training_loss: 1.56367
Epoch: 2/20, step: 44140, training_loss: 2.30956
Epoch: 2/20, step: 44160, training_loss: 2.02821
Epoch: 2/20, step: 44180, training_loss: 2.61510
Epoch: 2/20, step: 44200, training_loss: 1.05856
Epoch: 2/20, step: 44220, training_loss: 2.65344
Epoch: 2/20, step: 44240, training_loss: 1.35691
Epoch: 2/20, step: 44260, training_loss: 2.07584
Epoch: 2/20, step: 44280, training_loss: 2.37419
Epoch: 2/20, step: 44300, training_loss: 2.65138
Epoch: 2/20, step: 44320, training_loss: 1.95564
Epoch: 2/20, step: 44340, training_loss: 2.44670
Epoch: 2/20, step: 44360, training_loss: 2.22759
Epoch: 2/20, step: 44380, training_loss: 2.54645
Epoch: 2/20, step: 44400, training_loss: 1.29220
Epoch: 2/20, step: 44420, training_loss: 2.02683
Epoch: 2/20, step: 44440, training_loss: 2.52507
Epoch: 2/20, step: 44460, training_loss: 2.36435
Epoch: 2/20, step: 44480, training_loss: 2.25189
Epoch: 2/20, step: 44500, training_loss: 1.58890
Epoch: 2/20, step: 44520, training_loss: 2.04303
Epoch: 2/20, step: 44540, training_loss: 2.26582
Epoch: 2/20, step: 44560, training_loss: 2.32119
Epoch: 2/20, step: 44580, training_loss: 2.40281
Epoch: 2/20, step: 44600, training_loss: 1.85014
Epoch: 2/20, step: 44620, training_loss: 2.06364
Epoch: 2/20, step: 44640, training_loss: 2.17840
Epoch: 2/20, step: 44660, training_loss: 2.84001
Epoch: 2/20, step: 44680, training_loss: 1.71042
Epoch: 2/20, step: 44700, training_loss: 2.74938
Epoch: 2/20, step: 44720, training_loss: 2.67621
Epoch: 2/20, step: 44740, training_loss: 1.19566
Epoch: 2/20, step: 44760, training_loss: 2.53072
Epoch: 2/20, step: 44780, training_loss: 1.82895
Epoch: 2/20, step: 44800, training_loss: 2.43196
Epoch: 2/20, step: 44820, training_loss: 2.97572
Epoch: 2/20, step: 44840, training_loss: 2.08857
Epoch: 2/20, step: 44860, training_loss: 2.54876
Epoch: 2/20, step: 44880, training_loss: 1.85402
Epoch: 2/20, step: 44900, training_loss: 2.21975
Epoch: 2/20, step: 44920, training_loss: 1.95990
Epoch: 2/20, step: 44940, training_loss: 2.38681
Epoch: 2/20, step: 44960, training_loss: 3.28150
Epoch: 2/20, step: 44980, training_loss: 2.54911
Epoch: 2/20, step: 45000, training_loss: 2.33937
accuracy: 0.41, validation_loss: 2.1610701084136963, num_samples: 100
Epoch: 2/20, step: 45020, training_loss: 1.63970
Epoch: 2/20, step: 45040, training_loss: 2.23502
Epoch: 2/20, step: 45060, training_loss: 2.51651
Epoch: 2/20, step: 45080, training_loss: 2.61866
Epoch: 2/20, step: 45100, training_loss: 2.55622
Epoch: 2/20, step: 45120, training_loss: 1.83511
Epoch: 2/20, step: 45140, training_loss: 2.13476
Epoch: 2/20, step: 45160, training_loss: 2.35448
Epoch: 2/20, step: 45180, training_loss: 1.26180
Epoch: 2/20, step: 45200, training_loss: 2.02430
Epoch: 2/20, step: 45220, training_loss: 2.39967
Epoch: 2/20, step: 45240, training_loss: 2.56765
Epoch: 2/20, step: 45260, training_loss: 2.40326
Epoch: 2/20, step: 45280, training_loss: 2.79105
Epoch: 2/20, step: 45300, training_loss: 2.04589
Epoch: 2/20, step: 45320, training_loss: 1.93883
Epoch: 2/20, step: 45340, training_loss: 2.14811
Epoch: 2/20, step: 45360, training_loss: 1.62036
Epoch: 2/20, step: 45380, training_loss: 2.14746
Epoch: 2/20, step: 45400, training_loss: 2.91601
Epoch: 2/20, step: 45420, training_loss: 1.99363
Epoch: 2/20, step: 45440, training_loss: 2.26583
Epoch: 2/20, step: 45460, training_loss: 2.55813
Epoch: 2/20, step: 45480, training_loss: 2.53852
Epoch: 2/20, step: 45500, training_loss: 2.84593
Epoch: 2/20, step: 45520, training_loss: 2.70608
Epoch: 2/20, step: 45540, training_loss: 1.22829
Epoch: 2/20, step: 45560, training_loss: 2.30094
Epoch: 2/20, step: 45580, training_loss: 2.53724
Epoch: 2/20, step: 45600, training_loss: 3.02324
Epoch: 2/20, step: 45620, training_loss: 2.45036
Epoch: 2/20, step: 45640, training_loss: 2.44936
Epoch: 2/20, step: 45660, training_loss: 1.67399
Epoch: 2/20, step: 45680, training_loss: 2.12561
Epoch: 2/20, step: 45700, training_loss: 2.31694
Epoch: 2/20, step: 45720, training_loss: 1.98767
Epoch: 2/20, step: 45740, training_loss: 1.91627
Epoch: 2/20, step: 45760, training_loss: 2.81904
Epoch: 2/20, step: 45780, training_loss: 1.70078
Epoch: 2/20, step: 45800, training_loss: 2.32296
Epoch: 2/20, step: 45820, training_loss: 2.04304
Epoch: 2/20, step: 45840, training_loss: 1.37652
Epoch: 2/20, step: 45860, training_loss: 2.19475
Epoch: 2/20, step: 45880, training_loss: 1.54836
Epoch: 2/20, step: 45900, training_loss: 1.97393
Epoch: 2/20, step: 45920, training_loss: 2.15465
Epoch: 2/20, step: 45940, training_loss: 2.48721
Epoch: 2/20, step: 45960, training_loss: 2.47292
Epoch: 2/20, step: 45980, training_loss: 2.11090
Epoch: 2/20, step: 46000, training_loss: 1.73239
accuracy: 0.38, validation_loss: 2.2465903759002686, num_samples: 100
Epoch: 2/20, step: 46020, training_loss: 2.35575
Epoch: 2/20, step: 46040, training_loss: 2.40619
Epoch: 2/20, step: 46060, training_loss: 2.19117
Epoch: 2/20, step: 46080, training_loss: 2.59094
Epoch: 2/20, step: 46100, training_loss: 2.43529
Epoch: 2/20, step: 46120, training_loss: 2.33280
Epoch: 2/20, step: 46140, training_loss: 1.54428
Epoch: 2/20, step: 46160, training_loss: 1.56467
Epoch: 2/20, step: 46180, training_loss: 2.19975
Epoch: 2/20, step: 46200, training_loss: 3.09875
Epoch: 2/20, step: 46220, training_loss: 1.85217
Epoch: 2/20, step: 46240, training_loss: 3.43986
Epoch: 2/20, step: 46260, training_loss: 1.68065
Epoch: 2/20, step: 46280, training_loss: 1.86667
Epoch: 2/20, step: 46300, training_loss: 2.95434
Epoch: 2/20, step: 46320, training_loss: 1.73680
Epoch: 2/20, step: 46340, training_loss: 2.07773
Epoch: 2/20, step: 46360, training_loss: 1.65255
Epoch: 2/20, step: 46380, training_loss: 1.98070
Epoch: 2/20, step: 46400, training_loss: 2.24335
Epoch: 2/20, step: 46420, training_loss: 2.04656
Epoch: 2/20, step: 46440, training_loss: 2.16232
Epoch: 2/20, step: 46460, training_loss: 1.12852
Epoch: 2/20, step: 46480, training_loss: 1.64608
Epoch: 2/20, step: 46500, training_loss: 2.99913
Epoch: 2/20, step: 46520, training_loss: 2.68536
Epoch: 2/20, step: 46540, training_loss: 2.96697
Epoch: 2/20, step: 46560, training_loss: 2.18409
Epoch: 2/20, step: 46580, training_loss: 1.86926
Epoch: 2/20, step: 46600, training_loss: 2.27910
Epoch: 2/20, step: 46620, training_loss: 1.94785
Epoch: 2/20, step: 46640, training_loss: 2.13227
Epoch: 2/20, step: 46660, training_loss: 1.96995
Epoch: 2/20, step: 46680, training_loss: 1.71388
Epoch: 2/20, step: 46700, training_loss: 2.82909
Epoch: 2/20, step: 46720, training_loss: 1.83217
Epoch: 2/20, step: 46740, training_loss: 2.18957
Epoch: 2/20, step: 46760, training_loss: 1.80064
Epoch: 2/20, step: 46780, training_loss: 1.94948
Epoch: 2/20, step: 46800, training_loss: 2.43257
Epoch: 2/20, step: 46820, training_loss: 2.19617
Epoch: 2/20, step: 46840, training_loss: 2.46065
Epoch: 2/20, step: 46860, training_loss: 2.23846
Epoch: 2/20, step: 46880, training_loss: 1.52045
Epoch: 2/20, step: 46900, training_loss: 1.92971
Epoch: 2/20, step: 46920, training_loss: 1.22968
Epoch: 2/20, step: 46940, training_loss: 2.27526
Epoch: 2/20, step: 46960, training_loss: 2.28055
Epoch: 2/20, step: 46980, training_loss: 1.88472
Epoch: 2/20, step: 47000, training_loss: 2.83835
accuracy: 0.47, validation_loss: 2.0103113651275635, num_samples: 100
Epoch: 2/20, step: 47020, training_loss: 2.90363
Epoch: 2/20, step: 47040, training_loss: 2.46052
Epoch: 2/20, step: 47060, training_loss: 1.67024
Epoch: 2/20, step: 47080, training_loss: 2.84199
Epoch: 2/20, step: 47100, training_loss: 1.90101
Epoch: 2/20, step: 47120, training_loss: 1.88818
Epoch: 2/20, step: 47140, training_loss: 1.75609
Epoch: 2/20, step: 47160, training_loss: 2.23307
Epoch: 2/20, step: 47180, training_loss: 2.04346
Epoch: 2/20, step: 47200, training_loss: 1.77760
Epoch: 2/20, step: 47220, training_loss: 2.86652
Epoch: 2/20, step: 47240, training_loss: 2.60351
Epoch: 2/20, step: 47260, training_loss: 3.26550
Epoch: 2/20, step: 47280, training_loss: 2.56552
Epoch: 2/20, step: 47300, training_loss: 3.16863
Epoch: 2/20, step: 47320, training_loss: 1.74161
Epoch: 2/20, step: 47340, training_loss: 2.11595
Epoch: 2/20, step: 47360, training_loss: 2.42000
Epoch: 2/20, step: 47380, training_loss: 3.01656
Epoch: 2/20, step: 47400, training_loss: 2.03651
Epoch: 2/20, step: 47420, training_loss: 1.98386
Epoch: 2/20, step: 47440, training_loss: 2.29124
Epoch: 2/20, step: 47460, training_loss: 1.82635
Epoch: 2/20, step: 47480, training_loss: 2.61177
Epoch: 2/20, step: 47500, training_loss: 2.17720
Epoch: 2/20, step: 47520, training_loss: 2.44499
Epoch: 2/20, step: 47540, training_loss: 1.70639
Epoch: 2/20, step: 47560, training_loss: 1.43299
Epoch: 2/20, step: 47580, training_loss: 2.32706
Epoch: 2/20, step: 47600, training_loss: 1.46146
Epoch: 2/20, step: 47620, training_loss: 2.45438
Epoch: 2/20, step: 47640, training_loss: 1.77124
Epoch: 2/20, step: 47660, training_loss: 2.12081
Epoch: 2/20, step: 47680, training_loss: 1.39442
Epoch: 2/20, step: 47700, training_loss: 3.31202
Epoch: 2/20, step: 47720, training_loss: 1.38306
Epoch: 2/20, step: 47740, training_loss: 2.36894
Epoch: 2/20, step: 47760, training_loss: 2.87410
Epoch: 2/20, step: 47780, training_loss: 2.17253
Epoch: 2/20, step: 47800, training_loss: 1.56598
Epoch: 2/20, step: 47820, training_loss: 1.78450
Epoch: 2/20, step: 47840, training_loss: 1.66382
Epoch: 2/20, step: 47860, training_loss: 1.71621
Epoch: 2/20, step: 47880, training_loss: 1.81016
Epoch: 2/20, step: 47900, training_loss: 2.42604
Epoch: 2/20, step: 47920, training_loss: 1.88622
Epoch: 2/20, step: 47940, training_loss: 2.33270
Epoch: 2/20, step: 47960, training_loss: 2.20977
Epoch: 2/20, step: 47980, training_loss: 2.97100
Epoch: 2/20, step: 48000, training_loss: 2.45669
accuracy: 0.36, validation_loss: 2.3396944999694824, num_samples: 100
Epoch: 2/20, step: 48020, training_loss: 1.32939
Epoch: 2/20, step: 48040, training_loss: 2.37954
Epoch: 2/20, step: 48060, training_loss: 2.26261
Epoch: 2/20, step: 48080, training_loss: 2.05647
Epoch: 2/20, step: 48100, training_loss: 2.53927
Epoch: 2/20, step: 48120, training_loss: 1.93005
Epoch: 2/20, step: 48140, training_loss: 1.72028
Epoch: 2/20, step: 48160, training_loss: 2.20854
Epoch: 2/20, step: 48180, training_loss: 2.69368
Epoch: 2/20, step: 48200, training_loss: 2.46834
Epoch: 2/20, step: 48220, training_loss: 2.05631
Epoch: 2/20, step: 48240, training_loss: 2.24998
Epoch: 2/20, step: 48260, training_loss: 2.07817
Epoch: 2/20, step: 48280, training_loss: 1.45072
Epoch: 2/20, step: 48300, training_loss: 2.88854
Epoch: 2/20, step: 48320, training_loss: 2.11239
Epoch: 2/20, step: 48340, training_loss: 2.59096
Epoch: 2/20, step: 48360, training_loss: 2.24289
Epoch: 2/20, step: 48380, training_loss: 2.34917
Epoch: 2/20, step: 48400, training_loss: 2.01240
Epoch: 2/20, step: 48420, training_loss: 1.22079
Epoch: 2/20, step: 48440, training_loss: 2.51820
Epoch: 2/20, step: 48460, training_loss: 1.59008
Epoch: 2/20, step: 48480, training_loss: 2.87618
Epoch: 2/20, step: 48500, training_loss: 2.15815
Epoch: 2/20, step: 48520, training_loss: 1.53147
Epoch: 2/20, step: 48540, training_loss: 2.06221
Epoch: 2/20, step: 48560, training_loss: 2.97370
Epoch: 2/20, step: 48580, training_loss: 1.62281
Epoch: 2/20, step: 48600, training_loss: 2.20243
Epoch: 2/20, step: 48620, training_loss: 2.17985
Epoch: 2/20, step: 48640, training_loss: 2.14752
Epoch: 2/20, step: 48660, training_loss: 2.42970
Epoch: 2/20, step: 48680, training_loss: 2.65961
Epoch: 2/20, step: 48700, training_loss: 1.79246
Epoch: 2/20, step: 48720, training_loss: 2.30836
Epoch: 2/20, step: 48740, training_loss: 2.51389
Epoch: 2/20, step: 48760, training_loss: 1.88687
Epoch: 2/20, step: 48780, training_loss: 2.36138
Epoch: 2/20, step: 48800, training_loss: 3.36363
Epoch: 2/20, step: 48820, training_loss: 2.89332
Epoch: 2/20, step: 48840, training_loss: 1.76470
Epoch: 2/20, step: 48860, training_loss: 2.77755
Epoch: 2/20, step: 48880, training_loss: 2.25741
Epoch: 2/20, step: 48900, training_loss: 2.10944
Epoch: 2/20, step: 48920, training_loss: 2.45695
Epoch: 2/20, step: 48940, training_loss: 2.28338
Epoch: 2/20, step: 48960, training_loss: 3.09026
Epoch: 2/20, step: 48980, training_loss: 2.63212
Epoch: 2/20, step: 49000, training_loss: 2.21744
accuracy: 0.38, validation_loss: 2.3335840702056885, num_samples: 100
Epoch: 2/20, step: 49020, training_loss: 2.57920
Epoch: 2/20, step: 49040, training_loss: 1.13359
Epoch: 2/20, step: 49060, training_loss: 1.75193
Epoch: 2/20, step: 49080, training_loss: 2.56157
Epoch: 2/20, step: 49100, training_loss: 2.06375
Epoch: 2/20, step: 49120, training_loss: 2.40354
Epoch: 2/20, step: 49140, training_loss: 1.60411
Epoch: 2/20, step: 49160, training_loss: 2.72198
Epoch: 2/20, step: 49180, training_loss: 2.01669
Epoch: 2/20, step: 49200, training_loss: 2.18198
Epoch: 2/20, step: 49220, training_loss: 1.66133
Epoch: 2/20, step: 49240, training_loss: 1.74617
Epoch: 2/20, step: 49260, training_loss: 2.07395
Epoch: 2/20, step: 49280, training_loss: 1.67044
Epoch: 2/20, step: 49300, training_loss: 1.80085
Epoch: 2/20, step: 49320, training_loss: 2.31912
Epoch: 2/20, step: 49340, training_loss: 1.67082
Epoch: 2/20, step: 49360, training_loss: 2.72900
Epoch: 2/20, step: 49380, training_loss: 2.23408
Epoch: 2/20, step: 49400, training_loss: 2.39363
Epoch: 2/20, step: 49420, training_loss: 1.97567
Epoch: 2/20, step: 49440, training_loss: 2.06901
Epoch: 2/20, step: 49460, training_loss: 1.41296
Epoch: 2/20, step: 49480, training_loss: 1.70518
Epoch: 2/20, step: 49500, training_loss: 1.99013
Epoch: 2/20, step: 49520, training_loss: 2.05537
Epoch: 2/20, step: 49540, training_loss: 2.83549
Epoch: 2/20, step: 49560, training_loss: 2.13410
Epoch: 2/20, step: 49580, training_loss: 2.83168
Epoch: 2/20, step: 49600, training_loss: 2.65970
Epoch: 2/20, step: 49620, training_loss: 1.77666
Epoch: 2/20, step: 49640, training_loss: 1.85193
Epoch: 2/20, step: 49660, training_loss: 2.36231
Epoch: 2/20, step: 49680, training_loss: 2.30184
Epoch: 2/20, step: 49700, training_loss: 1.79657
Epoch: 2/20, step: 49720, training_loss: 2.51277
Epoch: 2/20, step: 49740, training_loss: 2.75363
Epoch: 2/20, step: 49760, training_loss: 2.50162
Epoch: 2/20, step: 49780, training_loss: 2.00342
Epoch: 2/20, step: 49800, training_loss: 1.87512
Epoch: 2/20, step: 49820, training_loss: 2.22536
Epoch: 2/20, step: 49840, training_loss: 1.74151
Epoch: 2/20, step: 49860, training_loss: 1.75152
Epoch: 2/20, step: 49880, training_loss: 2.36048
Epoch: 2/20, step: 49900, training_loss: 2.80181
Epoch: 2/20, step: 49920, training_loss: 1.97173
Epoch: 2/20, step: 49940, training_loss: 1.87882
Epoch: 2/20, step: 49960, training_loss: 1.88659
Epoch: 2/20, step: 49980, training_loss: 2.50734
Epoch: 2/20, step: 50000, training_loss: 1.80190
accuracy: 0.52, validation_loss: 1.8006410598754883, num_samples: 100
Epoch: 2/20, step: 50020, training_loss: 1.48989
Epoch: 2/20, step: 50040, training_loss: 1.63384
Epoch: 2/20, step: 50060, training_loss: 2.35164
Epoch: 2/20, step: 50080, training_loss: 2.06695
Epoch: 2/20, step: 50100, training_loss: 2.27739
Epoch: 2/20, step: 50120, training_loss: 2.05860
Epoch: 2/20, step: 50140, training_loss: 2.66143
Epoch: 2/20, step: 50160, training_loss: 2.69051
Epoch: 2/20, step: 50180, training_loss: 2.19479
Epoch: 2/20, step: 50200, training_loss: 1.83122
Epoch: 2/20, step: 50220, training_loss: 1.95259
Epoch: 2/20, step: 50240, training_loss: 2.01194
Epoch: 2/20, step: 50260, training_loss: 1.52846
Epoch: 2/20, step: 50280, training_loss: 3.16194
Epoch: 2/20, step: 50300, training_loss: 2.42688
Epoch: 2/20, step: 50320, training_loss: 2.44980
Epoch: 2/20, step: 50340, training_loss: 2.15105
Epoch: 2/20, step: 50360, training_loss: 2.41067
Epoch: 2/20, step: 50380, training_loss: 1.80176
Epoch: 2/20, step: 50400, training_loss: 2.45706
Epoch: 2/20, step: 50420, training_loss: 1.41063
Epoch: 2/20, step: 50440, training_loss: 2.13209
Epoch: 2/20, step: 50460, training_loss: 2.66323
Epoch: 2/20, step: 50480, training_loss: 1.94895
Epoch: 2/20, step: 50500, training_loss: 2.06140
Epoch: 2/20, step: 50520, training_loss: 2.49156
Epoch: 2/20, step: 50540, training_loss: 2.03329
Epoch: 2/20, step: 50560, training_loss: 2.59058
Epoch: 2/20, step: 50580, training_loss: 2.62776
Epoch: 2/20, step: 50600, training_loss: 1.87495
Epoch: 2/20, step: 50620, training_loss: 2.21625
Epoch: 2/20, step: 50640, training_loss: 2.47298
Epoch: 2/20, step: 50660, training_loss: 2.11384
Epoch: 2/20, step: 50680, training_loss: 2.37309
Epoch: 2/20, step: 50700, training_loss: 2.09323
Epoch: 2/20, step: 50720, training_loss: 2.43403
Epoch: 2/20, step: 50740, training_loss: 2.85758
Epoch: 2/20, step: 50760, training_loss: 2.20428
Epoch: 2/20, step: 50780, training_loss: 2.28882
Epoch: 2/20, step: 50800, training_loss: 2.96548
Epoch: 2/20, step: 50820, training_loss: 2.01410
Epoch: 2/20, step: 50840, training_loss: 2.26836
Epoch: 2/20, step: 50860, training_loss: 2.61875
Epoch: 2/20, step: 50880, training_loss: 2.57925
Epoch: 2/20, step: 50900, training_loss: 1.98568
Epoch: 2/20, step: 50920, training_loss: 2.85013
Epoch: 2/20, step: 50940, training_loss: 2.74705
Epoch: 2/20, step: 50960, training_loss: 2.00189
Epoch: 2/20, step: 50980, training_loss: 2.12664
Epoch: 2/20, step: 51000, training_loss: 2.19062
accuracy: 0.53, validation_loss: 1.883291482925415, num_samples: 100
Epoch: 2/20, step: 51020, training_loss: 2.76160
Epoch: 2/20, step: 51040, training_loss: 2.84886
Epoch: 2/20, step: 51060, training_loss: 2.08418
Epoch: 2/20, step: 51080, training_loss: 1.64402
Epoch: 2/20, step: 51100, training_loss: 2.15873
Epoch: 2/20, step: 51120, training_loss: 1.83041
Epoch: 2/20, step: 51140, training_loss: 2.80791
Epoch: 2/20, step: 51160, training_loss: 1.52258
Epoch: 2/20, step: 51180, training_loss: 2.33631
Epoch: 2/20, step: 51200, training_loss: 2.47977
Epoch: 2/20, step: 51220, training_loss: 2.14217
Epoch: 2/20, step: 51240, training_loss: 1.94824
Epoch: 2/20, step: 51260, training_loss: 1.60593
Epoch: 2/20, step: 51280, training_loss: 1.93594
Epoch: 2/20, step: 51300, training_loss: 1.92570
Epoch: 2/20, step: 51320, training_loss: 1.33292
Epoch: 2/20, step: 51340, training_loss: 2.01559
Epoch: 2/20, step: 51360, training_loss: 2.17552
Epoch: 2/20, step: 51380, training_loss: 2.50586
Epoch: 2/20, step: 51400, training_loss: 2.58978
Epoch: 2/20, step: 51420, training_loss: 2.13143
Epoch: 2/20, step: 51440, training_loss: 2.36027
Epoch: 2/20, step: 51460, training_loss: 2.77453
Epoch: 2/20, step: 51480, training_loss: 2.30200
Epoch: 2/20, step: 51500, training_loss: 2.66123
Epoch: 2/20, step: 51520, training_loss: 2.60729
Epoch: 2/20, step: 51540, training_loss: 1.78789
Epoch: 2/20, step: 51560, training_loss: 2.02370
Epoch: 2/20, step: 51580, training_loss: 1.59472
Epoch: 2/20, step: 51600, training_loss: 2.11000
Epoch: 2/20, step: 51620, training_loss: 2.29495
Epoch: 2/20, step: 51640, training_loss: 1.77602
Epoch: 2/20, step: 51660, training_loss: 1.69587
Epoch: 2/20, step: 51680, training_loss: 2.37318
Epoch: 2/20, step: 51700, training_loss: 3.18653
Epoch: 2/20, step: 51720, training_loss: 1.64242
Epoch: 2/20, step: 51740, training_loss: 2.05084
Epoch: 2/20, step: 51760, training_loss: 2.37748
Epoch: 2/20, step: 51780, training_loss: 2.40987
Epoch: 2/20, step: 51800, training_loss: 2.30656
Epoch: 2/20, step: 51820, training_loss: 1.82425
Epoch: 2/20, step: 51840, training_loss: 2.32828
Epoch: 2/20, step: 51860, training_loss: 2.24491
Epoch: 2/20, step: 51880, training_loss: 2.29360
Epoch: 2/20, step: 51900, training_loss: 2.61533
Epoch: 2/20, step: 51920, training_loss: 2.16647
Epoch: 2/20, step: 51940, training_loss: 1.65421
Epoch: 2/20, step: 51960, training_loss: 1.46813
Epoch: 2/20, step: 51980, training_loss: 2.08361
Epoch: 2/20, step: 52000, training_loss: 1.32493
accuracy: 0.43, validation_loss: 2.0991666316986084, num_samples: 100
Epoch: 2/20, step: 52020, training_loss: 2.57445
Epoch: 2/20, step: 52040, training_loss: 2.62085
Epoch: 2/20, step: 52060, training_loss: 3.21275
Epoch: 2/20, step: 52080, training_loss: 2.57849
Epoch: 2/20, step: 52100, training_loss: 2.25043
Epoch: 2/20, step: 52120, training_loss: 2.67222
Epoch: 2/20, step: 52140, training_loss: 1.40333
Epoch: 2/20, step: 52160, training_loss: 2.08771
Epoch: 2/20, step: 52180, training_loss: 2.04793
Epoch: 2/20, step: 52200, training_loss: 2.97974
Epoch: 2/20, step: 52220, training_loss: 1.55433
Epoch: 2/20, step: 52240, training_loss: 3.08712
Epoch: 2/20, step: 52260, training_loss: 3.31670
Epoch: 2/20, step: 52280, training_loss: 2.14543
Epoch: 2/20, step: 52300, training_loss: 2.32364
Epoch: 2/20, step: 52320, training_loss: 1.74610
Epoch: 2/20, step: 52340, training_loss: 2.30627
Epoch: 2/20, step: 52360, training_loss: 2.71955
Epoch: 2/20, step: 52380, training_loss: 1.88847
Epoch: 2/20, step: 52400, training_loss: 2.79605
Epoch: 2/20, step: 52420, training_loss: 2.89134
Epoch: 2/20, step: 52440, training_loss: 1.35311
Epoch: 2/20, step: 52460, training_loss: 2.62656
Epoch: 2/20, step: 52480, training_loss: 1.68931
Epoch: 2/20, step: 52500, training_loss: 1.79931
Epoch: 2/20, step: 52520, training_loss: 2.45111
Epoch: 2/20, step: 52540, training_loss: 2.50140
Epoch: 2/20, step: 52560, training_loss: 2.62876
Epoch: 2/20, step: 52580, training_loss: 1.98011
Epoch: 2/20, step: 52600, training_loss: 2.74436
Epoch: 2/20, step: 52620, training_loss: 1.86790
Epoch: 2/20, step: 52640, training_loss: 1.67959
Epoch: 2/20, step: 52660, training_loss: 1.84142
Epoch: 2/20, step: 52680, training_loss: 2.06833
Epoch: 2/20, step: 52700, training_loss: 2.24360
Epoch: 2/20, step: 52720, training_loss: 2.39964
Epoch: 2/20, step: 52740, training_loss: 2.66416
Epoch: 2/20, step: 52760, training_loss: 1.79213
Epoch: 2/20, step: 52780, training_loss: 2.07628
Epoch: 2/20, step: 52800, training_loss: 1.75063
Epoch: 2/20, step: 52820, training_loss: 1.28142
Epoch: 2/20, step: 52840, training_loss: 2.09617
Epoch: 2/20, step: 52860, training_loss: 2.29905
Epoch: 2/20, step: 52880, training_loss: 1.67247
Epoch: 2/20, step: 52900, training_loss: 2.08665
Epoch: 2/20, step: 52920, training_loss: 1.62265
Epoch: 2/20, step: 52940, training_loss: 3.14420
Epoch: 2/20, step: 52960, training_loss: 2.44720
Epoch: 2/20, step: 52980, training_loss: 1.70993
Epoch: 2/20, step: 53000, training_loss: 1.26446
accuracy: 0.35, validation_loss: 2.1361501216888428, num_samples: 100
Epoch: 2/20, step: 53020, training_loss: 2.35403
Epoch: 2/20, step: 53040, training_loss: 2.02652
Epoch: 2/20, step: 53060, training_loss: 2.48492
Epoch: 2/20, step: 53080, training_loss: 2.22776
Epoch: 2/20, step: 53100, training_loss: 2.50741
Epoch: 2/20, step: 53120, training_loss: 2.44461
Epoch: 2/20, step: 53140, training_loss: 1.33474
Epoch: 2/20, step: 53160, training_loss: 2.33552
Epoch: 2/20, step: 53180, training_loss: 2.48628
Epoch: 2/20, step: 53200, training_loss: 1.81915
Epoch: 2/20, step: 53220, training_loss: 2.55613
Epoch: 2/20, step: 53240, training_loss: 1.70051
Epoch: 2/20, step: 53260, training_loss: 1.87004
Epoch: 2/20, step: 53280, training_loss: 2.20913
Epoch: 2/20, step: 53300, training_loss: 2.12665
Epoch: 2/20, step: 53320, training_loss: 2.26027
Epoch: 2/20, step: 53340, training_loss: 2.18898
Epoch: 2/20, step: 53360, training_loss: 2.50648
Epoch: 2/20, step: 53380, training_loss: 2.31451
Epoch: 2/20, step: 53400, training_loss: 1.60544
Epoch: 2/20, step: 53420, training_loss: 2.25706
Epoch: 2/20, step: 53440, training_loss: 2.40368
Epoch: 2/20, step: 53460, training_loss: 2.25600
Epoch: 2/20, step: 53480, training_loss: 2.16517
Epoch: 2/20, step: 53500, training_loss: 2.08447
Epoch: 2/20, step: 53520, training_loss: 2.55780
Epoch: 2/20, step: 53540, training_loss: 2.13315
Epoch: 2/20, step: 53560, training_loss: 2.43454
Epoch: 2/20, step: 53580, training_loss: 2.00599
Epoch: 2/20, step: 53600, training_loss: 2.23208
Epoch: 2/20, step: 53620, training_loss: 2.47696
Epoch: 2/20, step: 53640, training_loss: 2.04036
Epoch: 2/20, step: 53660, training_loss: 2.52584
Epoch: 2/20, step: 53680, training_loss: 2.85840
Epoch: 2/20, step: 53700, training_loss: 2.00654
Epoch: 2/20, step: 53720, training_loss: 1.72768
Epoch: 2/20, step: 53740, training_loss: 2.88664
Epoch: 2/20, step: 53760, training_loss: 1.96332
Epoch: 2/20, step: 53780, training_loss: 1.61922
Epoch: 2/20, step: 53800, training_loss: 1.90888
Epoch: 2/20, step: 53820, training_loss: 2.04388
Epoch: 2/20, step: 53840, training_loss: 1.65752
Epoch: 2/20, step: 53860, training_loss: 2.73382
Epoch: 2/20, step: 53880, training_loss: 1.61179
Epoch: 2/20, step: 53900, training_loss: 2.09131
Epoch: 2/20, step: 53920, training_loss: 1.67350
Epoch: 2/20, step: 53940, training_loss: 2.63400
Epoch: 2/20, step: 53960, training_loss: 2.10235
Epoch: 2/20, step: 53980, training_loss: 1.72143
Epoch: 2/20, step: 54000, training_loss: 1.61458
accuracy: 0.42, validation_loss: 2.223466157913208, num_samples: 100
Epoch: 2/20, step: 54020, training_loss: 1.89650
Epoch: 2/20, step: 54040, training_loss: 1.82951
Epoch: 2/20, step: 54060, training_loss: 2.02737
Epoch: 2/20, step: 54080, training_loss: 1.69920
Epoch: 2/20, step: 54100, training_loss: 2.50969
Epoch: 2/20, step: 54120, training_loss: 1.97449
Epoch: 2/20, step: 54140, training_loss: 1.79926
Epoch: 2/20, step: 54160, training_loss: 2.15875
Epoch: 2/20, step: 54180, training_loss: 2.16820
Epoch: 2/20, step: 54200, training_loss: 1.94929
Epoch: 2/20, step: 54220, training_loss: 1.89138
Epoch: 2/20, step: 54240, training_loss: 2.17143
Epoch: 2/20, step: 54260, training_loss: 2.05808
Epoch: 2/20, step: 54280, training_loss: 1.84601
Epoch: 2/20, step: 54300, training_loss: 2.29778
Epoch: 2/20, step: 54320, training_loss: 2.34073
Epoch: 2/20, step: 54340, training_loss: 2.15280
Epoch: 2/20, step: 54360, training_loss: 1.69802
Epoch: 2/20, step: 54380, training_loss: 2.43821
Epoch: 2/20, step: 54400, training_loss: 1.77476
Epoch: 2/20, step: 54420, training_loss: 1.82435
Epoch: 2/20, step: 54440, training_loss: 1.87252
Epoch: 2/20, step: 54460, training_loss: 1.86854
Epoch: 2/20, step: 54480, training_loss: 2.94559
Epoch: 2/20, step: 54500, training_loss: 2.32891
Epoch: 2/20, step: 54520, training_loss: 2.75051
Epoch: 2/20, step: 54540, training_loss: 2.78119
Epoch: 2/20, step: 54560, training_loss: 2.75892
Epoch: 2/20, step: 54580, training_loss: 1.32246
Epoch: 2/20, step: 54600, training_loss: 2.03200
Epoch: 2/20, step: 54620, training_loss: 2.06195
Epoch: 2/20, step: 54640, training_loss: 2.27310
Epoch: 2/20, step: 54660, training_loss: 2.36119
Epoch: 2/20, step: 54680, training_loss: 2.63060
Epoch: 2/20, step: 54700, training_loss: 2.57332
Epoch: 2/20, step: 54720, training_loss: 1.96729
Epoch: 2/20, step: 54740, training_loss: 2.45552
Epoch: 2/20, step: 54760, training_loss: 2.75905
Epoch: 2/20, step: 54780, training_loss: 1.67778
Epoch: 2/20, step: 54800, training_loss: 1.57986
Epoch: 2/20, step: 54820, training_loss: 1.69510
Epoch: 2/20, step: 54840, training_loss: 2.29762
Epoch: 2/20, step: 54860, training_loss: 2.84253
Epoch: 2/20, step: 54880, training_loss: 1.84257
Epoch: 2/20, step: 54900, training_loss: 2.68887
Epoch: 2/20, step: 54920, training_loss: 1.75953
Epoch: 2/20, step: 54940, training_loss: 3.39545
Epoch: 2/20, step: 54960, training_loss: 1.61325
Epoch: 2/20, step: 54980, training_loss: 2.63969
Epoch: 2/20, step: 55000, training_loss: 1.91855
accuracy: 0.38, validation_loss: 2.2396488189697266, num_samples: 100
Epoch: 2/20, step: 55020, training_loss: 3.68545
Epoch: 2/20, step: 55040, training_loss: 2.29053
Epoch: 2/20, step: 55060, training_loss: 2.09174
Epoch: 2/20, step: 55080, training_loss: 2.66366
Epoch: 2/20, step: 55100, training_loss: 3.01201
Epoch: 2/20, step: 55120, training_loss: 2.92811
Epoch: 2/20, step: 55140, training_loss: 3.03222
Epoch: 2/20, step: 55160, training_loss: 3.19362
Epoch: 2/20, step: 55180, training_loss: 2.03604
Epoch: 2/20, step: 55200, training_loss: 2.46156
Epoch: 2/20, step: 55220, training_loss: 2.06319
Epoch: 2/20, step: 55240, training_loss: 2.52249
Epoch: 2/20, step: 55260, training_loss: 1.48210
Epoch: 2/20, step: 55280, training_loss: 1.69879
Epoch: 2/20, step: 55300, training_loss: 2.09526
Epoch: 2/20, step: 55320, training_loss: 2.32354
Epoch: 2/20, step: 55340, training_loss: 2.35019
Epoch: 2/20, step: 55360, training_loss: 2.52590
Epoch: 2/20, step: 55380, training_loss: 2.82601
Epoch: 2/20, step: 55400, training_loss: 1.93021
Epoch: 2/20, step: 55420, training_loss: 1.61903
Epoch: 2/20, step: 55440, training_loss: 2.75067
Epoch: 2/20, step: 55460, training_loss: 2.71779
Epoch: 2/20, step: 55480, training_loss: 2.36633
Epoch: 2/20, step: 55500, training_loss: 2.26857
Epoch: 2/20, step: 55520, training_loss: 1.49390
Epoch: 2/20, step: 55540, training_loss: 2.09386
Epoch: 2/20, step: 55560, training_loss: 2.56194
Epoch: 2/20, step: 55580, training_loss: 2.13878
Epoch: 2/20, step: 55600, training_loss: 2.53705
Epoch: 2/20, step: 55620, training_loss: 2.56119
Epoch: 2/20, step: 55640, training_loss: 2.43319
Epoch: 2/20, step: 55660, training_loss: 2.04680
Epoch: 2/20, step: 55680, training_loss: 1.77478
Epoch: 2/20, step: 55700, training_loss: 2.04730
Epoch: 2/20, step: 55720, training_loss: 2.40742
Epoch: 2/20, step: 55740, training_loss: 2.25130
Epoch: 2/20, step: 55760, training_loss: 3.05146
Epoch: 2/20, step: 55780, training_loss: 2.11379
Epoch: 2/20, step: 55800, training_loss: 2.05503
Epoch: 2/20, step: 55820, training_loss: 2.26120
Epoch: 2/20, step: 55840, training_loss: 1.94317
Epoch: 2/20, step: 55860, training_loss: 2.26304
Epoch: 2/20, step: 55880, training_loss: 2.68219
Epoch: 2/20, step: 55900, training_loss: 1.51033
Epoch: 2/20, step: 55920, training_loss: 1.52114
Epoch: 2/20, step: 55940, training_loss: 2.41124
Epoch: 2/20, step: 55960, training_loss: 2.74232
Epoch: 2/20, step: 55980, training_loss: 2.56593
Epoch: 2/20, step: 56000, training_loss: 2.71713
accuracy: 0.5, validation_loss: 1.899399995803833, num_samples: 100
Epoch: 2/20, step: 56020, training_loss: 2.64664
Epoch: 2/20, step: 56040, training_loss: 2.87175
Epoch: 2/20, step: 56060, training_loss: 1.82041
Epoch: 2/20, step: 56080, training_loss: 2.25955
Epoch: 2/20, step: 56100, training_loss: 2.99108
Epoch: 2/20, step: 56120, training_loss: 2.29872
Epoch: 2/20, step: 56140, training_loss: 2.24243
Epoch: 2/20, step: 56160, training_loss: 1.79872
Epoch: 2/20, step: 56180, training_loss: 1.69327
Epoch: 2/20, step: 56200, training_loss: 2.84509
Epoch: 2/20, step: 56220, training_loss: 2.32964
Epoch: 2/20, step: 56240, training_loss: 2.25366
Epoch: 2/20, step: 56260, training_loss: 1.91972
Epoch: 2/20, step: 56280, training_loss: 2.61454
Epoch: 2/20, step: 56300, training_loss: 2.02888
Epoch: 2/20, step: 56320, training_loss: 1.49060
Epoch: 2/20, step: 56340, training_loss: 1.86038
Epoch: 2/20, step: 56360, training_loss: 2.18646
Epoch: 2/20, step: 56380, training_loss: 2.26219
Epoch: 2/20, step: 56400, training_loss: 2.28945
Epoch: 2/20, step: 56420, training_loss: 1.96804
Epoch: 2/20, step: 56440, training_loss: 1.29183
Epoch: 2/20, step: 56460, training_loss: 1.78554
Epoch: 2/20, step: 56480, training_loss: 1.93369
Epoch: 2/20, step: 56500, training_loss: 2.41604
Epoch: 2/20, step: 56520, training_loss: 2.69308
Epoch: 2/20, step: 56540, training_loss: 2.12255
Epoch: 2/20, step: 56560, training_loss: 2.67924
Epoch: 2/20, step: 56580, training_loss: 2.21940
Epoch: 2/20, step: 56600, training_loss: 2.29328
Epoch: 2/20, step: 56620, training_loss: 1.97520
Epoch: 2/20, step: 56640, training_loss: 2.34895
Epoch: 2/20, step: 56660, training_loss: 1.60807
Epoch: 2/20, step: 56680, training_loss: 2.22495
Epoch: 2/20, step: 56700, training_loss: 2.47293
Epoch: 2/20, step: 56720, training_loss: 3.04857
Epoch: 2/20, step: 56740, training_loss: 2.53894
Epoch: 2/20, step: 56760, training_loss: 1.74683
Epoch: 2/20, step: 56780, training_loss: 3.05261
Epoch: 2/20, step: 56800, training_loss: 2.92642
Epoch: 2/20, step: 56820, training_loss: 2.51647
Epoch: 2/20, step: 56840, training_loss: 2.42056
Epoch: 2/20, step: 56860, training_loss: 2.24093
Epoch: 2/20, step: 56880, training_loss: 2.36087
Epoch: 2/20, step: 56900, training_loss: 2.62441
Epoch: 2/20, step: 56920, training_loss: 2.67173
Epoch: 2/20, step: 56940, training_loss: 2.17183
Epoch: 2/20, step: 56960, training_loss: 2.21434
Epoch: 2/20, step: 56980, training_loss: 2.66320
Epoch: 2/20, step: 57000, training_loss: 1.88314
accuracy: 0.49, validation_loss: 2.202984571456909, num_samples: 100
Epoch: 2/20, step: 57020, training_loss: 2.63291
Epoch: 2/20, step: 57040, training_loss: 2.51243
Epoch: 2/20, step: 57060, training_loss: 2.66120
Epoch: 2/20, step: 57080, training_loss: 2.29768
Epoch: 2/20, step: 57100, training_loss: 2.14950
Epoch: 2/20, step: 57120, training_loss: 2.44043
Epoch: 2/20, step: 57140, training_loss: 1.61068
Epoch: 2/20, step: 57160, training_loss: 2.09963
Epoch: 2/20, step: 57180, training_loss: 2.21367
Epoch: 2/20, step: 57200, training_loss: 1.47685
Epoch: 2/20, step: 57220, training_loss: 2.63063
Epoch: 2/20, step: 57240, training_loss: 2.47056
Epoch: 2/20, step: 57260, training_loss: 1.84505
Epoch: 2/20, step: 57280, training_loss: 3.38191
Epoch: 2/20, step: 57300, training_loss: 2.22830
Epoch: 2/20, step: 57320, training_loss: 2.72476
Epoch: 2/20, step: 57340, training_loss: 1.91885
Epoch: 2/20, step: 57360, training_loss: 1.87389
Epoch: 2/20, step: 57380, training_loss: 2.00758
Epoch: 2/20, step: 57400, training_loss: 1.85855
Epoch: 2/20, step: 57420, training_loss: 2.74375
Epoch: 2/20, step: 57440, training_loss: 2.40184
Epoch: 2/20, step: 57460, training_loss: 2.42172
Epoch: 2/20, step: 57480, training_loss: 1.64370
Epoch: 2/20, step: 57500, training_loss: 2.51925
Epoch: 2/20, step: 57520, training_loss: 2.34783
Epoch: 2/20, step: 57540, training_loss: 2.70469
Epoch: 2/20, step: 57560, training_loss: 1.83047
Epoch: 2/20, step: 57580, training_loss: 1.44921
Epoch: 2/20, step: 57600, training_loss: 3.20507
Epoch: 2/20, step: 57620, training_loss: 1.96197
Epoch: 2/20, step: 57640, training_loss: 1.59392
Epoch: 2/20, step: 57660, training_loss: 2.81672
Epoch: 2/20, step: 57680, training_loss: 1.99590
Epoch: 2/20, step: 57700, training_loss: 1.87429
Epoch: 2/20, step: 57720, training_loss: 2.24508
Epoch: 2/20, step: 57740, training_loss: 2.78525
Epoch: 2/20, step: 57760, training_loss: 1.40960
Epoch: 2/20, step: 57780, training_loss: 1.67393
Epoch: 2/20, step: 57800, training_loss: 1.73262
Epoch: 2/20, step: 57820, training_loss: 2.60029
Epoch: 2/20, step: 57840, training_loss: 1.94104
Epoch: 2/20, step: 57860, training_loss: 2.36354
Epoch: 2/20, step: 57880, training_loss: 2.47276
Epoch: 2/20, step: 57900, training_loss: 2.08707
Epoch: 2/20, step: 57920, training_loss: 1.95001
Epoch: 2/20, step: 57940, training_loss: 1.32202
Epoch: 2/20, step: 57960, training_loss: 1.65523
Epoch: 2/20, step: 57980, training_loss: 2.25201
Epoch: 2/20, step: 58000, training_loss: 1.98770
accuracy: 0.41, validation_loss: 2.1580076217651367, num_samples: 100
Epoch: 2/20, step: 58020, training_loss: 2.34244
Epoch: 2/20, step: 58040, training_loss: 1.81662
Epoch: 2/20, step: 58060, training_loss: 1.89528
Epoch: 2/20, step: 58080, training_loss: 1.90649
Epoch: 2/20, step: 58100, training_loss: 1.63248
Epoch: 2/20, step: 58120, training_loss: 2.17058
Epoch: 2/20, step: 58140, training_loss: 2.47284
Epoch: 2/20, step: 58160, training_loss: 1.52193
Epoch: 2/20, step: 58180, training_loss: 2.72697
Epoch: 2/20, step: 58200, training_loss: 2.15831
Epoch: 2/20, step: 58220, training_loss: 2.60982
Epoch: 2/20, step: 58240, training_loss: 1.95633
Epoch: 2/20, step: 58260, training_loss: 1.49633
Epoch: 2/20, step: 58280, training_loss: 1.66402
Epoch: 2/20, step: 58300, training_loss: 2.52314
Epoch: 2/20, step: 58320, training_loss: 2.34215
Epoch: 2/20, step: 58340, training_loss: 2.89834
Epoch: 2/20, step: 58360, training_loss: 2.58425
Epoch: 2/20, step: 58380, training_loss: 2.05937
Epoch: 2/20, step: 58400, training_loss: 2.38526
Epoch: 2/20, step: 58420, training_loss: 2.26433
Epoch: 2/20, step: 58440, training_loss: 1.78570
Epoch: 2/20, step: 58460, training_loss: 3.09142
Epoch: 2/20, step: 58480, training_loss: 1.86913
Epoch: 2/20, step: 58500, training_loss: 2.12495
Epoch: 2/20, step: 58520, training_loss: 2.65417
Epoch: 2/20, step: 58540, training_loss: 2.95581
Epoch: 2/20, step: 58560, training_loss: 2.74770
Epoch: 2/20, step: 58580, training_loss: 1.89131
Epoch: 2/20, step: 58600, training_loss: 2.34328
Epoch: 2/20, step: 58620, training_loss: 2.65065
Epoch: 2/20, step: 58640, training_loss: 1.60879
Epoch: 2/20, step: 58660, training_loss: 2.37642
Epoch: 2/20, step: 58680, training_loss: 2.66893
Epoch: 2/20, step: 58700, training_loss: 2.65612
Epoch: 2/20, step: 58720, training_loss: 1.42755
Epoch: 2/20, step: 58740, training_loss: 2.03054
Epoch: 2/20, step: 58760, training_loss: 2.47276
Epoch: 2/20, step: 58780, training_loss: 2.11670
Epoch: 2/20, step: 58800, training_loss: 2.08374
Epoch: 2/20, step: 58820, training_loss: 1.94337
Epoch: 2/20, step: 58840, training_loss: 2.52675
Epoch: 2/20, step: 58860, training_loss: 2.22820
Epoch: 2/20, step: 58880, training_loss: 3.07091
Epoch: 2/20, step: 58900, training_loss: 2.10317
Epoch: 2/20, step: 58920, training_loss: 2.44410
Epoch: 2/20, step: 58940, training_loss: 3.04792
Epoch: 2/20, step: 58960, training_loss: 2.27391
Epoch: 2/20, step: 58980, training_loss: 2.58511
Epoch: 2/20, step: 59000, training_loss: 1.82012
accuracy: 0.44, validation_loss: 1.8817330598831177, num_samples: 100
Epoch: 2/20, step: 59020, training_loss: 1.38160
Epoch: 2/20, step: 59040, training_loss: 2.28606
Epoch: 2/20, step: 59060, training_loss: 1.89086
Epoch: 2/20, step: 59080, training_loss: 1.89179
Epoch: 2/20, step: 59100, training_loss: 1.98180
Epoch: 2/20, step: 59120, training_loss: 1.94834
Epoch: 2/20, step: 59140, training_loss: 2.65318
Epoch: 2/20, step: 59160, training_loss: 1.69557
Epoch: 2/20, step: 59180, training_loss: 2.00960
Epoch: 2/20, step: 59200, training_loss: 2.17859
Epoch: 2/20, step: 59220, training_loss: 3.17177
Epoch: 2/20, step: 59240, training_loss: 2.39143
Epoch: 2/20, step: 59260, training_loss: 2.54663
Epoch: 2/20, step: 59280, training_loss: 2.15929
Epoch: 2/20, step: 59300, training_loss: 2.62157
Epoch: 2/20, step: 59320, training_loss: 1.85884
Epoch: 2/20, step: 59340, training_loss: 1.94362
Epoch: 2/20, step: 59360, training_loss: 2.41908
Epoch: 2/20, step: 59380, training_loss: 2.48949
Epoch: 2/20, step: 59400, training_loss: 1.75702
Epoch: 2/20, step: 59420, training_loss: 1.70917
Epoch: 2/20, step: 59440, training_loss: 2.06580
Epoch: 2/20, step: 59460, training_loss: 2.57136
Epoch: 2/20, step: 59480, training_loss: 2.84567
Epoch: 2/20, step: 59500, training_loss: 1.62582
Epoch: 2/20, step: 59520, training_loss: 2.38572
Epoch: 2/20, step: 59540, training_loss: 2.59221
Epoch: 2/20, step: 59560, training_loss: 1.62649
Epoch: 2/20, step: 59580, training_loss: 1.83186
Epoch: 2/20, step: 59600, training_loss: 1.66482
Epoch: 2/20, step: 59620, training_loss: 1.91705
Epoch: 2/20, step: 59640, training_loss: 2.15404
Epoch: 2/20, step: 59660, training_loss: 1.39351
Epoch: 2/20, step: 59680, training_loss: 2.21285
Epoch: 2/20, step: 59700, training_loss: 2.18689
Epoch: 2/20, step: 59720, training_loss: 2.79047
Epoch: 2/20, step: 59740, training_loss: 2.58915
Epoch: 2/20, step: 59760, training_loss: 1.45161
Epoch: 2/20, step: 59780, training_loss: 2.35601
Epoch: 2/20, step: 59800, training_loss: 2.37943
Epoch: 2/20, step: 59820, training_loss: 1.96308
Epoch: 2/20, step: 59840, training_loss: 1.95587
Epoch: 2/20, step: 59860, training_loss: 2.79300
Epoch: 2/20, step: 59880, training_loss: 2.26511
Epoch: 2/20, step: 59900, training_loss: 1.85783
Epoch: 2/20, step: 59920, training_loss: 1.56509
Epoch: 2/20, step: 59940, training_loss: 1.87806
Epoch: 2/20, step: 59960, training_loss: 1.60147
Epoch: 2/20, step: 59980, training_loss: 2.27744
Epoch: 2/20, step: 60000, training_loss: 1.71861
accuracy: 0.42, validation_loss: 2.160860538482666, num_samples: 100
Epoch: 2/20, step: 60020, training_loss: 1.98150
Epoch: 2/20, step: 60040, training_loss: 3.01413
Epoch: 2/20, step: 60060, training_loss: 3.00384
Epoch: 2/20, step: 60080, training_loss: 2.54777
Epoch: 2/20, step: 60100, training_loss: 2.25516
Epoch: 2/20, step: 60120, training_loss: 1.89685
Epoch: 2/20, step: 60140, training_loss: 2.68347
Epoch: 2/20, step: 60160, training_loss: 2.72169
Epoch: 2/20, step: 60180, training_loss: 2.19119
Epoch: 2/20, step: 60200, training_loss: 2.10268
Epoch: 2/20, step: 60220, training_loss: 1.89337
Epoch: 2/20, step: 60240, training_loss: 2.75571
Epoch: 2/20, step: 60260, training_loss: 2.16783
Epoch: 2/20, step: 60280, training_loss: 1.88809
Epoch: 2/20, step: 60300, training_loss: 2.70588
Epoch: 2/20, step: 60320, training_loss: 2.26283
Epoch: 2/20, step: 60340, training_loss: 2.49521
Epoch: 2/20, step: 60360, training_loss: 2.67672
Epoch: 2/20, step: 60380, training_loss: 1.92757
Epoch: 2/20, step: 60400, training_loss: 2.23106
Epoch: 2/20, step: 60420, training_loss: 2.86524
Epoch: 2/20, step: 60440, training_loss: 2.33853
Epoch: 2/20, step: 60460, training_loss: 2.36876
Epoch: 2/20, step: 60480, training_loss: 1.92366
Epoch: 2/20, step: 60500, training_loss: 1.78358
Epoch: 2/20, step: 60520, training_loss: 2.76421
Epoch: 2/20, step: 60540, training_loss: 1.99685
Epoch: 2/20, step: 60560, training_loss: 1.82337
Epoch: 2/20, step: 60580, training_loss: 1.81029
Epoch: 2/20, step: 60600, training_loss: 2.53322
Epoch: 2/20, step: 60620, training_loss: 2.69403
Epoch: 2/20, step: 60640, training_loss: 2.09875
Epoch: 2/20, step: 60660, training_loss: 1.43914
Epoch: 2/20, step: 60680, training_loss: 1.58930
Epoch: 2/20, step: 60700, training_loss: 1.60533
Epoch: 2/20, step: 60720, training_loss: 2.33561
Epoch: 2/20, step: 60740, training_loss: 1.84152
Epoch: 2/20, step: 60760, training_loss: 2.00632
Epoch: 2/20, step: 60780, training_loss: 2.40186
Epoch: 2/20, step: 60800, training_loss: 1.86748
Epoch: 2/20, step: 60820, training_loss: 2.34175
Epoch: 2/20, step: 60840, training_loss: 3.13574
Epoch: 2/20, step: 60860, training_loss: 2.79979
Epoch: 2/20, step: 60880, training_loss: 1.70665
Epoch: 2/20, step: 60900, training_loss: 1.95252
Epoch: 2/20, step: 60920, training_loss: 2.00910
Epoch: 2/20, step: 60940, training_loss: 2.02439
Epoch: 2/20, step: 60960, training_loss: 2.74010
Epoch: 2/20, step: 60980, training_loss: 1.60950
Epoch: 2/20, step: 61000, training_loss: 1.86922
accuracy: 0.42, validation_loss: 2.3481287956237793, num_samples: 100
Epoch: 2/20, step: 61020, training_loss: 2.04391
Epoch: 2/20, step: 61040, training_loss: 2.15778
Epoch: 2/20, step: 61060, training_loss: 1.77390
Epoch: 2/20, step: 61080, training_loss: 2.47489
Epoch: 2/20, step: 61100, training_loss: 2.00266
Epoch: 2/20, step: 61120, training_loss: 1.90127
Epoch: 2/20, step: 61140, training_loss: 1.21128
Epoch: 2/20, step: 61160, training_loss: 2.91685
Epoch: 2/20, step: 61180, training_loss: 1.87699
Epoch: 2/20, step: 61200, training_loss: 2.01439
Epoch: 2/20, step: 61220, training_loss: 2.24002
Epoch: 2/20, step: 61240, training_loss: 2.99614
Epoch: 2/20, step: 61260, training_loss: 1.89858
Epoch: 2/20, step: 61280, training_loss: 2.91135
Epoch: 2/20, step: 61300, training_loss: 1.92635
Epoch: 2/20, step: 61320, training_loss: 2.65062
Epoch: 2/20, step: 61340, training_loss: 2.58777
Epoch: 2/20, step: 61360, training_loss: 1.41184
Epoch: 2/20, step: 61380, training_loss: 2.15476
Epoch: 2/20, step: 61400, training_loss: 1.71065
Epoch: 2/20, step: 61420, training_loss: 2.07064
Epoch: 2/20, step: 61440, training_loss: 2.40793
Epoch: 2/20, step: 61460, training_loss: 2.27372
Epoch: 2/20, step: 61480, training_loss: 1.31363
Epoch: 2/20, step: 61500, training_loss: 2.66049
Epoch: 2/20, step: 61520, training_loss: 1.60141
Epoch: 2/20, step: 61540, training_loss: 1.85372
Epoch: 2/20, step: 61560, training_loss: 2.56582
Epoch: 2/20, step: 61580, training_loss: 2.47281
Epoch: 2/20, step: 61600, training_loss: 2.59982
Epoch: 2/20, step: 61620, training_loss: 1.92767
Epoch: 2/20, step: 61640, training_loss: 1.80146
Epoch: 2/20, step: 61660, training_loss: 2.28010
Epoch: 2/20, step: 61680, training_loss: 2.73157
Epoch: 2/20, step: 61700, training_loss: 1.47176
Epoch: 2/20, step: 61720, training_loss: 2.60159
Epoch: 2/20, step: 61740, training_loss: 3.38371
Epoch: 2/20, step: 61760, training_loss: 1.90412
Epoch: 2/20, step: 61780, training_loss: 2.00672
Epoch: 2/20, step: 61800, training_loss: 2.11562
Epoch: 2/20, step: 61820, training_loss: 2.62384
Epoch: 2/20, step: 61840, training_loss: 2.82377
Epoch: 2/20, step: 61860, training_loss: 2.14709
Epoch: 2/20, step: 61880, training_loss: 1.93539
Epoch: 2/20, step: 61900, training_loss: 1.64248
Epoch: 2/20, step: 61920, training_loss: 2.51731
Epoch: 2/20, step: 61940, training_loss: 2.06256
Epoch: 2/20, step: 61960, training_loss: 3.07586
Epoch: 2/20, step: 61980, training_loss: 2.53255
Epoch: 2/20, step: 62000, training_loss: 1.91053
accuracy: 0.32, validation_loss: 2.4807403087615967, num_samples: 100
Epoch: 2/20, step: 62020, training_loss: 1.09288
Epoch: 2/20, step: 62040, training_loss: 2.15664
Epoch: 2/20, step: 62060, training_loss: 2.80134
Epoch: 2/20, step: 62080, training_loss: 2.52729
Epoch: 2/20, step: 62100, training_loss: 2.39585
Epoch: 2/20, step: 62120, training_loss: 3.23727
Epoch: 2/20, step: 62140, training_loss: 1.97975
Epoch: 2/20, step: 62160, training_loss: 2.33008
Epoch: 2/20, step: 62180, training_loss: 1.85614
Epoch: 2/20, step: 62200, training_loss: 1.49036
Epoch: 2/20, step: 62220, training_loss: 1.81375
Epoch: 2/20, step: 62240, training_loss: 1.95890
Epoch: 2/20, step: 62260, training_loss: 2.78765
Epoch: 2/20, step: 62280, training_loss: 2.09400
Epoch: 2/20, step: 62300, training_loss: 2.36177
Epoch: 2/20, step: 62320, training_loss: 1.65542
Epoch: 2/20, step: 62340, training_loss: 2.41998
Epoch: 2/20, step: 62360, training_loss: 2.83473
Epoch: 2/20, step: 62380, training_loss: 2.34396
Epoch: 2/20, step: 62400, training_loss: 2.55755
Epoch: 2/20, step: 62420, training_loss: 2.46942
Epoch: 2/20, step: 62440, training_loss: 1.85710
Epoch: 2/20, step: 62460, training_loss: 2.25354
Epoch: 2/20, step: 62480, training_loss: 1.84491
Epoch: 2/20, step: 62500, training_loss: 2.49465
Epoch: 2/20, step: 62520, training_loss: 2.19494
Epoch: 2/20, step: 62540, training_loss: 3.28673
Epoch: 2/20, step: 62560, training_loss: 2.81786
Epoch: 2/20, step: 62580, training_loss: 2.68694
Epoch: 2/20, step: 62600, training_loss: 2.74306
Epoch: 2/20, step: 62620, training_loss: 1.33469
Epoch: 2/20, step: 62640, training_loss: 2.42705
Epoch: 2/20, step: 62660, training_loss: 1.22076
Epoch: 2/20, step: 62680, training_loss: 2.43852
Epoch: 2/20, step: 62700, training_loss: 2.01225
Epoch: 2/20, step: 62720, training_loss: 2.14744
Epoch: 2/20, step: 62740, training_loss: 2.69962
Epoch: 2/20, step: 62760, training_loss: 2.63243
Epoch: 2/20, step: 62780, training_loss: 2.23259
Epoch: 2/20, step: 62800, training_loss: 2.46657
Epoch: 2/20, step: 62820, training_loss: 2.60901
Epoch: 2/20, step: 62840, training_loss: 2.55733
Epoch: 2/20, step: 62860, training_loss: 2.21612
Epoch: 2/20, step: 62880, training_loss: 2.43688
Epoch: 2/20, step: 62900, training_loss: 2.03897
Epoch: 2/20, step: 62920, training_loss: 2.48400
Epoch: 2/20, step: 62940, training_loss: 2.93027
Epoch: 2/20, step: 62960, training_loss: 1.24707
Epoch: 2/20, step: 62980, training_loss: 2.47256
Epoch: 2/20, step: 63000, training_loss: 1.80546
accuracy: 0.38, validation_loss: 2.2932024002075195, num_samples: 100
Epoch: 2/20, step: 63020, training_loss: 2.07647
Epoch: 2/20, step: 63040, training_loss: 1.42369
Epoch: 2/20, step: 63060, training_loss: 1.83769
Epoch: 2/20, step: 63080, training_loss: 2.64860
Epoch: 2/20, step: 63100, training_loss: 2.43177
Epoch: 2/20, step: 63120, training_loss: 2.40053
Epoch: 2/20, step: 63140, training_loss: 2.31424
Epoch: 2/20, step: 63160, training_loss: 1.96170
Epoch: 2/20, step: 63180, training_loss: 2.37150
Epoch: 2/20, step: 63200, training_loss: 2.52062
Epoch: 2/20, step: 63220, training_loss: 1.75279
Epoch: 2/20, step: 63240, training_loss: 1.91110
Epoch: 2/20, step: 63260, training_loss: 1.45455
Epoch: 2/20, step: 63280, training_loss: 2.62684
Epoch: 2/20, step: 63300, training_loss: 2.07943
Epoch: 2/20, step: 63320, training_loss: 2.29522
Epoch: 2/20, step: 63340, training_loss: 1.59350
Epoch: 2/20, step: 63360, training_loss: 2.50622
Epoch: 2/20, step: 63380, training_loss: 1.67689
Epoch: 2/20, step: 63400, training_loss: 2.53822
Epoch: 2/20, step: 63420, training_loss: 2.21317
Epoch: 2/20, step: 63440, training_loss: 2.19009
Epoch: 2/20, step: 63460, training_loss: 2.08655
Epoch: 2/20, step: 63480, training_loss: 2.03553
Epoch: 2/20, step: 63500, training_loss: 2.31130
Epoch: 2/20, step: 63520, training_loss: 2.87370
Epoch: 2/20, step: 63540, training_loss: 2.77152
Epoch: 2/20, step: 63560, training_loss: 1.26821
Epoch: 2/20, step: 63580, training_loss: 1.99717
Epoch: 2/20, step: 63600, training_loss: 2.53427
Epoch: 2/20, step: 63620, training_loss: 2.02501
Epoch: 2/20, step: 63640, training_loss: 2.13253
Epoch: 2/20, step: 63660, training_loss: 1.86561
Epoch: 2/20, step: 63680, training_loss: 1.85178
Epoch: 2/20, step: 63700, training_loss: 2.85444
Epoch: 2/20, step: 63720, training_loss: 1.35664
Epoch: 2/20, step: 63740, training_loss: 1.74746
Epoch: 2/20, step: 63760, training_loss: 2.25062
Epoch: 2/20, step: 63780, training_loss: 3.00082
Epoch: 2/20, step: 63800, training_loss: 2.59024
Epoch: 2/20, step: 63820, training_loss: 2.39976
Epoch: 2/20, step: 63840, training_loss: 1.47828
Epoch: 2/20, step: 63860, training_loss: 2.71388
Epoch: 2/20, step: 63880, training_loss: 1.75300
Epoch: 2/20, step: 63900, training_loss: 1.70673
Epoch: 2/20, step: 63920, training_loss: 1.91276
Epoch: 2/20, step: 63940, training_loss: 1.83196
Epoch: 2/20, step: 63960, training_loss: 1.66937
Epoch: 2/20, step: 63980, training_loss: 2.67567
Epoch: 2/20, step: 64000, training_loss: 1.42755
accuracy: 0.49, validation_loss: 1.971505880355835, num_samples: 100
Epoch: 2/20, step: 64020, training_loss: 1.84685
Epoch: 2/20, step: 64040, training_loss: 1.82808
Epoch: 2/20, step: 64060, training_loss: 2.01341
Epoch: 2/20, step: 64080, training_loss: 2.06371
Epoch: 2/20, step: 64100, training_loss: 2.34468
Epoch: 2/20, step: 64120, training_loss: 2.26667
Epoch: 2/20, step: 64140, training_loss: 2.41288
Epoch: 2/20, step: 64160, training_loss: 2.24070
Epoch: 2/20, step: 64180, training_loss: 2.87868
Epoch: 2/20, step: 64200, training_loss: 2.26496
Epoch: 2/20, step: 64220, training_loss: 1.40633
Epoch: 2/20, step: 64240, training_loss: 1.89076
Epoch: 2/20, step: 64260, training_loss: 2.45888
Epoch: 2/20, step: 64280, training_loss: 2.19156
Epoch: 2/20, step: 64300, training_loss: 2.11459
Epoch: 2/20, step: 64320, training_loss: 1.35273
Epoch: 2/20, step: 64340, training_loss: 2.82446
Epoch: 2/20, step: 64360, training_loss: 1.71214
Epoch: 2/20, step: 64380, training_loss: 2.19953
Epoch: 2/20, step: 64400, training_loss: 1.78583
Epoch: 2/20, step: 64420, training_loss: 1.62508
Epoch: 2/20, step: 64440, training_loss: 3.21052
Epoch: 2/20, step: 64460, training_loss: 1.95617
Epoch: 2/20, step: 64480, training_loss: 2.76295
Epoch: 2/20, step: 64500, training_loss: 2.13812
Epoch: 2/20, step: 64520, training_loss: 1.63786
Epoch: 2/20, step: 64540, training_loss: 2.10794
Epoch: 2/20, step: 64560, training_loss: 2.28447
Epoch: 2/20, step: 64580, training_loss: 1.22621
Epoch: 2/20, step: 64600, training_loss: 2.07525
Epoch: 2/20, step: 64620, training_loss: 1.92749
Epoch: 2/20, step: 64640, training_loss: 2.48452
Epoch: 2/20, step: 64660, training_loss: 2.02553
Epoch: 2/20, step: 64680, training_loss: 1.94857
Epoch: 2/20, step: 64700, training_loss: 2.18596
Epoch: 2/20, step: 64720, training_loss: 2.83249
Epoch: 2/20, step: 64740, training_loss: 2.44270
Epoch: 2/20, step: 64760, training_loss: 2.53191
Epoch: 2/20, step: 64780, training_loss: 2.37500
Epoch: 2/20, step: 64800, training_loss: 1.59023
Epoch: 2/20, step: 64820, training_loss: 2.03407
Epoch: 2/20, step: 64840, training_loss: 1.81376
Epoch: 2/20, step: 64860, training_loss: 2.19123
Epoch: 2/20, step: 64880, training_loss: 2.10005
Epoch: 2/20, step: 64900, training_loss: 1.82891
Epoch: 2/20, step: 64920, training_loss: 1.64472
Epoch: 2/20, step: 64940, training_loss: 2.67521
Epoch: 2/20, step: 64960, training_loss: 2.14805
Epoch: 2/20, step: 64980, training_loss: 1.88765
Epoch: 2/20, step: 65000, training_loss: 2.17290
accuracy: 0.45, validation_loss: 2.1591386795043945, num_samples: 100
Epoch: 2/20, step: 65020, training_loss: 2.14392
Epoch: 2/20, step: 65040, training_loss: 2.78710
Epoch: 2/20, step: 65060, training_loss: 2.24244
Epoch: 2/20, step: 65080, training_loss: 2.21127
Epoch: 2/20, step: 65100, training_loss: 2.54799
Epoch: 2/20, step: 65120, training_loss: 2.08068
Epoch: 2/20, step: 65140, training_loss: 1.98303
Epoch: 2/20, step: 65160, training_loss: 2.19389
Epoch: 2/20, step: 65180, training_loss: 2.62732
Epoch: 2/20, step: 65200, training_loss: 2.54445
Epoch: 2/20, step: 65220, training_loss: 1.66453
Epoch: 2/20, step: 65240, training_loss: 2.19000
Epoch: 2/20, step: 65260, training_loss: 1.83854
Epoch: 2/20, step: 65280, training_loss: 2.13026
Epoch: 2/20, step: 65300, training_loss: 2.33114
Epoch: 2/20, step: 65320, training_loss: 2.59331
Epoch: 2/20, step: 65340, training_loss: 2.78694
Epoch: 2/20, step: 65360, training_loss: 2.32405
Epoch: 2/20, step: 65380, training_loss: 1.68554
Epoch: 2/20, step: 65400, training_loss: 2.92423
Epoch: 2/20, step: 65420, training_loss: 2.14718
Epoch: 2/20, step: 65440, training_loss: 2.91938
Epoch: 2/20, step: 65460, training_loss: 1.56487
Epoch: 2/20, step: 65480, training_loss: 2.28087
Epoch: 2/20, step: 65500, training_loss: 2.60556
Epoch: 2/20, step: 65520, training_loss: 2.43863
Epoch: 2/20, step: 65540, training_loss: 2.80658
Epoch: 2/20, step: 65560, training_loss: 2.53810
Epoch: 2/20, step: 65580, training_loss: 2.30059
Epoch: 2/20, step: 65600, training_loss: 1.70857
Epoch: 2/20, step: 65620, training_loss: 2.26455
Epoch: 2/20, step: 65640, training_loss: 2.71838
Epoch: 2/20, step: 65660, training_loss: 2.27009
Epoch: 2/20, step: 65680, training_loss: 2.91623
Epoch: 2/20, step: 65700, training_loss: 2.30147
Epoch: 2/20, step: 65720, training_loss: 1.69756
Epoch: 2/20, step: 65740, training_loss: 2.50161
Epoch: 2/20, step: 65760, training_loss: 2.47619
Epoch: 2/20, step: 65780, training_loss: 2.89183
Epoch: 2/20, step: 65800, training_loss: 1.77930
Epoch: 2/20, step: 65820, training_loss: 1.83235
Epoch: 2/20, step: 65840, training_loss: 1.44380
Epoch: 2/20, step: 65860, training_loss: 2.50959
Epoch: 2/20, step: 65880, training_loss: 2.44381
Epoch: 2/20, step: 65900, training_loss: 2.13402
Epoch: 2/20, step: 65920, training_loss: 1.73889
Epoch: 2/20, step: 65940, training_loss: 1.79483
Epoch: 2/20, step: 65960, training_loss: 2.81260
Epoch: 2/20, step: 65980, training_loss: 1.99677
Epoch: 2/20, step: 66000, training_loss: 1.60325
accuracy: 0.4, validation_loss: 2.1660499572753906, num_samples: 100
Epoch: 2/20, step: 66020, training_loss: 2.13703
Epoch: 2/20, step: 66040, training_loss: 1.89152
Epoch: 2/20, step: 66060, training_loss: 2.32649
Epoch: 2/20, step: 66080, training_loss: 2.10763
Epoch: 2/20, step: 66100, training_loss: 1.67669
Epoch: 2/20, step: 66120, training_loss: 1.59058
Epoch: 2/20, step: 66140, training_loss: 3.17666
Epoch: 2/20, step: 66160, training_loss: 2.27412
Epoch: 2/20, step: 66180, training_loss: 2.78627
Epoch: 2/20, step: 66200, training_loss: 2.46480
Epoch: 2/20, step: 66220, training_loss: 2.72739
Epoch: 2/20, step: 66240, training_loss: 1.89826
Epoch: 2/20, step: 66260, training_loss: 1.04536
Epoch: 2/20, step: 66280, training_loss: 1.52990
Epoch: 2/20, step: 66300, training_loss: 2.77413
Epoch: 2/20, step: 66320, training_loss: 2.76703
Epoch: 2/20, step: 66340, training_loss: 1.68222
Epoch: 2/20, step: 66360, training_loss: 2.65967
Epoch: 2/20, step: 66380, training_loss: 2.02184
Epoch: 2/20, step: 66400, training_loss: 2.57587
Epoch: 2/20, step: 66420, training_loss: 2.37836
Epoch: 2/20, step: 66440, training_loss: 2.41400
Epoch: 2/20, step: 66460, training_loss: 1.79033
Epoch: 2/20, step: 66480, training_loss: 2.04665
Epoch: 2/20, step: 66500, training_loss: 1.75222
Epoch: 2/20, step: 66520, training_loss: 1.77028
Epoch: 2/20, step: 66540, training_loss: 2.97856
Epoch: 2/20, step: 66560, training_loss: 2.09874
Epoch: 2/20, step: 66580, training_loss: 2.05694
Epoch: 2/20, step: 66600, training_loss: 2.59288
Epoch: 2/20, step: 66620, training_loss: 2.35707
Epoch: 2/20, step: 66640, training_loss: 1.64887
Epoch: 2/20, step: 66660, training_loss: 2.34586
Epoch: 2/20, step: 66680, training_loss: 2.68549
Epoch: 2/20, step: 66700, training_loss: 2.09730
Epoch: 2/20, step: 66720, training_loss: 2.53076
Epoch: 2/20, step: 66740, training_loss: 2.34306
Epoch: 2/20, step: 66760, training_loss: 3.17540
Epoch: 2/20, step: 66780, training_loss: 1.77815
Epoch: 2/20, step: 66800, training_loss: 2.17317
Epoch: 2/20, step: 66820, training_loss: 3.34450
Epoch: 2/20, step: 66840, training_loss: 2.30960
Epoch: 2/20, step: 66860, training_loss: 1.65582
Epoch: 2/20, step: 66880, training_loss: 3.22942
Epoch: 2/20, step: 66900, training_loss: 2.20363
Epoch: 2/20, step: 66920, training_loss: 2.47537
Epoch: 2/20, step: 66940, training_loss: 2.05174
Epoch: 2/20, step: 66960, training_loss: 1.54838
Epoch: 2/20, step: 66980, training_loss: 2.04503
Epoch: 2/20, step: 67000, training_loss: 2.26207
accuracy: 0.4, validation_loss: 2.157595157623291, num_samples: 100
Epoch: 2/20, step: 67020, training_loss: 2.47763
Epoch: 2/20, step: 67040, training_loss: 2.03796
Epoch: 2/20, step: 67060, training_loss: 2.16914
Epoch: 2/20, step: 67080, training_loss: 3.07806
Epoch: 2/20, step: 67100, training_loss: 1.85117
Epoch: 2/20, step: 67120, training_loss: 2.20127
Epoch: 2/20, step: 67140, training_loss: 3.00078
Epoch: 2/20, step: 67160, training_loss: 1.94981
Epoch: 2/20, step: 67180, training_loss: 2.10324
Epoch: 2/20, step: 67200, training_loss: 2.49110
Epoch: 2/20, step: 67220, training_loss: 1.73177
Epoch: 2/20, step: 67240, training_loss: 1.78354
Epoch: 2/20, step: 67260, training_loss: 2.08984
Epoch: 2/20, step: 67280, training_loss: 1.42207
Epoch: 2/20, step: 67300, training_loss: 1.71077
Epoch: 2/20, step: 67320, training_loss: 2.96633
Epoch: 2/20, step: 67340, training_loss: 1.98438
Epoch: 2/20, step: 67360, training_loss: 2.00370
Epoch: 2/20, step: 67380, training_loss: 2.53964
Epoch: 2/20, step: 67400, training_loss: 1.98201
Epoch: 2/20, step: 67420, training_loss: 1.43280
Epoch: 2/20, step: 67440, training_loss: 2.93424
Epoch: 2/20, step: 67460, training_loss: 1.89720
Epoch: 2/20, step: 67480, training_loss: 1.72604
Epoch: 2/20, step: 67500, training_loss: 3.07343
Epoch: 2/20, step: 67520, training_loss: 3.03703
Epoch: 2/20, step: 67540, training_loss: 1.53856
Epoch: 2/20, step: 67560, training_loss: 1.62794
Epoch: 2/20, step: 67580, training_loss: 3.20798
Epoch: 2/20, step: 67600, training_loss: 1.96719
Epoch: 2/20, step: 67620, training_loss: 1.81265
Epoch: 2/20, step: 67640, training_loss: 2.26018
Epoch: 2/20, step: 67660, training_loss: 1.97684
Epoch: 2/20, step: 67680, training_loss: 2.56785
Epoch: 2/20, step: 67700, training_loss: 2.60379
Epoch: 2/20, step: 67720, training_loss: 2.38784
Epoch: 2/20, step: 67740, training_loss: 2.25880
Epoch: 2/20, step: 67760, training_loss: 1.74817
Epoch: 2/20, step: 67780, training_loss: 1.91682
Epoch: 2/20, step: 67800, training_loss: 2.92389
Epoch: 2/20, step: 67820, training_loss: 3.04976
Epoch: 2/20, step: 67840, training_loss: 2.02420
Epoch: 2/20, step: 67860, training_loss: 1.90707
Epoch: 2/20, step: 67880, training_loss: 1.88492
Epoch: 2/20, step: 67900, training_loss: 1.62969
Epoch: 2/20, step: 67920, training_loss: 2.22495
Epoch: 2/20, step: 67940, training_loss: 2.96800
Epoch: 2/20, step: 67960, training_loss: 2.47138
Epoch: 2/20, step: 67980, training_loss: 2.02093
Epoch: 2/20, step: 68000, training_loss: 1.91721
accuracy: 0.41, validation_loss: 2.251354455947876, num_samples: 100
Epoch: 2/20, step: 68020, training_loss: 1.77023
Epoch: 2/20, step: 68040, training_loss: 1.91462
Epoch: 2/20, step: 68060, training_loss: 2.15074
Epoch: 2/20, step: 68080, training_loss: 2.72234
Epoch: 2/20, step: 68100, training_loss: 2.37527
Epoch: 2/20, step: 68120, training_loss: 1.65375
Epoch: 2/20, step: 68140, training_loss: 1.76682
Epoch: 2/20, step: 68160, training_loss: 1.46486
Epoch: 2/20, step: 68180, training_loss: 1.68442
Epoch: 2/20, step: 68200, training_loss: 2.46136
Epoch: 2/20, step: 68220, training_loss: 2.84836
Epoch: 2/20, step: 68240, training_loss: 2.04091
Epoch: 2/20, step: 68260, training_loss: 2.15547
Epoch: 2/20, step: 68280, training_loss: 2.44206
Epoch: 2/20, step: 68300, training_loss: 2.01198
Epoch: 2/20, step: 68320, training_loss: 2.35588
Epoch: 2/20, step: 68340, training_loss: 2.89184
Epoch: 2/20, step: 68360, training_loss: 2.19470
Epoch: 2/20, step: 68380, training_loss: 2.17148
Epoch: 2/20, step: 68400, training_loss: 1.65213
Epoch: 2/20, step: 68420, training_loss: 1.89738
Epoch: 2/20, step: 68440, training_loss: 2.30003
Epoch: 2/20, step: 68460, training_loss: 2.12836
Epoch: 2/20, step: 68480, training_loss: 2.94486
Epoch: 2/20, step: 68500, training_loss: 2.18729
Epoch: 2/20, step: 68520, training_loss: 2.89084
Epoch: 2/20, step: 68540, training_loss: 2.80410
Epoch: 2/20, step: 68560, training_loss: 1.54347
Epoch: 2/20, step: 68580, training_loss: 3.09674
Epoch: 2/20, step: 68600, training_loss: 1.86052
Epoch: 2/20, step: 68620, training_loss: 1.28974
Epoch: 2/20, step: 68640, training_loss: 1.76851
Epoch: 2/20, step: 68660, training_loss: 1.32495
Epoch: 2/20, step: 68680, training_loss: 1.30785
Epoch: 2/20, step: 68700, training_loss: 1.93471
Epoch: 2/20, step: 68720, training_loss: 1.90709
Epoch: 2/20, step: 68740, training_loss: 0.95999
Epoch: 2/20, step: 68760, training_loss: 1.94447
Epoch: 2/20, step: 68780, training_loss: 2.02921
Epoch: 2/20, step: 68800, training_loss: 1.45310
Epoch: 2/20, step: 68820, training_loss: 1.88916
Epoch: 2/20, step: 68840, training_loss: 1.91855
Epoch: 2/20, step: 68860, training_loss: 1.69923
Epoch: 2/20, step: 68880, training_loss: 2.54866
Epoch: 2/20, step: 68900, training_loss: 1.67126
Epoch: 2/20, step: 68920, training_loss: 2.02564
Epoch: 2/20, step: 68940, training_loss: 2.13424
Epoch: 2/20, step: 68960, training_loss: 2.01818
Epoch: 2/20, step: 68980, training_loss: 2.42236
Epoch: 2/20, step: 69000, training_loss: 1.84558
accuracy: 0.36, validation_loss: 2.33174204826355, num_samples: 100
Epoch: 2/20, step: 69020, training_loss: 1.73539
Epoch: 2/20, step: 69040, training_loss: 3.14688
Epoch: 2/20, step: 69060, training_loss: 1.80634
Epoch: 2/20, step: 69080, training_loss: 2.88319
Epoch: 2/20, step: 69100, training_loss: 1.43630
Epoch: 2/20, step: 69120, training_loss: 2.88601
Epoch: 2/20, step: 69140, training_loss: 3.07923
Epoch: 2/20, step: 69160, training_loss: 1.75502
Epoch: 2/20, step: 69180, training_loss: 2.18388
Epoch: 2/20, step: 69200, training_loss: 1.77780
Epoch: 2/20, step: 69220, training_loss: 2.61226
Epoch: 2/20, step: 69240, training_loss: 1.79060
Epoch: 2/20, step: 69260, training_loss: 2.25721
Epoch: 2/20, step: 69280, training_loss: 2.26134
Epoch: 2/20, step: 69300, training_loss: 2.08096
Epoch: 2/20, step: 69320, training_loss: 1.81923
Epoch: 2/20, step: 69340, training_loss: 1.81439
Epoch: 2/20, step: 69360, training_loss: 2.36414
Epoch: 2/20, step: 69380, training_loss: 2.16899
Epoch: 2/20, step: 69400, training_loss: 2.49336
Epoch: 2/20, step: 69420, training_loss: 2.20696
Epoch: 2/20, step: 69440, training_loss: 2.55761
Epoch: 2/20, step: 69460, training_loss: 1.30589
Epoch: 2/20, step: 69480, training_loss: 2.02742
Epoch: 2/20, step: 69500, training_loss: 2.87840
Epoch: 2/20, step: 69520, training_loss: 2.64180
Epoch: 2/20, step: 69540, training_loss: 1.67971
Epoch: 2/20, step: 69560, training_loss: 2.01128
Epoch: 2/20, step: 69580, training_loss: 2.01085
Epoch: 2/20, step: 69600, training_loss: 1.67254
Epoch: 2/20, step: 69620, training_loss: 2.34381
Epoch: 2/20, step: 69640, training_loss: 1.40594
Epoch: 2/20, step: 69660, training_loss: 2.40425
Epoch: 2/20, step: 69680, training_loss: 2.56929
Epoch: 2/20, step: 69700, training_loss: 1.24192
Epoch: 2/20, step: 69720, training_loss: 1.96633
Epoch: 2/20, step: 69740, training_loss: 1.36742
Epoch: 2/20, step: 69760, training_loss: 2.27141
Epoch: 2/20, step: 69780, training_loss: 2.17502
Epoch: 2/20, step: 69800, training_loss: 2.59224
Epoch: 2/20, step: 69820, training_loss: 2.37987
Epoch: 2/20, step: 69840, training_loss: 3.11544
Epoch: 2/20, step: 69860, training_loss: 1.53359
Epoch: 2/20, step: 69880, training_loss: 2.07460
Epoch: 2/20, step: 69900, training_loss: 1.84633
Epoch: 2/20, step: 69920, training_loss: 2.40333
Epoch: 2/20, step: 69940, training_loss: 1.49079
Epoch: 2/20, step: 69960, training_loss: 1.54342
Epoch: 2/20, step: 69980, training_loss: 1.85135
Epoch: 2/20, step: 70000, training_loss: 2.74686
accuracy: 0.46, validation_loss: 1.7751069068908691, num_samples: 100
Epoch: 2/20, step: 70020, training_loss: 2.62049
Epoch: 2/20, step: 70040, training_loss: 2.25468
Epoch: 2/20, step: 70060, training_loss: 2.51797
Epoch: 2/20, step: 70080, training_loss: 2.01381
Epoch: 2/20, step: 70100, training_loss: 2.68367
Epoch: 2/20, step: 70120, training_loss: 2.28552
Epoch: 2/20, step: 70140, training_loss: 1.69864
Epoch: 2/20, step: 70160, training_loss: 2.21744
Epoch: 2/20, step: 70180, training_loss: 2.55815
Epoch: 2/20, step: 70200, training_loss: 2.17169
Epoch: 2/20, step: 70220, training_loss: 2.28302
Epoch: 2/20, step: 70240, training_loss: 1.76512
Epoch: 2/20, step: 70260, training_loss: 2.25233
Epoch: 2/20, step: 70280, training_loss: 1.82042
Epoch: 2/20, step: 70300, training_loss: 1.77265
Epoch: 2/20, step: 70320, training_loss: 2.45310
Epoch: 2/20, step: 70340, training_loss: 1.93159
Epoch: 2/20, step: 70360, training_loss: 3.00611
Epoch: 2/20, step: 70380, training_loss: 2.76053
Epoch: 2/20, step: 70400, training_loss: 2.51951
Epoch: 2/20, step: 70420, training_loss: 2.77346
Epoch: 2/20, step: 70440, training_loss: 2.98375
Epoch: 2/20, step: 70460, training_loss: 1.47325
Epoch: 2/20, step: 70480, training_loss: 2.37516
Epoch: 2/20, step: 70500, training_loss: 1.02194
Epoch: 2/20, step: 70520, training_loss: 1.64204
Epoch: 2/20, step: 70540, training_loss: 2.39855
Epoch: 2/20, step: 70560, training_loss: 2.14890
Epoch: 2/20, step: 70580, training_loss: 2.73161
Epoch: 2/20, step: 70600, training_loss: 3.01392
Epoch: 2/20, step: 70620, training_loss: 1.82520
Epoch: 2/20, step: 70640, training_loss: 2.81660
Epoch: 2/20, step: 70660, training_loss: 2.54864
Epoch: 2/20, step: 70680, training_loss: 2.15106
Epoch: 2/20, step: 70700, training_loss: 1.81851
Epoch: 2/20, step: 70720, training_loss: 2.50813
Epoch: 2/20, step: 70740, training_loss: 2.00267
Epoch: 2/20, step: 70760, training_loss: 2.47941
Epoch: 2/20, step: 70780, training_loss: 1.70422
Epoch: 2/20, step: 70800, training_loss: 1.69476
Epoch: 2/20, step: 70820, training_loss: 1.80686
Epoch: 2/20, step: 70840, training_loss: 2.57748
Epoch: 2/20, step: 70860, training_loss: 2.34449
Epoch: 2/20, step: 70880, training_loss: 2.25105
Epoch: 2/20, step: 70900, training_loss: 1.40063
Epoch: 2/20, step: 70920, training_loss: 1.83255
Epoch: 2/20, step: 70940, training_loss: 2.37853
Epoch: 2/20, step: 70960, training_loss: 1.83045
Epoch: 2/20, step: 70980, training_loss: 1.74151
Epoch: 2/20, step: 71000, training_loss: 2.26586
accuracy: 0.49, validation_loss: 2.2121164798736572, num_samples: 100
Epoch: 2/20, step: 71020, training_loss: 2.41799
Epoch: 2/20, step: 71040, training_loss: 1.66616
Epoch: 2/20, step: 71060, training_loss: 1.89102
Epoch: 2/20, step: 71080, training_loss: 1.78087
Epoch: 2/20, step: 71100, training_loss: 1.98828
Epoch: 2/20, step: 71120, training_loss: 2.21468
Epoch: 2/20, step: 71140, training_loss: 1.71355
Epoch: 2/20, step: 71160, training_loss: 2.37470
Epoch: 2/20, step: 71180, training_loss: 1.87621
Epoch: 2/20, step: 71200, training_loss: 2.23368
Epoch: 2/20, step: 71220, training_loss: 1.79213
Epoch: 2/20, step: 71240, training_loss: 2.03700
Epoch: 2/20, step: 71260, training_loss: 2.04616
Epoch: 2/20, step: 71280, training_loss: 2.41272
Epoch: 2/20, step: 71300, training_loss: 2.33268
Epoch: 2/20, step: 71320, training_loss: 2.65801
Epoch: 2/20, step: 71340, training_loss: 1.80532
Epoch: 2/20, step: 71360, training_loss: 2.00559
Epoch: 2/20, step: 71380, training_loss: 2.27792
Epoch: 2/20, step: 71400, training_loss: 2.14169
Epoch: 2/20, step: 71420, training_loss: 2.16915
Epoch: 2/20, step: 71440, training_loss: 2.26687
Epoch: 2/20, step: 71460, training_loss: 1.73656
Epoch: 2/20, step: 71480, training_loss: 1.94028
Epoch: 2/20, step: 71500, training_loss: 2.28766
Epoch: 2/20, step: 71520, training_loss: 2.23607
Epoch: 2/20, step: 71540, training_loss: 1.58675
Epoch: 2/20, step: 71560, training_loss: 2.53631
Epoch: 2/20, step: 71580, training_loss: 2.19254
Epoch: 2/20, step: 71600, training_loss: 1.91841
Epoch: 2/20, step: 71620, training_loss: 2.04104
Epoch: 2/20, step: 71640, training_loss: 1.62462
Epoch: 2/20, step: 71660, training_loss: 1.95664
Epoch: 2/20, step: 71680, training_loss: 2.63180
Epoch: 2/20, step: 71700, training_loss: 1.65784
Epoch: 2/20, step: 71720, training_loss: 2.64474
Epoch: 2/20, step: 71740, training_loss: 2.30872
Epoch: 2/20, step: 71760, training_loss: 2.39050
Epoch: 2/20, step: 71780, training_loss: 2.23054
Epoch: 2/20, step: 71800, training_loss: 2.23801
Epoch: 2/20, step: 71820, training_loss: 2.87788
Epoch: 2/20, step: 71840, training_loss: 1.92864
Epoch: 2/20, step: 71860, training_loss: 2.17135
Epoch: 2/20, step: 71880, training_loss: 2.23116
Epoch: 2/20, step: 71900, training_loss: 2.24701
Epoch: 2/20, step: 71920, training_loss: 2.98902
Epoch: 2/20, step: 71940, training_loss: 1.66579
Epoch: 2/20, step: 71960, training_loss: 2.92113
Epoch: 2/20, step: 71980, training_loss: 1.54947
Epoch: 2/20, step: 72000, training_loss: 2.47609
accuracy: 0.38, validation_loss: 2.0239250659942627, num_samples: 100
Epoch: 2/20, step: 72020, training_loss: 2.39965
Epoch: 2/20, step: 72040, training_loss: 1.84175
Epoch: 2/20, step: 72060, training_loss: 1.28466
Epoch: 2/20, step: 72080, training_loss: 2.21924
Epoch: 2/20, step: 72100, training_loss: 2.67203
Epoch: 2/20, step: 72120, training_loss: 3.05983
Epoch: 2/20, step: 72140, training_loss: 1.53456
Epoch: 2/20, step: 72160, training_loss: 2.23733
Epoch: 2/20, step: 72180, training_loss: 2.72889
Epoch: 2/20, step: 72200, training_loss: 1.84281
Epoch: 2/20, step: 72220, training_loss: 1.47395
Epoch: 2/20, step: 72240, training_loss: 2.05268
Epoch: 2/20, step: 72260, training_loss: 1.88726
Epoch: 2/20, step: 72280, training_loss: 2.39824
Epoch: 2/20, step: 72300, training_loss: 1.85358
Epoch: 2/20, step: 72320, training_loss: 1.90295
Epoch: 2/20, step: 72340, training_loss: 2.29317
Epoch: 2/20, step: 72360, training_loss: 1.91174
Epoch: 2/20, step: 72380, training_loss: 2.57217
Epoch: 2/20, step: 72400, training_loss: 1.77795
Epoch: 2/20, step: 72420, training_loss: 1.98118
Epoch: 2/20, step: 72440, training_loss: 2.17636
Epoch: 2/20, step: 72460, training_loss: 1.85596
Epoch: 2/20, step: 72480, training_loss: 2.27346
Epoch: 2/20, step: 72500, training_loss: 1.61453
Epoch: 2/20, step: 72520, training_loss: 2.54191
Epoch: 2/20, step: 72540, training_loss: 2.11205
Epoch: 2/20, step: 72560, training_loss: 1.55002
Epoch: 2/20, step: 72580, training_loss: 2.15684
Epoch: 2/20, step: 72600, training_loss: 1.63514
Epoch: 2/20, step: 72620, training_loss: 2.31077
Epoch: 2/20, step: 72640, training_loss: 1.57918
Epoch: 2/20, step: 72660, training_loss: 2.27233
Epoch: 2/20, step: 72680, training_loss: 2.33166
Epoch: 2/20, step: 72700, training_loss: 3.38318
Epoch: 2/20, step: 72720, training_loss: 1.81689
Epoch: 2/20, step: 72740, training_loss: 2.23255
Epoch: 2/20, step: 72760, training_loss: 1.93959
Epoch: 2/20, step: 72780, training_loss: 1.94025
Epoch: 2/20, step: 72800, training_loss: 3.34254
Epoch: 2/20, step: 72820, training_loss: 1.79027
Epoch: 2/20, step: 72840, training_loss: 2.03117
Epoch: 2/20, step: 72860, training_loss: 1.97799
Epoch: 2/20, step: 72880, training_loss: 1.97185
Epoch: 2/20, step: 72900, training_loss: 2.31484
Epoch: 2/20, step: 72920, training_loss: 1.89609
Epoch: 2/20, step: 72940, training_loss: 2.52567
Epoch: 2/20, step: 72960, training_loss: 2.21321
Epoch: 2/20, step: 72980, training_loss: 2.00656
Epoch: 2/20, step: 73000, training_loss: 2.75709
accuracy: 0.36, validation_loss: 2.2111847400665283, num_samples: 100
Epoch: 2/20, step: 73020, training_loss: 2.37989
Epoch: 2/20, step: 73040, training_loss: 3.21066
Epoch: 2/20, step: 73060, training_loss: 2.78724
Epoch: 2/20, step: 73080, training_loss: 2.59898
Epoch: 2/20, step: 73100, training_loss: 1.53915
Epoch: 2/20, step: 73120, training_loss: 1.74063
Epoch: 2/20, step: 73140, training_loss: 2.68854
Epoch: 2/20, step: 73160, training_loss: 2.54202
Epoch: 2/20, step: 73180, training_loss: 1.90776
Epoch: 2/20, step: 73200, training_loss: 1.64417
Epoch: 2/20, step: 73220, training_loss: 2.27883
Epoch: 2/20, step: 73240, training_loss: 2.02944
Epoch: 2/20, step: 73260, training_loss: 2.40660
Epoch: 2/20, step: 73280, training_loss: 1.99540
Epoch: 2/20, step: 73300, training_loss: 1.15896
Epoch: 2/20, step: 73320, training_loss: 1.76723
Epoch: 2/20, step: 73340, training_loss: 3.10681
Epoch: 2/20, step: 73360, training_loss: 2.28378
Epoch: 2/20, step: 73380, training_loss: 2.41593
Epoch: 2/20, step: 73400, training_loss: 2.59183
Epoch: 2/20, step: 73420, training_loss: 3.24445
Epoch: 2/20, step: 73440, training_loss: 2.81705
Epoch: 2/20, step: 73460, training_loss: 1.57130
Epoch: 2/20, step: 73480, training_loss: 1.59878
Epoch: 2/20, step: 73500, training_loss: 2.86355
Epoch: 2/20, step: 73520, training_loss: 2.26415
Epoch: 2/20, step: 73540, training_loss: 1.73953
Epoch: 2/20, step: 73560, training_loss: 1.68637
Epoch: 2/20, step: 73580, training_loss: 1.72979
Epoch: 2/20, step: 73600, training_loss: 1.70923
Epoch: 2/20, step: 73620, training_loss: 2.69742
Epoch: 2/20, step: 73640, training_loss: 2.22449
Epoch: 2/20, step: 73660, training_loss: 2.09471
Epoch: 2/20, step: 73680, training_loss: 2.84148
Epoch: 2/20, step: 73700, training_loss: 1.92830
Epoch: 2/20, step: 73720, training_loss: 1.97961
Epoch: 2/20, step: 73740, training_loss: 1.94550
Epoch: 2/20, step: 73760, training_loss: 1.64991
Epoch: 2/20, step: 73780, training_loss: 1.66263
Epoch: 2/20, step: 73800, training_loss: 2.35117
Epoch: 2/20, step: 73820, training_loss: 2.42826
Epoch: 2/20, step: 73840, training_loss: 1.80085
Epoch: 2/20, step: 73860, training_loss: 2.12924
Epoch: 2/20, step: 73880, training_loss: 1.92575
Epoch: 2/20, step: 73900, training_loss: 1.55892
Epoch: 2/20, step: 73920, training_loss: 1.64851
Epoch: 2/20, step: 73940, training_loss: 1.94247
Epoch: 2/20, step: 73960, training_loss: 1.84118
Epoch: 2/20, step: 73980, training_loss: 3.19993
Epoch: 2/20, step: 74000, training_loss: 1.90603
accuracy: 0.42, validation_loss: 1.974624752998352, num_samples: 100
Epoch: 2/20, step: 74020, training_loss: 2.37247
Epoch: 2/20, step: 74040, training_loss: 1.87458
Epoch: 2/20, step: 74060, training_loss: 2.63892
Epoch: 2/20, step: 74080, training_loss: 2.38290
Epoch: 2/20, step: 74100, training_loss: 2.29698
Epoch: 2/20, step: 74120, training_loss: 2.63756
Epoch: 2/20, step: 74140, training_loss: 2.00281
Epoch: 2/20, step: 74160, training_loss: 1.99859
Epoch: 2/20, step: 74180, training_loss: 2.33244
Epoch: 2/20, step: 74200, training_loss: 1.61217
Epoch: 2/20, step: 74220, training_loss: 1.99232
Epoch: 2/20, step: 74240, training_loss: 2.40908
Epoch: 2/20, step: 74260, training_loss: 2.04148
Epoch: 2/20, step: 74280, training_loss: 1.65439
Epoch: 2/20, step: 74300, training_loss: 2.08708
Epoch: 2/20, step: 74320, training_loss: 2.33836
Epoch: 2/20, step: 74340, training_loss: 2.51527
Epoch: 2/20, step: 74360, training_loss: 2.88940
Epoch: 2/20, step: 74380, training_loss: 2.15538
Epoch: 2/20, step: 74400, training_loss: 2.05671
Epoch: 2/20, step: 74420, training_loss: 2.21779
Epoch: 2/20, step: 74440, training_loss: 2.21607
Epoch: 2/20, step: 74460, training_loss: 2.43554
Epoch: 2/20, step: 74480, training_loss: 2.10717
Epoch: 2/20, step: 74500, training_loss: 2.09412
Epoch: 2/20, step: 74520, training_loss: 1.45889
Epoch: 2/20, step: 74540, training_loss: 1.50758
Epoch: 2/20, step: 74560, training_loss: 1.91223
Epoch: 2/20, step: 74580, training_loss: 2.20546
Epoch: 2/20, step: 74600, training_loss: 1.74718
Epoch: 2/20, step: 74620, training_loss: 1.79369
Epoch: 2/20, step: 74640, training_loss: 1.89466
Epoch: 2/20, step: 74660, training_loss: 1.93554
Epoch: 2/20, step: 74680, training_loss: 2.14121
Epoch: 2/20, step: 74700, training_loss: 2.09533
Epoch: 2/20, step: 74720, training_loss: 2.06580
Epoch: 2/20, step: 74740, training_loss: 2.12974
Epoch: 2/20, step: 74760, training_loss: 2.33605
Epoch: 2/20, step: 74780, training_loss: 2.58775
Epoch: 2/20, step: 74800, training_loss: 1.32548
Epoch: 2/20, step: 74820, training_loss: 2.25567
Epoch: 2/20, step: 74840, training_loss: 2.01611
Epoch: 2/20, step: 74860, training_loss: 2.02861
Epoch: 2/20, step: 74880, training_loss: 1.84112
Epoch: 2/20, step: 74900, training_loss: 2.26712
Epoch: 2/20, step: 74920, training_loss: 2.17370
Epoch: 2/20, step: 74940, training_loss: 2.43213
Epoch: 2/20, step: 74960, training_loss: 2.27061
Epoch: 2/20, step: 74980, training_loss: 1.96157
Epoch: 2/20, step: 75000, training_loss: 1.57636
accuracy: 0.43, validation_loss: 2.0039615631103516, num_samples: 100
Epoch: 2/20, step: 75020, training_loss: 2.38451
Epoch: 2/20, step: 75040, training_loss: 1.16232
Epoch: 2/20, step: 75060, training_loss: 2.12383
Epoch: 2/20, step: 75080, training_loss: 2.96209
Epoch: 2/20, step: 75100, training_loss: 1.64479
Epoch: 2/20, step: 75120, training_loss: 2.50075
Epoch: 2/20, step: 75140, training_loss: 2.35591
Epoch: 2/20, step: 75160, training_loss: 1.79985
Epoch: 2/20, step: 75180, training_loss: 1.89378
Epoch: 2/20, step: 75200, training_loss: 2.23941
Epoch: 2/20, step: 75220, training_loss: 2.56806
Epoch: 2/20, step: 75240, training_loss: 1.93416
Epoch: 2/20, step: 75260, training_loss: 2.74504
Epoch: 2/20, step: 75280, training_loss: 1.88160
Epoch: 2/20, step: 75300, training_loss: 2.56203
Epoch: 2/20, step: 75320, training_loss: 2.14407
Epoch: 2/20, step: 75340, training_loss: 3.05495
Epoch: 2/20, step: 75360, training_loss: 1.78958
Epoch: 2/20, step: 75380, training_loss: 2.05118
Epoch: 2/20, step: 75400, training_loss: 1.89836
Epoch: 2/20, step: 75420, training_loss: 2.03147
Epoch: 2/20, step: 75440, training_loss: 1.96179
Epoch: 2/20, step: 75460, training_loss: 2.78444
Epoch: 2/20, step: 75480, training_loss: 2.61887
Epoch: 2/20, step: 75500, training_loss: 2.49011
Epoch: 2/20, step: 75520, training_loss: 2.51500
Epoch: 2/20, step: 75540, training_loss: 2.39273
Epoch: 2/20, step: 75560, training_loss: 1.86011
Epoch: 2/20, step: 75580, training_loss: 2.67976
Epoch: 2/20, step: 75600, training_loss: 2.21981
Epoch: 2/20, step: 75620, training_loss: 3.15719
Epoch: 2/20, step: 75640, training_loss: 2.41256
Epoch: 2/20, step: 75660, training_loss: 2.44184
Epoch: 2/20, step: 75680, training_loss: 1.35540
Epoch: 2/20, step: 75700, training_loss: 2.47875
Epoch: 2/20, step: 75720, training_loss: 2.31881
Epoch: 2/20, step: 75740, training_loss: 2.25847
Epoch: 2/20, step: 75760, training_loss: 2.18456
Epoch: 2/20, step: 75780, training_loss: 1.81470
Epoch: 2/20, step: 75800, training_loss: 1.96651
Epoch: 2/20, step: 75820, training_loss: 2.68629
Epoch: 2/20, step: 75840, training_loss: 2.35052
Epoch: 2/20, step: 75860, training_loss: 1.33497
Epoch: 2/20, step: 75880, training_loss: 1.77056
Epoch: 2/20, step: 75900, training_loss: 2.16504
Epoch: 2/20, step: 75920, training_loss: 2.19356
Epoch: 2/20, step: 75940, training_loss: 2.69397
Epoch: 2/20, step: 75960, training_loss: 2.01008
Epoch: 2/20, step: 75980, training_loss: 1.47909
Epoch: 2/20, step: 76000, training_loss: 2.22822
accuracy: 0.41, validation_loss: 2.0478153228759766, num_samples: 100
Epoch: 2/20, step: 76020, training_loss: 1.54894
Epoch: 2/20, step: 76040, training_loss: 2.38008
Epoch: 2/20, step: 76060, training_loss: 2.33865
Epoch: 2/20, step: 76080, training_loss: 2.27727
Epoch: 2/20, step: 76100, training_loss: 2.61686
Epoch: 2/20, step: 76120, training_loss: 2.19375
Epoch: 2/20, step: 76140, training_loss: 2.30562
Epoch: 2/20, step: 76160, training_loss: 2.95130
Epoch: 2/20, step: 76180, training_loss: 3.22581
Epoch: 2/20, step: 76200, training_loss: 2.03469
Epoch: 2/20, step: 76220, training_loss: 1.99628
Epoch: 2/20, step: 76240, training_loss: 2.85153
Epoch: 2/20, step: 76260, training_loss: 2.69732
Epoch: 2/20, step: 76280, training_loss: 1.51488
Epoch: 2/20, step: 76300, training_loss: 2.31948
Epoch: 2/20, step: 76320, training_loss: 1.69804
Epoch: 2/20, step: 76340, training_loss: 2.13938
Epoch: 2/20, step: 76360, training_loss: 1.92197
Epoch: 2/20, step: 76380, training_loss: 2.54573
Epoch: 2/20, step: 76400, training_loss: 1.59372
Epoch: 2/20, step: 76420, training_loss: 2.25442
Epoch: 2/20, step: 76440, training_loss: 1.73835
Epoch: 2/20, step: 76460, training_loss: 2.11177
Epoch: 2/20, step: 76480, training_loss: 1.93567
Epoch: 2/20, step: 76500, training_loss: 2.43891
Epoch: 2/20, step: 76520, training_loss: 2.19832
Epoch: 2/20, step: 76540, training_loss: 1.14008
Epoch: 2/20, step: 76560, training_loss: 2.59559
Epoch: 2/20, step: 76580, training_loss: 2.70818
Epoch: 2/20, step: 76600, training_loss: 1.94393
Epoch: 2/20, step: 76620, training_loss: 2.04069
Epoch: 2/20, step: 76640, training_loss: 1.89357
Epoch: 2/20, step: 76660, training_loss: 2.59537
Epoch: 2/20, step: 76680, training_loss: 1.91917
Epoch: 2/20, step: 76700, training_loss: 2.21756
Epoch: 2/20, step: 76720, training_loss: 1.78018
Epoch: 2/20, step: 76740, training_loss: 1.57967
Epoch: 2/20, step: 76760, training_loss: 2.24160
Epoch: 2/20, step: 76780, training_loss: 1.55524
Epoch: 2/20, step: 76800, training_loss: 3.03336
Epoch: 2/20, step: 76820, training_loss: 2.42109
Epoch: 2/20, step: 76840, training_loss: 1.87654
Epoch: 2/20, step: 76860, training_loss: 2.52119
Epoch: 2/20, step: 76880, training_loss: 1.74583
Epoch: 2/20, step: 76900, training_loss: 2.28056
Epoch: 2/20, step: 76920, training_loss: 2.34586
Epoch: 2/20, step: 76940, training_loss: 2.21756
Epoch: 2/20, step: 76960, training_loss: 1.46924
Epoch: 2/20, step: 76980, training_loss: 2.13256
Epoch: 2/20, step: 77000, training_loss: 1.87921
accuracy: 0.35, validation_loss: 2.0680904388427734, num_samples: 100
Epoch: 2/20, step: 77020, training_loss: 1.50240
Epoch: 2/20, step: 77040, training_loss: 2.98016
Epoch: 2/20, step: 77060, training_loss: 2.23274
Epoch: 2/20, step: 77080, training_loss: 1.62530
Epoch: 2/20, step: 77100, training_loss: 1.75383
Epoch: 2/20, step: 77120, training_loss: 2.33379
Epoch: 2/20, step: 77140, training_loss: 2.68420
Epoch: 2/20, step: 77160, training_loss: 2.51337
Epoch: 2/20, step: 77180, training_loss: 2.58286
Epoch: 2/20, step: 77200, training_loss: 2.19739
Epoch: 2/20, step: 77220, training_loss: 2.61450
Epoch: 2/20, step: 77240, training_loss: 1.67347
Epoch: 2/20, step: 77260, training_loss: 2.51729
Epoch: 2/20, step: 77280, training_loss: 1.39121
Epoch: 2/20, step: 77300, training_loss: 1.29038
Epoch: 2/20, step: 77320, training_loss: 1.84733
Epoch: 2/20, step: 77340, training_loss: 2.28958
Epoch: 2/20, step: 77360, training_loss: 3.26171
Epoch: 2/20, step: 77380, training_loss: 2.26117
Epoch: 2/20, step: 77400, training_loss: 1.52146
Epoch: 2/20, step: 77420, training_loss: 2.35752
Epoch: 2/20, step: 77440, training_loss: 1.31567
Epoch: 2/20, step: 77460, training_loss: 1.60450
Epoch: 2/20, step: 77480, training_loss: 1.94931
Epoch: 2/20, step: 77500, training_loss: 2.15470
Epoch: 2/20, step: 77520, training_loss: 1.27860
Epoch: 2/20, step: 77540, training_loss: 2.46947
Epoch: 2/20, step: 77560, training_loss: 1.87529
Epoch: 2/20, step: 77580, training_loss: 2.66733
Epoch: 2/20, step: 77600, training_loss: 2.06671
Epoch: 2/20, step: 77620, training_loss: 1.93253
Epoch: 2/20, step: 77640, training_loss: 2.20831
Epoch: 2/20, step: 77660, training_loss: 2.95073
Epoch: 2/20, step: 77680, training_loss: 1.20015
Epoch: 2/20, step: 77700, training_loss: 1.92487
Epoch: 2/20, step: 77720, training_loss: 1.70965
Epoch: 2/20, step: 77740, training_loss: 2.78147
Epoch: 2/20, step: 77760, training_loss: 2.13403
Epoch: 2/20, step: 77780, training_loss: 2.57429
Epoch: 2/20, step: 77800, training_loss: 2.05189
Epoch: 2/20, step: 77820, training_loss: 1.50832
Epoch: 2/20, step: 77840, training_loss: 1.43905
Epoch: 2/20, step: 77860, training_loss: 2.30796
Epoch: 2/20, step: 77880, training_loss: 2.27422
Epoch: 2/20, step: 77900, training_loss: 1.98998
Epoch: 2/20, step: 77920, training_loss: 1.91322
Epoch: 2/20, step: 77940, training_loss: 2.45769
Epoch: 2/20, step: 77960, training_loss: 1.88525
Epoch: 2/20, step: 77980, training_loss: 2.45070
Epoch: 2/20, step: 78000, training_loss: 2.34686
accuracy: 0.4, validation_loss: 2.1880104541778564, num_samples: 100
Epoch: 2/20, step: 78020, training_loss: 2.58473
Epoch: 2/20, step: 78040, training_loss: 2.03667
Epoch: 2/20, step: 78060, training_loss: 1.72555
Epoch: 2/20, step: 78080, training_loss: 3.11145
Epoch: 2/20, step: 78100, training_loss: 2.43603
Epoch: 2/20, step: 78120, training_loss: 3.09895
Epoch: 2/20, step: 78140, training_loss: 3.06884
Epoch: 2/20, step: 78160, training_loss: 1.59978
Epoch: 2/20, step: 78180, training_loss: 2.27948
Epoch: 2/20, step: 78200, training_loss: 2.99638
Epoch: 2/20, step: 78220, training_loss: 1.74931
Epoch: 2/20, step: 78240, training_loss: 1.49298
Epoch: 2/20, step: 78260, training_loss: 1.63190
Epoch: 2/20, step: 78280, training_loss: 2.48535
Epoch: 2/20, step: 78300, training_loss: 3.14370
Epoch: 2/20, step: 78320, training_loss: 2.11715
Epoch: 2/20, step: 78340, training_loss: 2.99096
Epoch: 2/20, step: 78360, training_loss: 2.21801
Epoch: 2/20, step: 78380, training_loss: 2.61863
Epoch: 2/20, step: 78400, training_loss: 2.16325
Epoch: 2/20, step: 78420, training_loss: 2.10771
Epoch: 2/20, step: 78440, training_loss: 2.02292
Epoch: 2/20, step: 78460, training_loss: 1.96628
Epoch: 2/20, step: 78480, training_loss: 2.02514
Epoch: 2/20, step: 78500, training_loss: 1.86405
Epoch: 2/20, step: 78520, training_loss: 2.07358
Epoch: 2/20, step: 78540, training_loss: 2.05593
Epoch: 2/20, step: 78560, training_loss: 1.93783
Epoch: 2/20, step: 78580, training_loss: 2.37552
Epoch: 2/20, step: 78600, training_loss: 3.15168
Epoch: 2/20, step: 78620, training_loss: 2.42227
Epoch: 2/20, step: 78640, training_loss: 0.87516
Epoch: 2/20, step: 78660, training_loss: 2.01555
Epoch: 2/20, step: 78680, training_loss: 2.63586
Epoch: 2/20, step: 78700, training_loss: 2.45942
Epoch: 2/20, step: 78720, training_loss: 2.19119
Epoch: 2/20, step: 78740, training_loss: 2.47589
Epoch: 2/20, step: 78760, training_loss: 1.92884
Epoch: 2/20, step: 78780, training_loss: 2.45936
Epoch: 2/20, step: 78800, training_loss: 2.41945
Epoch: 2/20, step: 78820, training_loss: 1.64177
Epoch: 2/20, step: 78840, training_loss: 2.11419
Epoch: 2/20, step: 78860, training_loss: 2.07242
Epoch: 2/20, step: 78880, training_loss: 2.49490
Epoch: 2/20, step: 78900, training_loss: 2.83773
Epoch: 2/20, step: 78920, training_loss: 1.85902
Epoch: 2/20, step: 78940, training_loss: 1.62282
Epoch: 2/20, step: 78960, training_loss: 1.87249
Epoch: 2/20, step: 78980, training_loss: 2.35295
Epoch: 2/20, step: 79000, training_loss: 2.28135
accuracy: 0.47, validation_loss: 1.9412533044815063, num_samples: 100
Epoch: 2/20, step: 79020, training_loss: 2.00524
Epoch: 2/20, step: 79040, training_loss: 2.39163
Epoch: 2/20, step: 79060, training_loss: 2.61187
Epoch: 2/20, step: 79080, training_loss: 1.42686
Epoch: 2/20, step: 79100, training_loss: 2.89397
Epoch: 2/20, step: 79120, training_loss: 1.66745
Epoch: 2/20, step: 79140, training_loss: 1.71969
Epoch: 2/20, step: 79160, training_loss: 3.44246
Epoch: 2/20, step: 79180, training_loss: 2.63955
Epoch: 2/20, step: 79200, training_loss: 2.28053
Epoch: 2/20, step: 79220, training_loss: 2.19934
Epoch: 2/20, step: 79240, training_loss: 2.53118
Epoch: 2/20, step: 79260, training_loss: 1.89503
Epoch: 2/20, step: 79280, training_loss: 1.44116
Epoch: 2/20, step: 79300, training_loss: 2.52109
Epoch: 2/20, step: 79320, training_loss: 2.70319
Epoch: 2/20, step: 79340, training_loss: 1.83309
Epoch: 2/20, step: 79360, training_loss: 2.24437
Epoch: 2/20, step: 79380, training_loss: 2.17511
Epoch: 2/20, step: 79400, training_loss: 2.44604
Epoch: 2/20, step: 79420, training_loss: 1.44098
Epoch: 2/20, step: 79440, training_loss: 2.60993
Epoch: 2/20, step: 79460, training_loss: 1.68938
Epoch: 2/20, step: 79480, training_loss: 2.08559
Epoch: 2/20, step: 79500, training_loss: 1.90642
Epoch: 2/20, step: 79520, training_loss: 2.10641
Epoch: 2/20, step: 79540, training_loss: 2.59083
Epoch: 2/20, step: 79560, training_loss: 2.25981
Epoch: 2/20, step: 79580, training_loss: 2.93721
Epoch: 2/20, step: 79600, training_loss: 1.90306
Epoch: 2/20, step: 79620, training_loss: 2.09741
Epoch: 2/20, step: 79640, training_loss: 1.62534
Epoch: 2/20, step: 79660, training_loss: 2.01426
Epoch: 2/20, step: 79680, training_loss: 2.25161
Epoch: 2/20, step: 79700, training_loss: 1.62618
Epoch: 2/20, step: 79720, training_loss: 2.76030
Epoch: 2/20, step: 79740, training_loss: 2.05406
Epoch: 2/20, step: 79760, training_loss: 2.54959
Epoch: 2/20, step: 79780, training_loss: 2.05332
Epoch: 2/20, step: 79800, training_loss: 2.66464
Epoch: 2/20, step: 79820, training_loss: 2.58822
Epoch: 2/20, step: 79840, training_loss: 2.45809
Epoch: 2/20, step: 79860, training_loss: 2.16490
Epoch: 2/20, step: 79880, training_loss: 1.47053
Epoch: 2/20, step: 79900, training_loss: 1.92152
Epoch: 2/20, step: 79920, training_loss: 1.87937
Epoch: 2/20, step: 79940, training_loss: 2.09599
Epoch: 2/20, step: 79960, training_loss: 2.18184
Epoch: 2/20, step: 79980, training_loss: 2.87868
Epoch: 2/20, step: 80000, training_loss: 1.69129
accuracy: 0.44, validation_loss: 2.1779515743255615, num_samples: 100
Epoch: 2/20, step: 80020, training_loss: 2.01183
Epoch: 2/20, step: 80040, training_loss: 2.35145
Epoch: 2/20, step: 80060, training_loss: 1.36806
Epoch: 2/20, step: 80080, training_loss: 2.74992
Epoch: 2/20, step: 80100, training_loss: 2.26003
Epoch: 2/20, step: 80120, training_loss: 2.09718
Epoch: 2/20, step: 80140, training_loss: 2.22684
Epoch: 2/20, step: 80160, training_loss: 1.65322
Epoch: 2/20, step: 80180, training_loss: 2.78524
Epoch: 2/20, step: 80200, training_loss: 2.15865
Epoch: 2/20, step: 80220, training_loss: 1.62101
Epoch: 2/20, step: 80240, training_loss: 1.48658
Epoch: 2/20, step: 80260, training_loss: 1.70178
Epoch: 2/20, step: 80280, training_loss: 2.57602
Epoch: 2/20, step: 80300, training_loss: 1.69791
Epoch: 2/20, step: 80320, training_loss: 2.40789
Epoch: 2/20, step: 80340, training_loss: 2.10548
Epoch: 2/20, step: 80360, training_loss: 2.39310
Epoch: 2/20, step: 80380, training_loss: 2.28704
Epoch: 2/20, step: 80400, training_loss: 2.32293
Epoch: 2/20, step: 80420, training_loss: 2.95452
Epoch: 2/20, step: 80440, training_loss: 1.73602
Epoch: 2/20, step: 80460, training_loss: 2.23819
Epoch: 2/20, step: 80480, training_loss: 2.19403
Epoch: 2/20, step: 80500, training_loss: 2.32074
Epoch: 2/20, step: 80520, training_loss: 2.55294
Epoch: 2/20, step: 80540, training_loss: 2.14002
Epoch: 2/20, step: 80560, training_loss: 2.03007
Epoch: 2/20, step: 80580, training_loss: 2.43416
Epoch: 2/20, step: 80600, training_loss: 1.59030
Epoch: 2/20, step: 80620, training_loss: 1.89324
Epoch: 2/20, step: 80640, training_loss: 2.26087
Epoch: 2/20, step: 80660, training_loss: 2.76011
Epoch: 2/20, step: 80680, training_loss: 2.22906
Epoch: 2/20, step: 80700, training_loss: 1.88153
Epoch: 2/20, step: 80720, training_loss: 2.13264
Epoch: 2/20, step: 80740, training_loss: 2.24027
Epoch: 2/20, step: 80760, training_loss: 2.19697
Epoch: 2/20, step: 80780, training_loss: 2.00648
Epoch: 2/20, step: 80800, training_loss: 2.05442
Epoch: 2/20, step: 80820, training_loss: 2.11559
Epoch: 2/20, step: 80840, training_loss: 2.06757
Epoch: 2/20, step: 80860, training_loss: 2.23095
Epoch: 2/20, step: 80880, training_loss: 2.90608
Epoch: 2/20, step: 80900, training_loss: 2.98630
Epoch: 2/20, step: 80920, training_loss: 2.30695
Epoch: 2/20, step: 80940, training_loss: 3.07682
Epoch: 2/20, step: 80960, training_loss: 2.30490
Epoch: 2/20, step: 80980, training_loss: 1.72674
Epoch: 2/20, step: 81000, training_loss: 1.96075
accuracy: 0.37, validation_loss: 2.4380722045898438, num_samples: 100
Epoch: 2/20, step: 81020, training_loss: 1.93426
Epoch: 2/20, step: 81040, training_loss: 2.62578
Epoch: 2/20, step: 81060, training_loss: 1.48418
Epoch: 2/20, step: 81080, training_loss: 2.50033
Epoch: 2/20, step: 81100, training_loss: 2.29452
Epoch: 2/20, step: 81120, training_loss: 1.38623
Epoch: 2/20, step: 81140, training_loss: 2.12872
Epoch: 2/20, step: 81160, training_loss: 2.05056
Epoch: 2/20, step: 81180, training_loss: 2.95807
Epoch: 2/20, step: 81200, training_loss: 1.85369
Epoch: 2/20, step: 81220, training_loss: 1.83089
Epoch: 2/20, step: 81240, training_loss: 1.81138
Epoch: 2/20, step: 81260, training_loss: 2.68533
Epoch: 2/20, step: 81280, training_loss: 2.41161
Epoch: 2/20, step: 81300, training_loss: 1.82769
Epoch: 2/20, step: 81320, training_loss: 2.35276
Epoch: 2/20, step: 81340, training_loss: 1.49443
Epoch: 2/20, step: 81360, training_loss: 1.96328
Epoch: 2/20, step: 81380, training_loss: 1.94179
Epoch: 2/20, step: 81400, training_loss: 0.84899
Epoch: 2/20, step: 81420, training_loss: 2.77183
Epoch: 2/20, step: 81440, training_loss: 1.95812
Epoch: 2/20, step: 81460, training_loss: 3.23781
Epoch: 2/20, step: 81480, training_loss: 1.78597
Epoch: 2/20, step: 81500, training_loss: 1.64104
Epoch: 2/20, step: 81520, training_loss: 1.52589
Epoch: 2/20, step: 81540, training_loss: 2.06322
Epoch: 2/20, step: 81560, training_loss: 2.28148
Epoch: 2/20, step: 81580, training_loss: 1.60325
Epoch: 2/20, step: 81600, training_loss: 1.91238
Epoch: 2/20, step: 81620, training_loss: 1.44629
Epoch: 2/20, step: 81640, training_loss: 2.01705
Epoch: 2/20, step: 81660, training_loss: 2.57673
Epoch: 2/20, step: 81680, training_loss: 2.05339
Epoch: 2/20, step: 81700, training_loss: 2.17204
Epoch: 2/20, step: 81720, training_loss: 1.82797
Epoch: 2/20, step: 81740, training_loss: 1.28931
Epoch: 2/20, step: 81760, training_loss: 1.72236
Epoch: 2/20, step: 81780, training_loss: 1.44944
Epoch: 2/20, step: 81800, training_loss: 2.48288
Epoch: 2/20, step: 81820, training_loss: 2.10640
Epoch: 2/20, step: 81840, training_loss: 2.03339
Epoch: 2/20, step: 81860, training_loss: 1.39289
Epoch: 2/20, step: 81880, training_loss: 2.26414
Epoch: 2/20, step: 81900, training_loss: 2.08811
Epoch: 2/20, step: 81920, training_loss: 2.10179
Epoch: 2/20, step: 81940, training_loss: 2.33642
Epoch: 2/20, step: 81960, training_loss: 1.94109
Epoch: 2/20, step: 81980, training_loss: 2.02613
Epoch: 2/20, step: 82000, training_loss: 1.83614
accuracy: 0.44, validation_loss: 2.0555264949798584, num_samples: 100
Epoch: 2/20, step: 82020, training_loss: 2.40437
Epoch: 2/20, step: 82040, training_loss: 2.64788
Epoch: 2/20, step: 82060, training_loss: 2.04353
Epoch: 2/20, step: 82080, training_loss: 1.59938
Epoch: 2/20, step: 82100, training_loss: 1.49213
Epoch: 2/20, step: 82120, training_loss: 1.54029
Epoch: 2/20, step: 82140, training_loss: 2.30635
Epoch: 2/20, step: 82160, training_loss: 1.77118
Epoch: 2/20, step: 82180, training_loss: 1.64741
Epoch: 2/20, step: 82200, training_loss: 2.30292
Epoch: 2/20, step: 82220, training_loss: 3.02965
Epoch: 2/20, step: 82240, training_loss: 2.32080
Epoch: 2/20, step: 82260, training_loss: 3.26511
Epoch: 2/20, step: 82280, training_loss: 2.22909
Epoch: 2/20, step: 82300, training_loss: 2.86910
Epoch: 2/20, step: 82320, training_loss: 2.72166
Epoch: 2/20, step: 82340, training_loss: 2.07226
Epoch: 2/20, step: 82360, training_loss: 2.23870
Epoch: 2/20, step: 82380, training_loss: 1.26634
Epoch: 2/20, step: 82400, training_loss: 2.35989
Epoch: 2/20, step: 82420, training_loss: 1.82531
Epoch: 2/20, step: 82440, training_loss: 2.60769
Epoch: 2/20, step: 82460, training_loss: 1.99657
Epoch: 2/20, step: 82480, training_loss: 2.66528
Epoch: 2/20, step: 82500, training_loss: 2.63093
Epoch: 2/20, step: 82520, training_loss: 2.43950
Epoch: 2/20, step: 82540, training_loss: 2.44971
Epoch: 2/20, step: 82560, training_loss: 2.65548
Epoch: 2/20, step: 82580, training_loss: 1.90825
Epoch: 2/20, step: 82600, training_loss: 2.38465
Epoch: 2/20, step: 82620, training_loss: 1.91847
Epoch: 2/20, step: 82640, training_loss: 2.65001
Epoch: 2/20, step: 82660, training_loss: 1.64669
Epoch: 2/20, step: 82680, training_loss: 2.09258
Epoch: 2/20, step: 82700, training_loss: 2.47001
Epoch: 2/20, step: 82720, training_loss: 1.21053
Epoch: 2/20, step: 82740, training_loss: 2.20360
Epoch: 2/20, step: 82760, training_loss: 2.53641
Epoch: 2/20, step: 82780, training_loss: 1.89870
Epoch: 2/20, step: 82800, training_loss: 2.68160
Epoch: 2/20, step: 82820, training_loss: 2.26428
Epoch: 2/20, step: 82840, training_loss: 1.39739
Epoch: 2/20, step: 82860, training_loss: 2.08046
Epoch: 2/20, step: 82880, training_loss: 2.07649
Epoch: 2/20, step: 82900, training_loss: 2.21756
Epoch: 2/20, step: 82920, training_loss: 1.63236
Epoch: 2/20, step: 82940, training_loss: 2.20149
Epoch: 2/20, step: 82960, training_loss: 1.64114
Epoch: 2/20, step: 82980, training_loss: 2.40354
Epoch: 2/20, step: 83000, training_loss: 2.58011
accuracy: 0.45, validation_loss: 2.060452461242676, num_samples: 100
Epoch: 2/20, step: 83020, training_loss: 2.10225
Epoch: 2/20, step: 83040, training_loss: 1.83205
Epoch: 2/20, step: 83060, training_loss: 2.15727
Epoch: 2/20, step: 83080, training_loss: 2.80920
Epoch: 2/20, step: 83100, training_loss: 2.01605
Epoch: 2/20, step: 83120, training_loss: 2.94807
Epoch: 2/20, step: 83140, training_loss: 2.23562
Epoch: 2/20, step: 83160, training_loss: 1.97632
Epoch: 2/20, step: 83180, training_loss: 1.99820
Epoch: 2/20, step: 83200, training_loss: 1.49559
Epoch: 2/20, step: 83220, training_loss: 2.05782
Epoch: 2/20, step: 83240, training_loss: 1.83308
Epoch: 2/20, step: 83260, training_loss: 2.04123
Epoch: 2/20, step: 83280, training_loss: 2.52541
Epoch: 2/20, step: 83300, training_loss: 1.98657
Epoch: 2/20, step: 83320, training_loss: 2.19255
Epoch: 2/20, step: 83340, training_loss: 2.34047
Epoch: 2/20, step: 83360, training_loss: 1.48551
Epoch: 2/20, step: 83380, training_loss: 2.40376
Epoch: 2/20, step: 83400, training_loss: 2.38066
Epoch: 2/20, step: 83420, training_loss: 3.09478
Epoch: 2/20, step: 83440, training_loss: 3.01908
Epoch: 2/20, step: 83460, training_loss: 2.10807
Epoch: 2/20, step: 83480, training_loss: 1.17375
Epoch: 2/20, step: 83500, training_loss: 2.49445
Epoch: 2/20, step: 83520, training_loss: 3.15285
Epoch: 2/20, step: 83540, training_loss: 1.85787
Epoch: 2/20, step: 83560, training_loss: 1.97979
Epoch: 2/20, step: 83580, training_loss: 2.57392
Epoch: 2/20, step: 83600, training_loss: 2.35314
Epoch: 2/20, step: 83620, training_loss: 1.90029
Epoch: 2/20, step: 83640, training_loss: 2.00890
Epoch: 2/20, step: 83660, training_loss: 2.86039
Epoch: 2/20, step: 83680, training_loss: 1.98446
Epoch: 2/20, step: 83700, training_loss: 1.30263
Epoch: 2/20, step: 83720, training_loss: 2.06465
Epoch: 2/20, step: 83740, training_loss: 1.28064
Epoch: 2/20, step: 83760, training_loss: 2.00749
Epoch: 2/20, step: 83780, training_loss: 2.03859
Epoch: 2/20, step: 83800, training_loss: 2.25963
Epoch: 2/20, step: 83820, training_loss: 1.95842
Epoch: 2/20, step: 83840, training_loss: 1.78495
Epoch: 2/20, step: 83860, training_loss: 1.91619
Epoch: 2/20, step: 83880, training_loss: 1.92076
Epoch: 2/20, step: 83900, training_loss: 1.34620
Epoch: 2/20, step: 83920, training_loss: 2.83574
Epoch: 2/20, step: 83940, training_loss: 3.03522
Epoch: 2/20, step: 83960, training_loss: 1.91267
Epoch: 2/20, step: 83980, training_loss: 2.16296
Epoch: 2/20, step: 84000, training_loss: 2.21189
accuracy: 0.52, validation_loss: 1.5996006727218628, num_samples: 100
Epoch: 2/20, step: 84020, training_loss: 2.77439
Epoch: 2/20, step: 84040, training_loss: 2.69320
Epoch: 2/20, step: 84060, training_loss: 1.79160
Epoch: 2/20, step: 84080, training_loss: 2.59799
Epoch: 2/20, step: 84100, training_loss: 2.68606
Epoch: 2/20, step: 84120, training_loss: 2.47974
Epoch: 2/20, step: 84140, training_loss: 2.40468
Epoch: 2/20, step: 84160, training_loss: 2.42004
Epoch: 2/20, step: 84180, training_loss: 2.46304
Epoch: 2/20, step: 84200, training_loss: 2.19661
Epoch: 2/20, step: 84220, training_loss: 1.62380
Epoch: 2/20, step: 84240, training_loss: 1.91481
Epoch: 2/20, step: 84260, training_loss: 3.25144
Epoch: 2/20, step: 84280, training_loss: 1.91552
Epoch: 2/20, step: 84300, training_loss: 2.06985
Epoch: 2/20, step: 84320, training_loss: 1.55409
Epoch: 2/20, step: 84340, training_loss: 2.46142
Epoch: 2/20, step: 84360, training_loss: 2.02850
Epoch: 2/20, step: 84380, training_loss: 1.90953
Epoch: 2/20, step: 84400, training_loss: 2.73678
Epoch: 2/20, step: 84420, training_loss: 1.61936
Epoch: 2/20, step: 84440, training_loss: 1.10846
Epoch: 2/20, step: 84460, training_loss: 1.51113
Epoch: 2/20, step: 84480, training_loss: 3.07243
Epoch: 2/20, step: 84500, training_loss: 2.14217
Epoch: 2/20, step: 84520, training_loss: 2.03164
Epoch: 2/20, step: 84540, training_loss: 2.06234
Epoch: 2/20, step: 84560, training_loss: 2.46030
Epoch: 2/20, step: 84580, training_loss: 1.91794
Epoch: 2/20, step: 84600, training_loss: 2.64621
Epoch: 2/20, step: 84620, training_loss: 1.56959
Epoch: 2/20, step: 84640, training_loss: 3.71747
Epoch: 2/20, step: 84660, training_loss: 2.09523
Epoch: 2/20, step: 84680, training_loss: 1.83074
Epoch: 2/20, step: 84700, training_loss: 1.88351
Epoch: 2/20, step: 84720, training_loss: 2.02295
Epoch: 2/20, step: 84740, training_loss: 2.62831
Epoch: 2/20, step: 84760, training_loss: 2.36083
Epoch: 2/20, step: 84780, training_loss: 2.03086
Epoch: 2/20, step: 84800, training_loss: 1.91697
Epoch: 2/20, step: 84820, training_loss: 1.99232
Epoch: 2/20, step: 84840, training_loss: 2.58880
Epoch: 2/20, step: 84860, training_loss: 2.88447
Epoch: 2/20, step: 84880, training_loss: 3.25507
Epoch: 2/20, step: 84900, training_loss: 1.93094
Epoch: 2/20, step: 84920, training_loss: 2.03525
Epoch: 2/20, step: 84940, training_loss: 1.57166
Epoch: 2/20, step: 84960, training_loss: 1.82272
Epoch: 2/20, step: 84980, training_loss: 1.76227
Epoch: 2/20, step: 85000, training_loss: 2.49114
accuracy: 0.39, validation_loss: 2.1836273670196533, num_samples: 100
Epoch: 2/20, step: 85020, training_loss: 2.89302
Epoch: 2/20, step: 85040, training_loss: 2.09268
Epoch: 2/20, step: 85060, training_loss: 2.51883
Epoch: 2/20, step: 85080, training_loss: 2.70410
Epoch: 2/20, step: 85100, training_loss: 1.79194
Epoch: 2/20, step: 85120, training_loss: 2.57344
Epoch: 2/20, step: 85140, training_loss: 1.75306
Epoch: 2/20, step: 85160, training_loss: 2.96443
Epoch: 2/20, step: 85180, training_loss: 2.13922
Epoch: 2/20, step: 85200, training_loss: 2.15846
Epoch: 2/20, step: 85220, training_loss: 2.06358
Epoch: 2/20, step: 85240, training_loss: 2.55212
Epoch: 2/20, step: 85260, training_loss: 2.73882
Epoch: 2/20, step: 85280, training_loss: 2.25203
Epoch: 2/20, step: 85300, training_loss: 1.74054
Epoch: 2/20, step: 85320, training_loss: 2.39497
Epoch: 2/20, step: 85340, training_loss: 1.78969
Epoch: 2/20, step: 85360, training_loss: 2.89924
Epoch: 2/20, step: 85380, training_loss: 2.40525
Epoch: 2/20, step: 85400, training_loss: 2.81426
Epoch: 2/20, step: 85420, training_loss: 2.35872
Epoch: 2/20, step: 85440, training_loss: 2.63219
Epoch: 2/20, step: 85460, training_loss: 2.89170
Epoch: 2/20, step: 85480, training_loss: 1.91710
Epoch: 2/20, step: 85500, training_loss: 2.44690
Epoch: 2/20, step: 85520, training_loss: 3.39042
Epoch: 2/20, step: 85540, training_loss: 3.22888
Epoch: 2/20, step: 85560, training_loss: 1.43259
Epoch: 2/20, step: 85580, training_loss: 2.45633
Epoch: 2/20, step: 85600, training_loss: 1.83308
Epoch: 2/20, step: 85620, training_loss: 2.81687
Epoch: 2/20, step: 85640, training_loss: 3.25118
Epoch: 2/20, step: 85660, training_loss: 2.49192
Epoch: 2/20, step: 85680, training_loss: 2.53197
Epoch: 2/20, step: 85700, training_loss: 1.66127
Epoch: 2/20, step: 85720, training_loss: 1.77718
Epoch: 2/20, step: 85740, training_loss: 2.47917
Epoch: 2/20, step: 85760, training_loss: 2.24717
Epoch: 2/20, step: 85780, training_loss: 2.34136
Epoch: 2/20, step: 85800, training_loss: 1.48958
Epoch: 2/20, step: 85820, training_loss: 2.47309
Epoch: 2/20, step: 85840, training_loss: 1.51865
Epoch: 2/20, step: 85860, training_loss: 1.60655
Epoch: 2/20, step: 85880, training_loss: 1.57984
Epoch: 2/20, step: 85900, training_loss: 2.59790
Epoch: 2/20, step: 85920, training_loss: 2.60175
Epoch: 2/20, step: 85940, training_loss: 1.98797
Epoch: 2/20, step: 85960, training_loss: 2.04915
Epoch: 2/20, step: 85980, training_loss: 1.93929
Epoch: 2/20, step: 86000, training_loss: 2.28372
accuracy: 0.48, validation_loss: 1.9850833415985107, num_samples: 100
Epoch: 2/20, step: 86020, training_loss: 2.11519
Epoch: 2/20, step: 86040, training_loss: 2.24783
Epoch: 2/20, step: 86060, training_loss: 2.05439
Epoch: 2/20, step: 86080, training_loss: 2.02638
Epoch: 2/20, step: 86100, training_loss: 1.94920
Epoch: 2/20, step: 86120, training_loss: 2.26295
Epoch: 2/20, step: 86140, training_loss: 1.46520
Epoch: 2/20, step: 86160, training_loss: 2.36617
Epoch: 2/20, step: 86180, training_loss: 1.90705
Epoch: 2/20, step: 86200, training_loss: 1.93282
Epoch: 2/20, step: 86220, training_loss: 1.72167
Epoch: 2/20, step: 86240, training_loss: 3.22157
Epoch: 2/20, step: 86260, training_loss: 2.15330
Epoch: 2/20, step: 86280, training_loss: 2.18596
Epoch: 2/20, step: 86300, training_loss: 1.57157
Epoch: 2/20, step: 86320, training_loss: 2.22258
Epoch: 2/20, step: 86340, training_loss: 2.71452
Epoch: 2/20, step: 86360, training_loss: 3.06174
Epoch: 2/20, step: 86380, training_loss: 2.08170
Epoch: 2/20, step: 86400, training_loss: 2.13311
Epoch: 2/20, step: 86420, training_loss: 2.57320
Epoch: 2/20, step: 86440, training_loss: 1.83044
Epoch: 2/20, step: 86460, training_loss: 2.20011
Epoch: 2/20, step: 86480, training_loss: 1.55438
Epoch: 2/20, step: 86500, training_loss: 2.36587
Epoch: 2/20, step: 86520, training_loss: 2.36022
Epoch: 2/20, step: 86540, training_loss: 1.51597
Epoch: 2/20, step: 86560, training_loss: 1.89455
Epoch: 2/20, step: 86580, training_loss: 2.86830
Epoch: 2/20, step: 86600, training_loss: 1.93902
Epoch: 2/20, step: 86620, training_loss: 2.76236
Epoch: 2/20, step: 86640, training_loss: 2.21143
Epoch: 2/20, step: 86660, training_loss: 3.04481
Epoch: 2/20, step: 86680, training_loss: 2.18551
Epoch: 2/20, step: 86700, training_loss: 2.55162
Epoch: 2/20, step: 86720, training_loss: 2.09290
Epoch: 2/20, step: 86740, training_loss: 2.37635
Epoch: 2/20, step: 86760, training_loss: 2.30385
Epoch: 2/20, step: 86780, training_loss: 2.81356
Epoch: 2/20, step: 86800, training_loss: 2.60721
Epoch: 2/20, step: 86820, training_loss: 3.12429
Epoch: 2/20, step: 86840, training_loss: 2.43757
Epoch: 2/20, step: 86860, training_loss: 1.92648
Epoch: 2/20, step: 86880, training_loss: 2.10008
Epoch: 2/20, step: 86900, training_loss: 2.60382
Epoch: 2/20, step: 86920, training_loss: 1.66018
Epoch: 2/20, step: 86940, training_loss: 3.05222
Epoch: 2/20, step: 86960, training_loss: 2.33130
Epoch: 2/20, step: 86980, training_loss: 2.55465
Epoch: 2/20, step: 87000, training_loss: 2.11760
accuracy: 0.42, validation_loss: 1.9950947761535645, num_samples: 100
Epoch: 2/20, step: 87020, training_loss: 1.86658
Epoch: 2/20, step: 87040, training_loss: 1.96289
Epoch: 2/20, step: 87060, training_loss: 2.05514
Epoch: 2/20, step: 87080, training_loss: 2.56129
Epoch: 2/20, step: 87100, training_loss: 3.52006
Epoch: 2/20, step: 87120, training_loss: 3.06534
Epoch: 2/20, step: 87140, training_loss: 1.89400
Epoch: 2/20, step: 87160, training_loss: 1.28133
Epoch: 2/20, step: 87180, training_loss: 2.19436
Epoch: 2/20, step: 87200, training_loss: 1.75783
Epoch: 2/20, step: 87220, training_loss: 1.69404
Epoch: 2/20, step: 87240, training_loss: 1.77264
Epoch: 2/20, step: 87260, training_loss: 2.21654
Epoch: 2/20, step: 87280, training_loss: 2.31449
Epoch: 2/20, step: 87300, training_loss: 2.31526
Epoch: 2/20, step: 87320, training_loss: 2.01883
Epoch: 2/20, step: 87340, training_loss: 1.95876
Epoch: 2/20, step: 87360, training_loss: 1.97993
Epoch: 2/20, step: 87380, training_loss: 1.93346
Epoch: 2/20, step: 87400, training_loss: 1.73107
Epoch: 2/20, step: 87420, training_loss: 1.91393
Epoch: 2/20, step: 87440, training_loss: 2.31871
Epoch: 2/20, step: 87460, training_loss: 1.36097
Epoch: 2/20, step: 87480, training_loss: 2.02577
Epoch: 2/20, step: 87500, training_loss: 2.86384
Epoch: 2/20, step: 87520, training_loss: 2.03645
Epoch: 2/20, step: 87540, training_loss: 2.00090
Epoch: 2/20, step: 87560, training_loss: 2.61184
Epoch: 2/20, step: 87580, training_loss: 2.37287
Epoch: 2/20, step: 87600, training_loss: 1.90037
Epoch: 2/20, step: 87620, training_loss: 1.93337
Epoch: 2/20, step: 87640, training_loss: 2.25768
Epoch: 2/20, step: 87660, training_loss: 2.25305
Epoch: 2/20, step: 87680, training_loss: 1.74864
Epoch: 2/20, step: 87700, training_loss: 2.51862
Epoch: 2/20, step: 87720, training_loss: 2.31366
Epoch: 2/20, step: 87740, training_loss: 2.45792
Epoch: 2/20, step: 87760, training_loss: 2.68651
Epoch: 2/20, step: 87780, training_loss: 2.14973
Epoch: 2/20, step: 87800, training_loss: 2.26523
Epoch: 2/20, step: 87820, training_loss: 1.34626
Epoch: 2/20, step: 87840, training_loss: 2.62916
Epoch: 2/20, step: 87860, training_loss: 2.10341
Epoch: 2/20, step: 87880, training_loss: 2.31410
Epoch: 2/20, step: 87900, training_loss: 1.81328
Epoch: 2/20, step: 87920, training_loss: 2.62803
Epoch: 2/20, step: 87940, training_loss: 2.03429
Epoch: 2/20, step: 87960, training_loss: 2.62075
Epoch: 2/20, step: 87980, training_loss: 2.50317
Epoch: 2/20, step: 88000, training_loss: 1.53370
accuracy: 0.44, validation_loss: 1.7029224634170532, num_samples: 100
Epoch: 2/20, step: 88020, training_loss: 1.59963
Epoch: 2/20, step: 88040, training_loss: 2.37313
Epoch: 2/20, step: 88060, training_loss: 3.31456
Epoch: 2/20, step: 88080, training_loss: 2.35419
Epoch: 2/20, step: 88100, training_loss: 1.70735
Epoch: 2/20, step: 88120, training_loss: 2.89836
Epoch: 2/20, step: 88140, training_loss: 2.02095
Epoch: 2/20, step: 88160, training_loss: 1.60358
Epoch: 2/20, step: 88180, training_loss: 2.36313
Epoch: 2/20, step: 88200, training_loss: 2.55556
Epoch: 2/20, step: 88220, training_loss: 2.06846
Epoch: 2/20, step: 88240, training_loss: 2.39148
Epoch: 2/20, step: 88260, training_loss: 2.41019
Epoch: 2/20, step: 88280, training_loss: 1.96307
Epoch: 2/20, step: 88300, training_loss: 1.92100
Epoch: 2/20, step: 88320, training_loss: 1.65830
Epoch: 2/20, step: 88340, training_loss: 2.16567
Epoch: 2/20, step: 88360, training_loss: 2.03675
Epoch: 2/20, step: 88380, training_loss: 2.35650
Epoch: 2/20, step: 88400, training_loss: 2.51583
Epoch: 2/20, step: 88420, training_loss: 2.00756
Epoch: 2/20, step: 88440, training_loss: 1.39783
Epoch: 2/20, step: 88460, training_loss: 1.95861
Epoch: 2/20, step: 88480, training_loss: 1.76277
Epoch: 2/20, step: 88500, training_loss: 2.50116
Epoch: 2/20, step: 88520, training_loss: 2.15500
Epoch: 2/20, step: 88540, training_loss: 2.04387
Epoch: 2/20, step: 88560, training_loss: 2.28019
Epoch: 2/20, step: 88580, training_loss: 1.73385
Epoch: 2/20, step: 88600, training_loss: 2.62448
Epoch: 2/20, step: 88620, training_loss: 1.76545
Epoch: 2/20, step: 88640, training_loss: 2.34981
Epoch: 2/20, step: 88660, training_loss: 2.29797
Epoch: 2/20, step: 88680, training_loss: 1.89090
Epoch: 2/20, step: 88700, training_loss: 2.32148
Epoch: 2/20, step: 88720, training_loss: 2.57689
Epoch: 2/20, step: 88740, training_loss: 2.40913
Epoch: 2/20, step: 88760, training_loss: 1.61908
Epoch: 2/20, step: 88780, training_loss: 2.14332
Epoch: 2/20, step: 88800, training_loss: 2.30101
Epoch: 2/20, step: 88820, training_loss: 2.68183
Epoch: 2/20, step: 88840, training_loss: 2.41010
Epoch: 2/20, step: 88860, training_loss: 1.52864
Epoch: 2/20, step: 88880, training_loss: 1.97194
Epoch: 2/20, step: 88900, training_loss: 1.98366
Epoch: 2/20, step: 88920, training_loss: 2.31180
Epoch: 2/20, step: 88940, training_loss: 1.53349
Epoch: 2/20, step: 88960, training_loss: 2.15874
Epoch: 2/20, step: 88980, training_loss: 3.27940
Epoch: 2/20, step: 89000, training_loss: 2.59500
accuracy: 0.36, validation_loss: 2.38456654548645, num_samples: 100
Epoch: 2/20, step: 89020, training_loss: 2.42592
Epoch: 2/20, step: 89040, training_loss: 2.37748
Epoch: 2/20, step: 89060, training_loss: 2.73166
Epoch: 2/20, step: 89080, training_loss: 2.13758
Epoch: 2/20, step: 89100, training_loss: 2.10183
Epoch: 2/20, step: 89120, training_loss: 2.99483
Epoch: 2/20, step: 89140, training_loss: 1.51119
Epoch: 2/20, step: 89160, training_loss: 2.58252
Epoch: 2/20, step: 89180, training_loss: 2.25552
Epoch: 2/20, step: 89200, training_loss: 1.83294
Epoch: 2/20, step: 89220, training_loss: 1.66762
Epoch: 2/20, step: 89240, training_loss: 2.27553
Epoch: 2/20, step: 89260, training_loss: 1.70769
Epoch: 2/20, step: 89280, training_loss: 1.78215
Epoch: 2/20, step: 89300, training_loss: 2.59879
Epoch: 2/20, step: 89320, training_loss: 2.55078
Epoch: 2/20, step: 89340, training_loss: 2.22899
Epoch: 2/20, step: 89360, training_loss: 1.83450
Epoch: 2/20, step: 89380, training_loss: 2.22672
Epoch: 2/20, step: 89400, training_loss: 1.58684
Epoch: 2/20, step: 89420, training_loss: 2.97044
Epoch: 2/20, step: 89440, training_loss: 2.06230
Epoch: 2/20, step: 89460, training_loss: 2.43399
Epoch: 2/20, step: 89480, training_loss: 2.75527
Epoch: 2/20, step: 89500, training_loss: 2.38596
Epoch: 2/20, step: 89520, training_loss: 2.61395
Epoch: 2/20, step: 89540, training_loss: 1.46113
Epoch: 2/20, step: 89560, training_loss: 2.58902
Epoch: 2/20, step: 89580, training_loss: 2.71936
Epoch: 2/20, step: 89600, training_loss: 2.17017
Epoch: 2/20, step: 89620, training_loss: 2.68216
Epoch: 2/20, step: 89640, training_loss: 2.12718
Epoch: 2/20, step: 89660, training_loss: 3.06011
Epoch: 2/20, step: 89680, training_loss: 3.10551
Epoch: 2/20, step: 89700, training_loss: 2.36916
Epoch: 2/20, step: 89720, training_loss: 1.46340
Epoch: 2/20, step: 89740, training_loss: 2.43122
Epoch: 2/20, step: 89760, training_loss: 2.32675
Epoch: 2/20, step: 89780, training_loss: 1.90435
Epoch: 2/20, step: 89800, training_loss: 2.89920
Epoch: 2/20, step: 89820, training_loss: 2.20190
Epoch: 2/20, step: 89840, training_loss: 2.86588
Epoch: 2/20, step: 89860, training_loss: 1.45729
Epoch: 2/20, step: 89880, training_loss: 1.56512
Epoch: 2/20, step: 89900, training_loss: 2.82671
Epoch: 2/20, step: 89920, training_loss: 2.65538
Epoch: 2/20, step: 89940, training_loss: 2.29423
Epoch: 2/20, step: 89960, training_loss: 2.46997
Epoch: 2/20, step: 89980, training_loss: 2.85569
Epoch: 2/20, step: 90000, training_loss: 1.59593
accuracy: 0.43, validation_loss: 1.9450085163116455, num_samples: 100
Epoch: 2/20, step: 90020, training_loss: 3.03715
Epoch: 2/20, step: 90040, training_loss: 2.42937
Epoch: 2/20, step: 90060, training_loss: 1.67529
Epoch: 2/20, step: 90080, training_loss: 1.15540
Epoch: 2/20, step: 90100, training_loss: 2.12130
Epoch: 2/20, step: 90120, training_loss: 2.78228
Epoch: 2/20, step: 90140, training_loss: 2.18350
Epoch: 2/20, step: 90160, training_loss: 2.23893
Epoch: 2/20, step: 90180, training_loss: 1.99683
Epoch: 2/20, step: 90200, training_loss: 1.73558
Epoch: 2/20, step: 90220, training_loss: 2.03107
Epoch: 2/20, step: 90240, training_loss: 1.81848
Epoch: 2/20, step: 90260, training_loss: 2.59184
Epoch: 2/20, step: 90280, training_loss: 2.43147
Epoch: 2/20, step: 90300, training_loss: 2.72734
Epoch: 2/20, step: 90320, training_loss: 2.66198
Epoch: 2/20, step: 90340, training_loss: 1.61631
Epoch: 2/20, step: 90360, training_loss: 2.61337
Epoch: 2/20, step: 90380, training_loss: 2.50941
Epoch: 2/20, step: 90400, training_loss: 1.59201
Epoch: 2/20, step: 90420, training_loss: 2.10956
Epoch: 2/20, step: 90440, training_loss: 1.64279
Epoch: 2/20, step: 90460, training_loss: 2.86469
Epoch: 2/20, step: 90480, training_loss: 1.90932
Epoch: 2/20, step: 90500, training_loss: 2.45764
Epoch: 2/20, step: 90520, training_loss: 1.73501
Epoch: 2/20, step: 90540, training_loss: 1.75942
Epoch: 2/20, step: 90560, training_loss: 1.07012
Epoch: 2/20, step: 90580, training_loss: 3.16681
Epoch: 2/20, step: 90600, training_loss: 2.67696
Epoch: 2/20, step: 90620, training_loss: 1.44563
Epoch: 2/20, step: 90640, training_loss: 2.24775
Epoch: 2/20, step: 90660, training_loss: 2.26443
Epoch: 2/20, step: 90680, training_loss: 1.66293
Epoch: 2/20, step: 90700, training_loss: 2.29447
Epoch: 2/20, step: 90720, training_loss: 2.31541
Epoch: 2/20, step: 90740, training_loss: 2.82354
Epoch: 2/20, step: 90760, training_loss: 1.99604
Epoch: 2/20, step: 90780, training_loss: 2.27271
Epoch: 2/20, step: 90800, training_loss: 1.80155
Epoch: 2/20, step: 90820, training_loss: 2.23293
Epoch: 2/20, step: 90840, training_loss: 2.74870
Epoch: 2/20, step: 90860, training_loss: 2.53898
Epoch: 2/20, step: 90880, training_loss: 2.04238
Epoch: 2/20, step: 90900, training_loss: 2.68063
Epoch: 2/20, step: 90920, training_loss: 1.86469
Epoch: 2/20, step: 90940, training_loss: 2.00556
Epoch: 2/20, step: 90960, training_loss: 2.11389
Epoch: 2/20, step: 90980, training_loss: 2.22094
Epoch: 2/20, step: 91000, training_loss: 1.65699
accuracy: 0.38, validation_loss: 2.319535255432129, num_samples: 100
Epoch: 2/20, step: 91020, training_loss: 2.44302
Epoch: 2/20, step: 91040, training_loss: 2.10576
Epoch: 2/20, step: 91060, training_loss: 3.05731
Epoch: 2/20, step: 91080, training_loss: 1.77206
Epoch: 2/20, step: 91100, training_loss: 2.58104
Epoch: 2/20, step: 91120, training_loss: 2.14062
Epoch: 2/20, step: 91140, training_loss: 2.09857
Epoch: 2/20, step: 91160, training_loss: 1.93047
Epoch: 2/20, step: 91180, training_loss: 2.10920
Epoch: 2/20, step: 91200, training_loss: 1.95812
Epoch: 2/20, step: 91220, training_loss: 1.66177
Epoch: 2/20, step: 91240, training_loss: 2.04732
Epoch: 2/20, step: 91260, training_loss: 2.38071
Epoch: 2/20, step: 91280, training_loss: 2.37454
Epoch: 2/20, step: 91300, training_loss: 1.74791
Epoch: 2/20, step: 91320, training_loss: 1.97198
Epoch: 2/20, step: 91340, training_loss: 2.57560
Epoch: 2/20, step: 91360, training_loss: 1.89075
Epoch: 2/20, step: 91380, training_loss: 2.83119
Epoch: 2/20, step: 91400, training_loss: 1.88743
Epoch: 2/20, step: 91420, training_loss: 1.32866
Epoch: 2/20, step: 91440, training_loss: 2.11895
Epoch: 2/20, step: 91460, training_loss: 2.75750
Epoch: 2/20, step: 91480, training_loss: 2.23547
Epoch: 2/20, step: 91500, training_loss: 2.41108
Epoch: 2/20, step: 91520, training_loss: 2.60812
Epoch: 2/20, step: 91540, training_loss: 2.63417
Epoch: 2/20, step: 91560, training_loss: 2.72531
Epoch: 2/20, step: 91580, training_loss: 2.17275
Epoch: 2/20, step: 91600, training_loss: 2.58043
Epoch: 2/20, step: 91620, training_loss: 1.56115
Epoch: 2/20, step: 91640, training_loss: 2.06802
Epoch: 2/20, step: 91660, training_loss: 2.49488
Epoch: 2/20, step: 91680, training_loss: 2.64322
Epoch: 2/20, step: 91700, training_loss: 2.11700
Epoch: 2/20, step: 91720, training_loss: 2.92200
Epoch: 2/20, step: 91740, training_loss: 2.21955
Epoch: 2/20, step: 91760, training_loss: 1.67742
Epoch: 2/20, step: 91780, training_loss: 2.04189
Epoch: 2/20, step: 91800, training_loss: 1.70683
Epoch: 2/20, step: 91820, training_loss: 1.39223
Epoch: 2/20, step: 91840, training_loss: 2.76978
Epoch: 2/20, step: 91860, training_loss: 2.25161
Epoch: 2/20, step: 91880, training_loss: 1.79927
Epoch: 2/20, step: 91900, training_loss: 2.53803
Epoch: 2/20, step: 91920, training_loss: 3.06441
Epoch: 2/20, step: 91940, training_loss: 2.04657
Epoch: 2/20, step: 91960, training_loss: 2.66843
Epoch: 2/20, step: 91980, training_loss: 2.65280
Epoch: 2/20, step: 92000, training_loss: 2.34725
accuracy: 0.37, validation_loss: 2.431732416152954, num_samples: 100
Epoch: 2/20, step: 92020, training_loss: 1.99131
Epoch: 2/20, step: 92040, training_loss: 2.09950
Epoch: 2/20, step: 92060, training_loss: 1.99039
Epoch: 2/20, step: 92080, training_loss: 1.62664
Epoch: 2/20, step: 92100, training_loss: 1.65209
Epoch: 2/20, step: 92120, training_loss: 1.88699
Epoch: 2/20, step: 92140, training_loss: 2.15281
Epoch: 2/20, step: 92160, training_loss: 2.51639
Epoch: 2/20, step: 92180, training_loss: 1.64116
Epoch: 2/20, step: 92200, training_loss: 1.86199
Epoch: 2/20, step: 92220, training_loss: 2.09020
Epoch: 2/20, step: 92240, training_loss: 2.60544
Epoch: 2/20, step: 92260, training_loss: 2.40553
Epoch: 2/20, step: 92280, training_loss: 1.86596
Epoch: 2/20, step: 92300, training_loss: 1.88494
Epoch: 2/20, step: 92320, training_loss: 3.47245
Epoch: 2/20, step: 92340, training_loss: 1.75246
Epoch: 2/20, step: 92360, training_loss: 1.60361
Epoch: 2/20, step: 92380, training_loss: 1.75237
Epoch: 2/20, step: 92400, training_loss: 2.53871
Epoch: 2/20, step: 92420, training_loss: 2.39863
Epoch: 2/20, step: 92440, training_loss: 2.66986
Epoch: 2/20, step: 92460, training_loss: 2.26943
Epoch: 2/20, step: 92480, training_loss: 2.77561
Epoch: 2/20, step: 92500, training_loss: 1.60534
Epoch: 2/20, step: 92520, training_loss: 2.98011
Epoch: 2/20, step: 92540, training_loss: 2.80254
Epoch: 2/20, step: 92560, training_loss: 2.06177
Epoch: 2/20, step: 92580, training_loss: 2.28234
Epoch: 2/20, step: 92600, training_loss: 2.48033
Epoch: 2/20, step: 92620, training_loss: 1.81692
Epoch: 2/20, step: 92640, training_loss: 1.73375
Epoch: 2/20, step: 92660, training_loss: 2.21330
Epoch: 2/20, step: 92680, training_loss: 2.02504
Epoch: 2/20, step: 92700, training_loss: 3.25704
Epoch: 2/20, step: 92720, training_loss: 2.36503
Epoch: 2/20, step: 92740, training_loss: 2.45207
Epoch: 2/20, step: 92760, training_loss: 2.17003
Epoch: 2/20, step: 92780, training_loss: 1.81261
Epoch: 2/20, step: 92800, training_loss: 2.72709
Epoch: 2/20, step: 92820, training_loss: 2.05250
Epoch: 2/20, step: 92840, training_loss: 2.11403
Epoch: 2/20, step: 92860, training_loss: 2.64344
Epoch: 2/20, step: 92880, training_loss: 2.64586
Epoch: 2/20, step: 92900, training_loss: 3.23791
Epoch: 2/20, step: 92920, training_loss: 1.66740
Epoch: 2/20, step: 92940, training_loss: 2.69599
Epoch: 2/20, step: 92960, training_loss: 2.01162
Epoch: 2/20, step: 92980, training_loss: 2.32939
Epoch: 2/20, step: 93000, training_loss: 2.15861
accuracy: 0.36, validation_loss: 2.4700422286987305, num_samples: 100
Epoch: 2/20, step: 93020, training_loss: 1.72022
Epoch: 2/20, step: 93040, training_loss: 1.74584
Epoch: 2/20, step: 93060, training_loss: 2.12519
Epoch: 2/20, step: 93080, training_loss: 2.23601
Epoch: 2/20, step: 93100, training_loss: 1.32399
Epoch: 2/20, step: 93120, training_loss: 2.54277
Epoch: 2/20, step: 93140, training_loss: 1.67819
Epoch: 2/20, step: 93160, training_loss: 2.16241
Epoch: 2/20, step: 93180, training_loss: 2.20649
Epoch: 2/20, step: 93200, training_loss: 2.33575
Epoch: 2/20, step: 93220, training_loss: 2.57093
Epoch: 2/20, step: 93240, training_loss: 1.82466
Epoch: 2/20, step: 93260, training_loss: 2.23175
Epoch: 2/20, step: 93280, training_loss: 1.28938
Epoch: 2/20, step: 93300, training_loss: 2.20012
Epoch: 2/20, step: 93320, training_loss: 1.70558
Epoch: 2/20, step: 93340, training_loss: 2.29642
Epoch: 2/20, step: 93360, training_loss: 2.47219
Epoch: 2/20, step: 93380, training_loss: 2.36577
Epoch: 2/20, step: 93400, training_loss: 2.39656
Epoch: 2/20, step: 93420, training_loss: 2.59860
Epoch: 2/20, step: 93440, training_loss: 2.16854
Epoch: 2/20, step: 93460, training_loss: 1.83281
Epoch: 2/20, step: 93480, training_loss: 1.90929
Epoch: 2/20, step: 93500, training_loss: 2.25997
Epoch: 2/20, step: 93520, training_loss: 1.36615
Epoch: 2/20, step: 93540, training_loss: 2.00195
Epoch: 2/20, step: 93560, training_loss: 1.87451
Epoch: 2/20, step: 93580, training_loss: 0.82453
Epoch: 2/20, step: 93600, training_loss: 2.26825
Epoch: 2/20, step: 93620, training_loss: 1.63194
Epoch: 2/20, step: 93640, training_loss: 1.87897
Epoch: 2/20, step: 93660, training_loss: 2.59251
Epoch: 2/20, step: 93680, training_loss: 1.37113
Epoch: 2/20, step: 93700, training_loss: 2.51431
Epoch: 2/20, step: 93720, training_loss: 1.76954
Epoch: 2/20, step: 93740, training_loss: 2.77512
Epoch: 2/20, step: 93760, training_loss: 1.96223
Epoch: 2/20, step: 93780, training_loss: 2.70232
Epoch: 2/20, step: 93800, training_loss: 1.41038
Epoch: 2/20, step: 93820, training_loss: 1.93129
Epoch: 2/20, step: 93840, training_loss: 2.32067
Epoch: 2/20, step: 93860, training_loss: 2.44590
Epoch: 2/20, step: 93880, training_loss: 2.41488
Epoch: 2/20, step: 93900, training_loss: 2.14237
Epoch: 2/20, step: 93920, training_loss: 2.16219
Epoch: 2/20, step: 93940, training_loss: 2.38559
Epoch: 2/20, step: 93960, training_loss: 1.95957
Epoch: 2/20, step: 93980, training_loss: 1.59737
Epoch: 2/20, step: 94000, training_loss: 2.37601
accuracy: 0.36, validation_loss: 2.515265464782715, num_samples: 100
Epoch: 2/20, step: 94020, training_loss: 2.18241
Epoch: 2/20, step: 94040, training_loss: 2.45631
Epoch: 2/20, step: 94060, training_loss: 2.10838
Epoch: 2/20, step: 94080, training_loss: 2.01076
Epoch: 2/20, step: 94100, training_loss: 1.65893
Epoch: 2/20, step: 94120, training_loss: 1.62612
Epoch: 2/20, step: 94140, training_loss: 3.09565
Epoch: 2/20, step: 94160, training_loss: 1.62988
Epoch: 2/20, step: 94180, training_loss: 2.39439
Epoch: 2/20, step: 94200, training_loss: 2.17309
Epoch: 2/20, step: 94220, training_loss: 1.39560
Epoch: 2/20, step: 94240, training_loss: 1.85063
Epoch: 2/20, step: 94260, training_loss: 2.47134
Epoch: 2/20, step: 94280, training_loss: 3.07554
Epoch: 2/20, step: 94300, training_loss: 2.17557
Epoch: 2/20, step: 94320, training_loss: 2.55817
Epoch: 2/20, step: 94340, training_loss: 2.26740
Epoch: 2/20, step: 94360, training_loss: 1.86652
Epoch: 2/20, step: 94380, training_loss: 2.03067
Epoch: 2/20, step: 94400, training_loss: 2.04194
Epoch: 2/20, step: 94420, training_loss: 1.85218
Epoch: 2/20, step: 94440, training_loss: 1.90147
Epoch: 2/20, step: 94460, training_loss: 2.30940
Epoch: 2/20, step: 94480, training_loss: 1.39112
Epoch: 2/20, step: 94500, training_loss: 2.07101
Epoch: 2/20, step: 94520, training_loss: 2.06107
Epoch: 2/20, step: 94540, training_loss: 2.00296
Epoch: 2/20, step: 94560, training_loss: 1.77589
Epoch: 2/20, step: 94580, training_loss: 1.73475
Epoch: 2/20, step: 94600, training_loss: 1.19536
Epoch: 2/20, step: 94620, training_loss: 2.71560
Epoch: 2/20, step: 94640, training_loss: 1.99871
Epoch: 2/20, step: 94660, training_loss: 2.47995
Epoch: 2/20, step: 94680, training_loss: 3.09002
Epoch: 2/20, step: 94700, training_loss: 1.38270
Epoch: 2/20, step: 94720, training_loss: 2.40621
Epoch: 2/20, step: 94740, training_loss: 2.38793
Epoch: 2/20, step: 94760, training_loss: 2.76737
Epoch: 2/20, step: 94780, training_loss: 1.72246
Epoch: 2/20, step: 94800, training_loss: 2.28428
Epoch: 2/20, step: 94820, training_loss: 2.15019
Epoch: 2/20, step: 94840, training_loss: 1.90835
Epoch: 2/20, step: 94860, training_loss: 1.57028
Epoch: 2/20, step: 94880, training_loss: 2.27093
Epoch: 2/20, step: 94900, training_loss: 2.25216
Epoch: 2/20, step: 94920, training_loss: 2.37388
Epoch: 2/20, step: 94940, training_loss: 1.40414
Epoch: 2/20, step: 94960, training_loss: 2.29359
Epoch: 2/20, step: 94980, training_loss: 1.67560
Epoch: 2/20, step: 95000, training_loss: 2.62731
accuracy: 0.35, validation_loss: 2.228039503097534, num_samples: 100
Epoch: 2/20, step: 95020, training_loss: 2.89086
Epoch: 2/20, step: 95040, training_loss: 1.51174
Epoch: 2/20, step: 95060, training_loss: 1.90533
Epoch: 2/20, step: 95080, training_loss: 2.34375
Epoch: 2/20, step: 95100, training_loss: 2.43226
Epoch: 2/20, step: 95120, training_loss: 1.54084
Epoch: 2/20, step: 95140, training_loss: 2.58045
Epoch: 2/20, step: 95160, training_loss: 1.58733
Epoch: 2/20, step: 95180, training_loss: 2.53061
Epoch: 2/20, step: 95200, training_loss: 2.41183
Epoch: 2/20, step: 95220, training_loss: 2.42103
Epoch: 2/20, step: 95240, training_loss: 2.17220
Epoch: 2/20, step: 95260, training_loss: 2.26535
Epoch: 2/20, step: 95280, training_loss: 2.99595
Epoch: 2/20, step: 95300, training_loss: 2.28851
Epoch: 2/20, step: 95320, training_loss: 1.24670
Epoch: 2/20, step: 95340, training_loss: 2.14825
Epoch: 2/20, step: 95360, training_loss: 2.01882
Epoch: 2/20, step: 95380, training_loss: 2.21644
Epoch: 2/20, step: 95400, training_loss: 2.38519
Epoch: 2/20, step: 95420, training_loss: 1.37592
Epoch: 2/20, step: 95440, training_loss: 1.84505
Epoch: 2/20, step: 95460, training_loss: 2.57367
Epoch: 2/20, step: 95480, training_loss: 1.70975
Epoch: 2/20, step: 95500, training_loss: 2.15484
Epoch: 2/20, step: 95520, training_loss: 2.12637
Epoch: 2/20, step: 95540, training_loss: 2.35047
Epoch: 2/20, step: 95560, training_loss: 2.25334
Epoch: 2/20, step: 95580, training_loss: 1.49301
Epoch: 2/20, step: 95600, training_loss: 2.10865
Epoch: 2/20, step: 95620, training_loss: 2.39413
Epoch: 2/20, step: 95640, training_loss: 1.71281
Epoch: 2/20, step: 95660, training_loss: 1.78095
Epoch: 2/20, step: 95680, training_loss: 2.64009
Epoch: 2/20, step: 95700, training_loss: 1.65429
Epoch: 2/20, step: 95720, training_loss: 2.40133
Epoch: 2/20, step: 95740, training_loss: 2.46985
Epoch: 2/20, step: 95760, training_loss: 2.21407
Epoch: 2/20, step: 95780, training_loss: 2.59426
Epoch: 2/20, step: 95800, training_loss: 1.33416
Epoch: 2/20, step: 95820, training_loss: 2.71377
Epoch: 2/20, step: 95840, training_loss: 2.11664
Epoch: 2/20, step: 95860, training_loss: 1.99348
Epoch: 2/20, step: 95880, training_loss: 2.41582
Epoch: 2/20, step: 95900, training_loss: 3.00522
Epoch: 2/20, step: 95920, training_loss: 1.93739
Epoch: 2/20, step: 95940, training_loss: 1.73031
Epoch: 2/20, step: 95960, training_loss: 1.95362
Epoch: 2/20, step: 95980, training_loss: 2.33164
Epoch: 2/20, step: 96000, training_loss: 2.24691
accuracy: 0.41, validation_loss: 2.0300889015197754, num_samples: 100
Epoch: 2/20, step: 96020, training_loss: 2.70964
Epoch: 2/20, step: 96040, training_loss: 2.18058
Epoch: 2/20, step: 96060, training_loss: 2.49496
Epoch: 2/20, step: 96080, training_loss: 2.33944
Epoch: 2/20, step: 96100, training_loss: 1.82382
Epoch: 2/20, step: 96120, training_loss: 2.64739
Epoch: 2/20, step: 96140, training_loss: 1.13408
Epoch: 2/20, step: 96160, training_loss: 1.96029
Epoch: 2/20, step: 96180, training_loss: 2.42335
Epoch: 2/20, step: 96200, training_loss: 1.84661
Epoch: 2/20, step: 96220, training_loss: 2.69516
Epoch: 2/20, step: 96240, training_loss: 1.57143
Epoch: 2/20, step: 96260, training_loss: 3.31848
Epoch: 2/20, step: 96280, training_loss: 2.38794
Epoch: 2/20, step: 96300, training_loss: 2.88496
Epoch: 2/20, step: 96320, training_loss: 2.18730
Epoch: 2/20, step: 96340, training_loss: 2.46157
Epoch: 2/20, step: 96360, training_loss: 2.40855
Epoch: 2/20, step: 96380, training_loss: 2.53848
Epoch: 2/20, step: 96400, training_loss: 2.02106
Epoch: 2/20, step: 96420, training_loss: 2.26155
Epoch: 2/20, step: 96440, training_loss: 2.10212
Epoch: 2/20, step: 96460, training_loss: 2.38885
Epoch: 2/20, step: 96480, training_loss: 1.47796
Epoch: 2/20, step: 96500, training_loss: 1.97322
Epoch: 2/20, step: 96520, training_loss: 1.97927
Epoch: 2/20, step: 96540, training_loss: 2.97163
Epoch: 2/20, step: 96560, training_loss: 2.10717
Epoch: 2/20, step: 96580, training_loss: 1.97911
Epoch: 2/20, step: 96600, training_loss: 2.79299
Epoch: 2/20, step: 96620, training_loss: 2.48708
Epoch: 2/20, step: 96640, training_loss: 2.57936
Epoch: 2/20, step: 96660, training_loss: 1.78160
Epoch: 2/20, step: 96680, training_loss: 2.47072
Epoch: 2/20, step: 96700, training_loss: 2.46746
Epoch: 2/20, step: 96720, training_loss: 2.34148
Epoch: 2/20, step: 96740, training_loss: 2.10170
Epoch: 2/20, step: 96760, training_loss: 2.62348
Epoch: 2/20, step: 96780, training_loss: 1.60770
Epoch: 2/20, step: 96800, training_loss: 2.93605
Epoch: 2/20, step: 96820, training_loss: 1.81574
Epoch: 2/20, step: 96840, training_loss: 3.44650
Epoch: 2/20, step: 96860, training_loss: 3.13602
Epoch: 2/20, step: 96880, training_loss: 2.03241
Epoch: 2/20, step: 96900, training_loss: 2.14256
Epoch: 2/20, step: 96920, training_loss: 2.87360
Epoch: 2/20, step: 96940, training_loss: 1.58999
Epoch: 2/20, step: 96960, training_loss: 1.30763
Epoch: 2/20, step: 96980, training_loss: 1.85367
Epoch: 2/20, step: 97000, training_loss: 1.91116
accuracy: 0.43, validation_loss: 2.0795063972473145, num_samples: 100
Epoch: 2/20, step: 97020, training_loss: 1.68473
Epoch: 2/20, step: 97040, training_loss: 1.78605
Epoch: 2/20, step: 97060, training_loss: 2.00779
Epoch: 2/20, step: 97080, training_loss: 1.75588
Epoch: 2/20, step: 97100, training_loss: 1.43501
Epoch: 2/20, step: 97120, training_loss: 3.11349
Epoch: 2/20, step: 97140, training_loss: 2.13717
Epoch: 2/20, step: 97160, training_loss: 2.44216
Epoch: 2/20, step: 97180, training_loss: 2.15975
Epoch: 2/20, step: 97200, training_loss: 3.03389
Epoch: 2/20, step: 97220, training_loss: 1.96010
Epoch: 2/20, step: 97240, training_loss: 2.29443
Epoch: 2/20, step: 97260, training_loss: 2.61309
Epoch: 2/20, step: 97280, training_loss: 1.54442
Epoch: 2/20, step: 97300, training_loss: 2.50271
Epoch: 2/20, step: 97320, training_loss: 2.22495
Epoch: 2/20, step: 97340, training_loss: 2.34083
Epoch: 2/20, step: 97360, training_loss: 1.00528
Epoch: 2/20, step: 97380, training_loss: 2.28605
Epoch: 2/20, step: 97400, training_loss: 1.88790
Epoch: 2/20, step: 97420, training_loss: 2.53969
Epoch: 2/20, step: 97440, training_loss: 1.68373
Epoch: 2/20, step: 97460, training_loss: 2.02362
Epoch: 2/20, step: 97480, training_loss: 1.73873
Epoch: 2/20, step: 97500, training_loss: 2.70272
Epoch: 2/20, step: 97520, training_loss: 2.03520
Epoch: 2/20, step: 97540, training_loss: 2.77595
Epoch: 2/20, step: 97560, training_loss: 2.47856
Epoch: 2/20, step: 97580, training_loss: 2.11895
Epoch: 2/20, step: 97600, training_loss: 1.85317
Epoch: 2/20, step: 97620, training_loss: 2.07860
Epoch: 2/20, step: 97640, training_loss: 1.76950
Epoch: 2/20, step: 97660, training_loss: 2.71468
Epoch: 2/20, step: 97680, training_loss: 1.85767
Epoch: 2/20, step: 97700, training_loss: 2.50812
Epoch: 2/20, step: 97720, training_loss: 2.60119
Epoch: 2/20, step: 97740, training_loss: 2.81148
Epoch: 2/20, step: 97760, training_loss: 1.77884
Epoch: 2/20, step: 97780, training_loss: 2.14069
Epoch: 2/20, step: 97800, training_loss: 1.54398
Epoch: 2/20, step: 97820, training_loss: 2.37817
Epoch: 2/20, step: 97840, training_loss: 1.68412
Epoch: 2/20, step: 97860, training_loss: 2.86623
Epoch: 2/20, step: 97880, training_loss: 2.38608
Epoch: 2/20, step: 97900, training_loss: 2.26699
Epoch: 2/20, step: 97920, training_loss: 1.94270
Epoch: 2/20, step: 97940, training_loss: 2.55287
Epoch: 2/20, step: 97960, training_loss: 1.85499
Epoch: 2/20, step: 97980, training_loss: 1.56430
Epoch: 2/20, step: 98000, training_loss: 1.13669
accuracy: 0.4, validation_loss: 2.2267000675201416, num_samples: 100
Epoch: 2/20, step: 98020, training_loss: 2.95823
Epoch: 2/20, step: 98040, training_loss: 2.34181
Epoch: 2/20, step: 98060, training_loss: 2.08361
Epoch: 2/20, step: 98080, training_loss: 1.76308
Epoch: 2/20, step: 98100, training_loss: 2.12075
Epoch: 2/20, step: 98120, training_loss: 2.77041
Epoch: 2/20, step: 98140, training_loss: 2.05542
Epoch: 2/20, step: 98160, training_loss: 1.30270
Epoch: 2/20, step: 98180, training_loss: 1.76738
Epoch: 2/20, step: 98200, training_loss: 1.87923
Epoch: 2/20, step: 98220, training_loss: 2.56538
Epoch: 2/20, step: 98240, training_loss: 2.32309
Epoch: 2/20, step: 98260, training_loss: 2.20291
Epoch: 2/20, step: 98280, training_loss: 1.82597
Epoch: 2/20, step: 98300, training_loss: 2.65276
Epoch: 2/20, step: 98320, training_loss: 2.49366
Epoch: 2/20, step: 98340, training_loss: 1.63547
Epoch: 2/20, step: 98360, training_loss: 2.56797
Epoch: 2/20, step: 98380, training_loss: 2.60726
Epoch: 2/20, step: 98400, training_loss: 1.95847
Epoch: 2/20, step: 98420, training_loss: 1.76153
Epoch: 2/20, step: 98440, training_loss: 1.92993
Epoch: 2/20, step: 98460, training_loss: 2.12416
Epoch: 2/20, step: 98480, training_loss: 1.84689
Epoch: 2/20, step: 98500, training_loss: 1.74586
Epoch: 2/20, step: 98520, training_loss: 2.52063
Epoch: 2/20, step: 98540, training_loss: 1.76713
Epoch: 2/20, step: 98560, training_loss: 2.02804
Epoch: 2/20, step: 98580, training_loss: 1.73242
Epoch: 2/20, step: 98600, training_loss: 2.25877
Epoch: 2/20, step: 98620, training_loss: 1.53022
Epoch: 2/20, step: 98640, training_loss: 1.54749
Epoch: 2/20, step: 98660, training_loss: 2.31631
Epoch: 2/20, step: 98680, training_loss: 1.98363
Epoch: 2/20, step: 98700, training_loss: 2.39740
Epoch: 2/20, step: 98720, training_loss: 2.34883
Epoch: 2/20, step: 98740, training_loss: 2.41787
Epoch: 2/20, step: 98760, training_loss: 2.49701
Epoch: 2/20, step: 98780, training_loss: 1.64231
Epoch: 2/20, step: 98800, training_loss: 2.26426
Epoch: 2/20, step: 98820, training_loss: 2.99338
Epoch: 2/20, step: 98840, training_loss: 2.27664
Epoch: 2/20, step: 98860, training_loss: 2.20039
Epoch: 2/20, step: 98880, training_loss: 2.39354
Epoch: 2/20, step: 98900, training_loss: 1.72557
Epoch: 2/20, step: 98920, training_loss: 1.25429
Epoch: 2/20, step: 98940, training_loss: 0.98811
Epoch: 2/20, step: 98960, training_loss: 2.14282
Epoch: 2/20, step: 98980, training_loss: 1.91773
Epoch: 2/20, step: 99000, training_loss: 2.46452
accuracy: 0.35, validation_loss: 2.2757067680358887, num_samples: 100
Epoch: 2/20, step: 99020, training_loss: 2.10951
Epoch: 2/20, step: 99040, training_loss: 1.92469
Epoch: 2/20, step: 99060, training_loss: 1.90348
Epoch: 2/20, step: 99080, training_loss: 3.01446
Epoch: 2/20, step: 99100, training_loss: 1.72714
Epoch: 2/20, step: 99120, training_loss: 2.24570
Epoch: 2/20, step: 99140, training_loss: 2.58887
Epoch: 2/20, step: 99160, training_loss: 1.91762
Epoch: 2/20, step: 99180, training_loss: 2.62394
Epoch: 2/20, step: 99200, training_loss: 2.78832
Epoch: 2/20, step: 99220, training_loss: 1.89004
Epoch: 2/20, step: 99240, training_loss: 2.46762
Epoch: 2/20, step: 99260, training_loss: 2.65422
Epoch: 2/20, step: 99280, training_loss: 2.34185
Epoch: 2/20, step: 99300, training_loss: 2.16106
Epoch: 2/20, step: 99320, training_loss: 3.18800
Epoch: 2/20, step: 99340, training_loss: 2.33344
Epoch: 2/20, step: 99360, training_loss: 2.04256
Epoch: 2/20, step: 99380, training_loss: 1.85563
Epoch: 2/20, step: 99400, training_loss: 2.62026
Epoch: 2/20, step: 99420, training_loss: 2.19066
Epoch: 2/20, step: 99440, training_loss: 2.84887
Epoch: 2/20, step: 99460, training_loss: 2.50580
Epoch: 2/20, step: 99480, training_loss: 3.10249
Epoch: 2/20, step: 99500, training_loss: 2.89139
Epoch: 2/20, step: 99520, training_loss: 2.45452
Epoch: 2/20, step: 99540, training_loss: 1.81515
Epoch: 2/20, step: 99560, training_loss: 1.99138
Epoch: 2/20, step: 99580, training_loss: 1.62675
Epoch: 2/20, step: 99600, training_loss: 1.57456
Epoch: 2/20, step: 99620, training_loss: 2.25909
Epoch: 2/20, step: 99640, training_loss: 2.44534
Epoch: 2/20, step: 99660, training_loss: 2.05549
Epoch: 2/20, step: 99680, training_loss: 1.80990
Epoch: 2/20, step: 99700, training_loss: 2.20019
Epoch: 2/20, step: 99720, training_loss: 2.48625
Epoch: 2/20, step: 99740, training_loss: 2.44687
Epoch: 2/20, step: 99760, training_loss: 2.12986
Epoch: 2/20, step: 99780, training_loss: 2.39760
Epoch: 2/20, step: 99800, training_loss: 2.51532
Epoch: 2/20, step: 99820, training_loss: 2.36663
Epoch: 2/20, step: 99840, training_loss: 2.39078
Epoch: 2/20, step: 99860, training_loss: 2.69437
Epoch: 2/20, step: 99880, training_loss: 2.23481
Epoch: 2/20, step: 99900, training_loss: 1.89166
Epoch: 2/20, step: 99920, training_loss: 2.56433
Epoch: 2/20, step: 99940, training_loss: 3.14769
Epoch: 2/20, step: 99960, training_loss: 2.46965
Epoch: 2/20, step: 99980, training_loss: 2.22607
Epoch: 2/20, step: 100000, training_loss: 2.14972
accuracy: 0.54, validation_loss: 1.8075451850891113, num_samples: 100
Epoch: 2/20, step: 100020, training_loss: 2.19527
Epoch: 2/20, step: 100040, training_loss: 1.98363
Epoch: 2/20, step: 100060, training_loss: 2.69033
Epoch: 2/20, step: 100080, training_loss: 1.53257
Epoch: 2/20, step: 100100, training_loss: 2.38922
Epoch: 2/20, step: 100120, training_loss: 2.24682
Epoch: 2/20, step: 100140, training_loss: 1.65563
Epoch: 2/20, step: 100160, training_loss: 1.78842
Epoch: 2/20, step: 100180, training_loss: 1.55120
Epoch: 2/20, step: 100200, training_loss: 1.70023
Epoch: 2/20, step: 100220, training_loss: 1.58327
Epoch: 2/20, step: 100240, training_loss: 1.31967
Epoch: 2/20, step: 100260, training_loss: 1.98248
Epoch: 2/20, step: 100280, training_loss: 2.03677
Epoch: 2/20, step: 100300, training_loss: 1.56993
Epoch: 2/20, step: 100320, training_loss: 1.80250
Epoch: 2/20, step: 100340, training_loss: 1.82540
Epoch: 2/20, step: 100360, training_loss: 1.77667
Epoch: 2/20, step: 100380, training_loss: 2.57257
Epoch: 2/20, step: 100400, training_loss: 1.89616
Epoch: 2/20, step: 100420, training_loss: 2.61851
Epoch: 2/20, step: 100440, training_loss: 1.80333
Epoch: 2/20, step: 100460, training_loss: 1.38706
Epoch: 2/20, step: 100480, training_loss: 1.63692
Epoch: 2/20, step: 100500, training_loss: 2.46952
Epoch: 2/20, step: 100520, training_loss: 2.45249
Epoch: 2/20, step: 100540, training_loss: 1.41346
Epoch: 2/20, step: 100560, training_loss: 2.03104
Epoch: 2/20, step: 100580, training_loss: 2.09669
Epoch: 2/20, step: 100600, training_loss: 3.02054
Epoch: 2/20, step: 100620, training_loss: 2.40769
Epoch: 2/20, step: 100640, training_loss: 2.38007
Epoch: 2/20, step: 100660, training_loss: 1.38979
Epoch: 2/20, step: 100680, training_loss: 2.96936
Epoch: 2/20, step: 100700, training_loss: 2.41626
Epoch: 2/20, step: 100720, training_loss: 2.32136
Epoch: 2/20, step: 100740, training_loss: 2.11499
Epoch: 2/20, step: 100760, training_loss: 2.55693
Epoch: 2/20, step: 100780, training_loss: 2.36310
Epoch: 2/20, step: 100800, training_loss: 1.87111
Epoch: 2/20, step: 100820, training_loss: 2.58704
Epoch: 2/20, step: 100840, training_loss: 2.03473
Epoch: 2/20, step: 100860, training_loss: 2.69878
Epoch: 2/20, step: 100880, training_loss: 2.08548
Epoch: 2/20, step: 100900, training_loss: 2.35113
Epoch: 2/20, step: 100920, training_loss: 2.41533
Epoch: 2/20, step: 100940, training_loss: 2.50428
Epoch: 2/20, step: 100960, training_loss: 1.39304
Epoch: 2/20, step: 100980, training_loss: 2.68231
Epoch: 3/20, step: 20, training_loss: 1.46095
Epoch: 3/20, step: 40, training_loss: 1.80295
Epoch: 3/20, step: 60, training_loss: 1.86974
Epoch: 3/20, step: 80, training_loss: 1.79454
Epoch: 3/20, step: 100, training_loss: 2.32274
Epoch: 3/20, step: 120, training_loss: 1.47064
Epoch: 3/20, step: 140, training_loss: 1.90138
Epoch: 3/20, step: 160, training_loss: 1.93409
Epoch: 3/20, step: 180, training_loss: 1.25510
Epoch: 3/20, step: 200, training_loss: 2.02474
Epoch: 3/20, step: 220, training_loss: 1.40391
Epoch: 3/20, step: 240, training_loss: 2.57781
Epoch: 3/20, step: 260, training_loss: 1.50681
Epoch: 3/20, step: 280, training_loss: 2.30116
Epoch: 3/20, step: 300, training_loss: 2.13796
Epoch: 3/20, step: 320, training_loss: 1.69542
Epoch: 3/20, step: 340, training_loss: 1.01095
Epoch: 3/20, step: 360, training_loss: 2.73638
Epoch: 3/20, step: 380, training_loss: 2.98883
Epoch: 3/20, step: 400, training_loss: 2.97646
Epoch: 3/20, step: 420, training_loss: 2.98451
Epoch: 3/20, step: 440, training_loss: 1.73596
Epoch: 3/20, step: 460, training_loss: 2.60568
Epoch: 3/20, step: 480, training_loss: 2.14627
Epoch: 3/20, step: 500, training_loss: 2.63967
Epoch: 3/20, step: 520, training_loss: 2.00110
Epoch: 3/20, step: 540, training_loss: 1.63248
Epoch: 3/20, step: 560, training_loss: 1.89109
Epoch: 3/20, step: 580, training_loss: 2.54875
Epoch: 3/20, step: 600, training_loss: 2.43812
Epoch: 3/20, step: 620, training_loss: 2.06223
Epoch: 3/20, step: 640, training_loss: 1.38650
Epoch: 3/20, step: 660, training_loss: 2.23929
Epoch: 3/20, step: 680, training_loss: 1.89340
Epoch: 3/20, step: 700, training_loss: 2.95530
Epoch: 3/20, step: 720, training_loss: 2.03103
Epoch: 3/20, step: 740, training_loss: 1.40261
Epoch: 3/20, step: 760, training_loss: 2.08041
Epoch: 3/20, step: 780, training_loss: 2.26123
Epoch: 3/20, step: 800, training_loss: 2.68393
Epoch: 3/20, step: 820, training_loss: 1.73877
Epoch: 3/20, step: 840, training_loss: 2.83881
Epoch: 3/20, step: 860, training_loss: 2.62702
Epoch: 3/20, step: 880, training_loss: 2.25191
Epoch: 3/20, step: 900, training_loss: 2.12304
Epoch: 3/20, step: 920, training_loss: 2.34630
Epoch: 3/20, step: 940, training_loss: 2.85164
Epoch: 3/20, step: 960, training_loss: 2.61951
Epoch: 3/20, step: 980, training_loss: 1.90003
Epoch: 3/20, step: 1000, training_loss: 2.70321
accuracy: 0.32, validation_loss: 2.3699212074279785, num_samples: 100
Epoch: 3/20, step: 1020, training_loss: 2.23584
Epoch: 3/20, step: 1040, training_loss: 1.26802
Epoch: 3/20, step: 1060, training_loss: 2.37861
Epoch: 3/20, step: 1080, training_loss: 1.48381
Epoch: 3/20, step: 1100, training_loss: 2.42570
Epoch: 3/20, step: 1120, training_loss: 2.51752
Epoch: 3/20, step: 1140, training_loss: 2.58739
Epoch: 3/20, step: 1160, training_loss: 1.76572
Epoch: 3/20, step: 1180, training_loss: 1.18444
Epoch: 3/20, step: 1200, training_loss: 3.27362
Epoch: 3/20, step: 1220, training_loss: 2.15698
Epoch: 3/20, step: 1240, training_loss: 2.10836
Epoch: 3/20, step: 1260, training_loss: 1.63176
Epoch: 3/20, step: 1280, training_loss: 1.69403
Epoch: 3/20, step: 1300, training_loss: 2.32291
Epoch: 3/20, step: 1320, training_loss: 2.59636
Epoch: 3/20, step: 1340, training_loss: 1.37511
Epoch: 3/20, step: 1360, training_loss: 1.59440
Epoch: 3/20, step: 1380, training_loss: 2.03849
Epoch: 3/20, step: 1400, training_loss: 2.01852
Epoch: 3/20, step: 1420, training_loss: 1.56915
Epoch: 3/20, step: 1440, training_loss: 2.86436
Epoch: 3/20, step: 1460, training_loss: 1.99072
Epoch: 3/20, step: 1480, training_loss: 2.26764
Epoch: 3/20, step: 1500, training_loss: 2.19975
Epoch: 3/20, step: 1520, training_loss: 2.30168
Epoch: 3/20, step: 1540, training_loss: 1.63245
Epoch: 3/20, step: 1560, training_loss: 1.60426
Epoch: 3/20, step: 1580, training_loss: 2.34443
Epoch: 3/20, step: 1600, training_loss: 1.73241
Epoch: 3/20, step: 1620, training_loss: 2.34157
Epoch: 3/20, step: 1640, training_loss: 1.84670
Epoch: 3/20, step: 1660, training_loss: 3.18718
Epoch: 3/20, step: 1680, training_loss: 2.65254
Epoch: 3/20, step: 1700, training_loss: 2.72709
Epoch: 3/20, step: 1720, training_loss: 3.24598
Epoch: 3/20, step: 1740, training_loss: 2.52848
Epoch: 3/20, step: 1760, training_loss: 2.65452
Epoch: 3/20, step: 1780, training_loss: 1.52017
Epoch: 3/20, step: 1800, training_loss: 2.15860
Epoch: 3/20, step: 1820, training_loss: 2.19937
Epoch: 3/20, step: 1840, training_loss: 2.38478
Epoch: 3/20, step: 1860, training_loss: 1.79810
Epoch: 3/20, step: 1880, training_loss: 3.29016
Epoch: 3/20, step: 1900, training_loss: 2.23859
Epoch: 3/20, step: 1920, training_loss: 3.05876
Epoch: 3/20, step: 1940, training_loss: 2.36188
Epoch: 3/20, step: 1960, training_loss: 2.44609
Epoch: 3/20, step: 1980, training_loss: 1.51198
Epoch: 3/20, step: 2000, training_loss: 1.76724
accuracy: 0.44, validation_loss: 2.1871745586395264, num_samples: 100
Epoch: 3/20, step: 2020, training_loss: 2.00536
Epoch: 3/20, step: 2040, training_loss: 1.68165
Epoch: 3/20, step: 2060, training_loss: 2.52190
Epoch: 3/20, step: 2080, training_loss: 1.88422
Epoch: 3/20, step: 2100, training_loss: 2.33870
Epoch: 3/20, step: 2120, training_loss: 2.39182
Epoch: 3/20, step: 2140, training_loss: 2.03395
Epoch: 3/20, step: 2160, training_loss: 2.02678
Epoch: 3/20, step: 2180, training_loss: 1.95268
Epoch: 3/20, step: 2200, training_loss: 1.42202
Epoch: 3/20, step: 2220, training_loss: 1.83245
Epoch: 3/20, step: 2240, training_loss: 2.72900
Epoch: 3/20, step: 2260, training_loss: 2.37713
Epoch: 3/20, step: 2280, training_loss: 3.05157
Epoch: 3/20, step: 2300, training_loss: 2.25566
Epoch: 3/20, step: 2320, training_loss: 2.01313
Epoch: 3/20, step: 2340, training_loss: 2.19258
Epoch: 3/20, step: 2360, training_loss: 1.32472
Epoch: 3/20, step: 2380, training_loss: 2.62660
Epoch: 3/20, step: 2400, training_loss: 2.17349
Epoch: 3/20, step: 2420, training_loss: 2.83242
Epoch: 3/20, step: 2440, training_loss: 1.70805
Epoch: 3/20, step: 2460, training_loss: 1.71894
Epoch: 3/20, step: 2480, training_loss: 2.16735
Epoch: 3/20, step: 2500, training_loss: 2.34931
Epoch: 3/20, step: 2520, training_loss: 2.88564
Epoch: 3/20, step: 2540, training_loss: 2.43070
Epoch: 3/20, step: 2560, training_loss: 2.49515
Epoch: 3/20, step: 2580, training_loss: 1.61919
Epoch: 3/20, step: 2600, training_loss: 2.40594
Epoch: 3/20, step: 2620, training_loss: 2.36276
Epoch: 3/20, step: 2640, training_loss: 1.69110
Epoch: 3/20, step: 2660, training_loss: 2.56157
Epoch: 3/20, step: 2680, training_loss: 1.71581
Epoch: 3/20, step: 2700, training_loss: 2.61571
Epoch: 3/20, step: 2720, training_loss: 2.17416
Epoch: 3/20, step: 2740, training_loss: 2.08082
Epoch: 3/20, step: 2760, training_loss: 1.65099
Epoch: 3/20, step: 2780, training_loss: 2.49232
Epoch: 3/20, step: 2800, training_loss: 1.94202
Epoch: 3/20, step: 2820, training_loss: 2.51719
Epoch: 3/20, step: 2840, training_loss: 2.51243
Epoch: 3/20, step: 2860, training_loss: 2.18336
Epoch: 3/20, step: 2880, training_loss: 2.55628
Epoch: 3/20, step: 2900, training_loss: 2.52632
Epoch: 3/20, step: 2920, training_loss: 2.18131
Epoch: 3/20, step: 2940, training_loss: 2.40128
Epoch: 3/20, step: 2960, training_loss: 2.11493
Epoch: 3/20, step: 2980, training_loss: 1.65557
Epoch: 3/20, step: 3000, training_loss: 1.81001
accuracy: 0.41, validation_loss: 2.378031015396118, num_samples: 100
Epoch: 3/20, step: 3020, training_loss: 2.33769
Epoch: 3/20, step: 3040, training_loss: 2.05910
Epoch: 3/20, step: 3060, training_loss: 2.60589
Epoch: 3/20, step: 3080, training_loss: 1.49425
Epoch: 3/20, step: 3100, training_loss: 1.70813
Epoch: 3/20, step: 3120, training_loss: 1.66105
Epoch: 3/20, step: 3140, training_loss: 2.02820
Epoch: 3/20, step: 3160, training_loss: 1.82910
Epoch: 3/20, step: 3180, training_loss: 2.04106
Epoch: 3/20, step: 3200, training_loss: 2.27332
Epoch: 3/20, step: 3220, training_loss: 2.39691
Epoch: 3/20, step: 3240, training_loss: 1.67524
Epoch: 3/20, step: 3260, training_loss: 3.22769
Epoch: 3/20, step: 3280, training_loss: 2.34076
Epoch: 3/20, step: 3300, training_loss: 1.78753
Epoch: 3/20, step: 3320, training_loss: 2.12196
Epoch: 3/20, step: 3340, training_loss: 1.88859
Epoch: 3/20, step: 3360, training_loss: 2.10356
Epoch: 3/20, step: 3380, training_loss: 2.54439
Epoch: 3/20, step: 3400, training_loss: 2.20571
Epoch: 3/20, step: 3420, training_loss: 1.80854
Epoch: 3/20, step: 3440, training_loss: 2.22945
Epoch: 3/20, step: 3460, training_loss: 1.64924
Epoch: 3/20, step: 3480, training_loss: 1.64361
Epoch: 3/20, step: 3500, training_loss: 3.20756
Epoch: 3/20, step: 3520, training_loss: 1.68416
Epoch: 3/20, step: 3540, training_loss: 1.48579
Epoch: 3/20, step: 3560, training_loss: 1.71417
Epoch: 3/20, step: 3580, training_loss: 1.96135
Epoch: 3/20, step: 3600, training_loss: 2.75144
Epoch: 3/20, step: 3620, training_loss: 1.93321
Epoch: 3/20, step: 3640, training_loss: 1.53652
Epoch: 3/20, step: 3660, training_loss: 2.83503
Epoch: 3/20, step: 3680, training_loss: 1.77980
Epoch: 3/20, step: 3700, training_loss: 2.11485
Epoch: 3/20, step: 3720, training_loss: 2.82003
Epoch: 3/20, step: 3740, training_loss: 1.88046
Epoch: 3/20, step: 3760, training_loss: 1.90287
Epoch: 3/20, step: 3780, training_loss: 2.19659
Epoch: 3/20, step: 3800, training_loss: 1.72740
Epoch: 3/20, step: 3820, training_loss: 1.68587
Epoch: 3/20, step: 3840, training_loss: 1.98978
Epoch: 3/20, step: 3860, training_loss: 2.35311
Epoch: 3/20, step: 3880, training_loss: 2.12192
Epoch: 3/20, step: 3900, training_loss: 1.59127
Epoch: 3/20, step: 3920, training_loss: 1.64418
Epoch: 3/20, step: 3940, training_loss: 2.84489
Epoch: 3/20, step: 3960, training_loss: 2.56285
Epoch: 3/20, step: 3980, training_loss: 1.40622
Epoch: 3/20, step: 4000, training_loss: 2.40989
accuracy: 0.39, validation_loss: 2.258780002593994, num_samples: 100
Epoch: 3/20, step: 4020, training_loss: 2.10678
Epoch: 3/20, step: 4040, training_loss: 1.71669
Epoch: 3/20, step: 4060, training_loss: 1.93802
Epoch: 3/20, step: 4080, training_loss: 2.82215
Epoch: 3/20, step: 4100, training_loss: 1.99600
Epoch: 3/20, step: 4120, training_loss: 1.46890
Epoch: 3/20, step: 4140, training_loss: 2.64898
Epoch: 3/20, step: 4160, training_loss: 0.94531
Epoch: 3/20, step: 4180, training_loss: 1.66998
Epoch: 3/20, step: 4200, training_loss: 2.82225
Epoch: 3/20, step: 4220, training_loss: 1.76985
Epoch: 3/20, step: 4240, training_loss: 1.98695
Epoch: 3/20, step: 4260, training_loss: 2.60664
Epoch: 3/20, step: 4280, training_loss: 1.74014
Epoch: 3/20, step: 4300, training_loss: 2.34159
Epoch: 3/20, step: 4320, training_loss: 2.04084
Epoch: 3/20, step: 4340, training_loss: 1.45703
Epoch: 3/20, step: 4360, training_loss: 2.00360
Epoch: 3/20, step: 4380, training_loss: 2.54325
Epoch: 3/20, step: 4400, training_loss: 1.20117
Epoch: 3/20, step: 4420, training_loss: 2.29770
Epoch: 3/20, step: 4440, training_loss: 2.82959
Epoch: 3/20, step: 4460, training_loss: 1.94751
Epoch: 3/20, step: 4480, training_loss: 2.14014
Epoch: 3/20, step: 4500, training_loss: 2.24818
Epoch: 3/20, step: 4520, training_loss: 3.08874
Epoch: 3/20, step: 4540, training_loss: 2.67287
Epoch: 3/20, step: 4560, training_loss: 2.12151
Epoch: 3/20, step: 4580, training_loss: 1.98684
Epoch: 3/20, step: 4600, training_loss: 2.26103
Epoch: 3/20, step: 4620, training_loss: 2.23760
Epoch: 3/20, step: 4640, training_loss: 2.42026
Epoch: 3/20, step: 4660, training_loss: 2.95101
Epoch: 3/20, step: 4680, training_loss: 2.73297
Epoch: 3/20, step: 4700, training_loss: 1.57974
Epoch: 3/20, step: 4720, training_loss: 1.88339
Epoch: 3/20, step: 4740, training_loss: 2.33070
Epoch: 3/20, step: 4760, training_loss: 2.53175
Epoch: 3/20, step: 4780, training_loss: 2.17930
Epoch: 3/20, step: 4800, training_loss: 1.97338
Epoch: 3/20, step: 4820, training_loss: 1.96544
Epoch: 3/20, step: 4840, training_loss: 2.38101
Epoch: 3/20, step: 4860, training_loss: 2.41138
Epoch: 3/20, step: 4880, training_loss: 1.75171
Epoch: 3/20, step: 4900, training_loss: 2.38556
Epoch: 3/20, step: 4920, training_loss: 2.46027
Epoch: 3/20, step: 4940, training_loss: 2.70851
Epoch: 3/20, step: 4960, training_loss: 1.72426
Epoch: 3/20, step: 4980, training_loss: 0.87215
Epoch: 3/20, step: 5000, training_loss: 1.95733
accuracy: 0.38, validation_loss: 2.2786877155303955, num_samples: 100
Epoch: 3/20, step: 5020, training_loss: 1.63762
Epoch: 3/20, step: 5040, training_loss: 2.23196
Epoch: 3/20, step: 5060, training_loss: 2.16238
Epoch: 3/20, step: 5080, training_loss: 2.47350
Epoch: 3/20, step: 5100, training_loss: 2.24515
Epoch: 3/20, step: 5120, training_loss: 2.52473
Epoch: 3/20, step: 5140, training_loss: 2.46955
Epoch: 3/20, step: 5160, training_loss: 2.50656
Epoch: 3/20, step: 5180, training_loss: 1.62494
Epoch: 3/20, step: 5200, training_loss: 1.79000
Epoch: 3/20, step: 5220, training_loss: 2.43156
Epoch: 3/20, step: 5240, training_loss: 2.27999
Epoch: 3/20, step: 5260, training_loss: 2.58750
Epoch: 3/20, step: 5280, training_loss: 2.52992
Epoch: 3/20, step: 5300, training_loss: 2.72385
Epoch: 3/20, step: 5320, training_loss: 2.20988
Epoch: 3/20, step: 5340, training_loss: 2.46179
Epoch: 3/20, step: 5360, training_loss: 2.27628
Epoch: 3/20, step: 5380, training_loss: 1.69666
Epoch: 3/20, step: 5400, training_loss: 2.81598
Epoch: 3/20, step: 5420, training_loss: 1.79978
Epoch: 3/20, step: 5440, training_loss: 2.41887
Epoch: 3/20, step: 5460, training_loss: 2.65275
Epoch: 3/20, step: 5480, training_loss: 1.45861
Epoch: 3/20, step: 5500, training_loss: 1.83906
Epoch: 3/20, step: 5520, training_loss: 2.84068
Epoch: 3/20, step: 5540, training_loss: 3.08478
Epoch: 3/20, step: 5560, training_loss: 2.74862
Epoch: 3/20, step: 5580, training_loss: 1.82721
Epoch: 3/20, step: 5600, training_loss: 1.68182
Epoch: 3/20, step: 5620, training_loss: 2.50884
Epoch: 3/20, step: 5640, training_loss: 0.87415
Epoch: 3/20, step: 5660, training_loss: 2.11187
Epoch: 3/20, step: 5680, training_loss: 2.15900
Epoch: 3/20, step: 5700, training_loss: 1.88035
Epoch: 3/20, step: 5720, training_loss: 2.90751
Epoch: 3/20, step: 5740, training_loss: 2.00884
Epoch: 3/20, step: 5760, training_loss: 1.94800
Epoch: 3/20, step: 5780, training_loss: 2.61204
Epoch: 3/20, step: 5800, training_loss: 2.81825
Epoch: 3/20, step: 5820, training_loss: 1.80411
Epoch: 3/20, step: 5840, training_loss: 2.87466
Epoch: 3/20, step: 5860, training_loss: 2.50747
Epoch: 3/20, step: 5880, training_loss: 1.35496
Epoch: 3/20, step: 5900, training_loss: 2.24518
Epoch: 3/20, step: 5920, training_loss: 1.68631
Epoch: 3/20, step: 5940, training_loss: 1.88617
Epoch: 3/20, step: 5960, training_loss: 2.32335
Epoch: 3/20, step: 5980, training_loss: 2.27190
Epoch: 3/20, step: 6000, training_loss: 2.11760
accuracy: 0.45, validation_loss: 2.030017137527466, num_samples: 100
Epoch: 3/20, step: 6020, training_loss: 1.44943
Epoch: 3/20, step: 6040, training_loss: 2.07589
Epoch: 3/20, step: 6060, training_loss: 1.72229
Epoch: 3/20, step: 6080, training_loss: 1.91002
Epoch: 3/20, step: 6100, training_loss: 2.24731
Epoch: 3/20, step: 6120, training_loss: 1.96893
Epoch: 3/20, step: 6140, training_loss: 1.55709
Epoch: 3/20, step: 6160, training_loss: 2.55861
Epoch: 3/20, step: 6180, training_loss: 2.65632
Epoch: 3/20, step: 6200, training_loss: 1.97905
Epoch: 3/20, step: 6220, training_loss: 2.19115
Epoch: 3/20, step: 6240, training_loss: 2.15113
Epoch: 3/20, step: 6260, training_loss: 2.30756
Epoch: 3/20, step: 6280, training_loss: 1.77784
Epoch: 3/20, step: 6300, training_loss: 1.74457
Epoch: 3/20, step: 6320, training_loss: 2.76622
Epoch: 3/20, step: 6340, training_loss: 1.56163
Epoch: 3/20, step: 6360, training_loss: 2.64159
Epoch: 3/20, step: 6380, training_loss: 2.48075
Epoch: 3/20, step: 6400, training_loss: 1.93155
Epoch: 3/20, step: 6420, training_loss: 1.90287
Epoch: 3/20, step: 6440, training_loss: 1.60614
Epoch: 3/20, step: 6460, training_loss: 2.82634
Epoch: 3/20, step: 6480, training_loss: 1.31493
Epoch: 3/20, step: 6500, training_loss: 1.48282
Epoch: 3/20, step: 6520, training_loss: 1.58167
Epoch: 3/20, step: 6540, training_loss: 2.45862
Epoch: 3/20, step: 6560, training_loss: 2.24712
Epoch: 3/20, step: 6580, training_loss: 1.70998
Epoch: 3/20, step: 6600, training_loss: 2.58094
Epoch: 3/20, step: 6620, training_loss: 2.50820
Epoch: 3/20, step: 6640, training_loss: 2.60706
Epoch: 3/20, step: 6660, training_loss: 1.77905
Epoch: 3/20, step: 6680, training_loss: 1.89313
Epoch: 3/20, step: 6700, training_loss: 1.35553
Epoch: 3/20, step: 6720, training_loss: 1.69147
Epoch: 3/20, step: 6740, training_loss: 2.04403
Epoch: 3/20, step: 6760, training_loss: 2.18618
Epoch: 3/20, step: 6780, training_loss: 1.71483
Epoch: 3/20, step: 6800, training_loss: 1.69480
Epoch: 3/20, step: 6820, training_loss: 2.45668
Epoch: 3/20, step: 6840, training_loss: 2.06237
Epoch: 3/20, step: 6860, training_loss: 1.61984
Epoch: 3/20, step: 6880, training_loss: 2.06518
Epoch: 3/20, step: 6900, training_loss: 2.16543
Epoch: 3/20, step: 6920, training_loss: 2.01588
Epoch: 3/20, step: 6940, training_loss: 2.71644
Epoch: 3/20, step: 6960, training_loss: 1.64091
Epoch: 3/20, step: 6980, training_loss: 1.89757
Epoch: 3/20, step: 7000, training_loss: 2.11692
accuracy: 0.37, validation_loss: 2.3297202587127686, num_samples: 100
Epoch: 3/20, step: 7020, training_loss: 2.14090
Epoch: 3/20, step: 7040, training_loss: 2.68629
Epoch: 3/20, step: 7060, training_loss: 2.12028
Epoch: 3/20, step: 7080, training_loss: 2.59800
Epoch: 3/20, step: 7100, training_loss: 1.96181
Epoch: 3/20, step: 7120, training_loss: 2.52295
Epoch: 3/20, step: 7140, training_loss: 2.06543
Epoch: 3/20, step: 7160, training_loss: 1.84004
Epoch: 3/20, step: 7180, training_loss: 2.57503
Epoch: 3/20, step: 7200, training_loss: 1.93095
Epoch: 3/20, step: 7220, training_loss: 2.22695
Epoch: 3/20, step: 7240, training_loss: 2.19396
Epoch: 3/20, step: 7260, training_loss: 2.21454
Epoch: 3/20, step: 7280, training_loss: 2.68236
Epoch: 3/20, step: 7300, training_loss: 2.48741
Epoch: 3/20, step: 7320, training_loss: 2.04497
Epoch: 3/20, step: 7340, training_loss: 2.70676
Epoch: 3/20, step: 7360, training_loss: 2.23120
Epoch: 3/20, step: 7380, training_loss: 2.02991
Epoch: 3/20, step: 7400, training_loss: 1.69075
Epoch: 3/20, step: 7420, training_loss: 1.94017
Epoch: 3/20, step: 7440, training_loss: 2.35091
Epoch: 3/20, step: 7460, training_loss: 2.42357
Epoch: 3/20, step: 7480, training_loss: 2.02622
Epoch: 3/20, step: 7500, training_loss: 2.09112
Epoch: 3/20, step: 7520, training_loss: 2.19680
Epoch: 3/20, step: 7540, training_loss: 2.04605
Epoch: 3/20, step: 7560, training_loss: 1.73300
Epoch: 3/20, step: 7580, training_loss: 2.47789
Epoch: 3/20, step: 7600, training_loss: 2.71058
Epoch: 3/20, step: 7620, training_loss: 2.40714
Epoch: 3/20, step: 7640, training_loss: 2.09105
Epoch: 3/20, step: 7660, training_loss: 1.70446
Epoch: 3/20, step: 7680, training_loss: 1.99587
Epoch: 3/20, step: 7700, training_loss: 2.21995
Epoch: 3/20, step: 7720, training_loss: 2.77222
Epoch: 3/20, step: 7740, training_loss: 2.37362
Epoch: 3/20, step: 7760, training_loss: 1.94736
Epoch: 3/20, step: 7780, training_loss: 2.01278
Epoch: 3/20, step: 7800, training_loss: 2.15315
Epoch: 3/20, step: 7820, training_loss: 1.90778
Epoch: 3/20, step: 7840, training_loss: 1.91272
Epoch: 3/20, step: 7860, training_loss: 2.33706
Epoch: 3/20, step: 7880, training_loss: 1.89916
Epoch: 3/20, step: 7900, training_loss: 1.98763
Epoch: 3/20, step: 7920, training_loss: 3.00168
Epoch: 3/20, step: 7940, training_loss: 2.09488
Epoch: 3/20, step: 7960, training_loss: 2.31901
Epoch: 3/20, step: 7980, training_loss: 2.56851
Epoch: 3/20, step: 8000, training_loss: 2.84527
accuracy: 0.46, validation_loss: 1.942826509475708, num_samples: 100
Epoch: 3/20, step: 8020, training_loss: 1.77699
Epoch: 3/20, step: 8040, training_loss: 2.52180
Epoch: 3/20, step: 8060, training_loss: 2.07777
Epoch: 3/20, step: 8080, training_loss: 2.79908
Epoch: 3/20, step: 8100, training_loss: 3.09745
Epoch: 3/20, step: 8120, training_loss: 2.26084
Epoch: 3/20, step: 8140, training_loss: 2.41201
Epoch: 3/20, step: 8160, training_loss: 1.99341
Epoch: 3/20, step: 8180, training_loss: 3.10908
Epoch: 3/20, step: 8200, training_loss: 2.15388
Epoch: 3/20, step: 8220, training_loss: 2.50520
Epoch: 3/20, step: 8240, training_loss: 2.62277
Epoch: 3/20, step: 8260, training_loss: 2.69564
Epoch: 3/20, step: 8280, training_loss: 1.77194
Epoch: 3/20, step: 8300, training_loss: 2.24365
Epoch: 3/20, step: 8320, training_loss: 2.40212
Epoch: 3/20, step: 8340, training_loss: 2.36998
Epoch: 3/20, step: 8360, training_loss: 2.80662
Epoch: 3/20, step: 8380, training_loss: 1.78348
Epoch: 3/20, step: 8400, training_loss: 2.01103
Epoch: 3/20, step: 8420, training_loss: 2.42544
Epoch: 3/20, step: 8440, training_loss: 1.80550
Epoch: 3/20, step: 8460, training_loss: 2.38840
Epoch: 3/20, step: 8480, training_loss: 1.53272
Epoch: 3/20, step: 8500, training_loss: 1.79498
Epoch: 3/20, step: 8520, training_loss: 2.27181
Epoch: 3/20, step: 8540, training_loss: 2.37888
Epoch: 3/20, step: 8560, training_loss: 2.02657
Epoch: 3/20, step: 8580, training_loss: 2.30258
Epoch: 3/20, step: 8600, training_loss: 1.29209
Epoch: 3/20, step: 8620, training_loss: 1.90276
Epoch: 3/20, step: 8640, training_loss: 2.18957
Epoch: 3/20, step: 8660, training_loss: 1.87225
Epoch: 3/20, step: 8680, training_loss: 1.42590
Epoch: 3/20, step: 8700, training_loss: 1.80857
Epoch: 3/20, step: 8720, training_loss: 2.53809
Epoch: 3/20, step: 8740, training_loss: 2.69507
Epoch: 3/20, step: 8760, training_loss: 2.08226
Epoch: 3/20, step: 8780, training_loss: 1.76917
Epoch: 3/20, step: 8800, training_loss: 2.38529
Epoch: 3/20, step: 8820, training_loss: 2.73536
Epoch: 3/20, step: 8840, training_loss: 2.40851
Epoch: 3/20, step: 8860, training_loss: 1.76618
Epoch: 3/20, step: 8880, training_loss: 1.41658
Epoch: 3/20, step: 8900, training_loss: 2.78581
Epoch: 3/20, step: 8920, training_loss: 1.83667
Epoch: 3/20, step: 8940, training_loss: 2.25277
Epoch: 3/20, step: 8960, training_loss: 2.13541
Epoch: 3/20, step: 8980, training_loss: 1.83222
Epoch: 3/20, step: 9000, training_loss: 2.05392
accuracy: 0.42, validation_loss: 1.8014034032821655, num_samples: 100
Epoch: 3/20, step: 9020, training_loss: 2.19203
Epoch: 3/20, step: 9040, training_loss: 2.08470
Epoch: 3/20, step: 9060, training_loss: 2.59628
Epoch: 3/20, step: 9080, training_loss: 1.95523
Epoch: 3/20, step: 9100, training_loss: 2.41587
Epoch: 3/20, step: 9120, training_loss: 1.83996
Epoch: 3/20, step: 9140, training_loss: 2.61281
Epoch: 3/20, step: 9160, training_loss: 1.73003
Epoch: 3/20, step: 9180, training_loss: 2.22315
Epoch: 3/20, step: 9200, training_loss: 2.34051
Epoch: 3/20, step: 9220, training_loss: 2.27658
Epoch: 3/20, step: 9240, training_loss: 2.12188
Epoch: 3/20, step: 9260, training_loss: 2.08327
Epoch: 3/20, step: 9280, training_loss: 1.74941
Epoch: 3/20, step: 9300, training_loss: 2.16969
Epoch: 3/20, step: 9320, training_loss: 1.72599
Epoch: 3/20, step: 9340, training_loss: 2.73392
Epoch: 3/20, step: 9360, training_loss: 2.23944
Epoch: 3/20, step: 9380, training_loss: 2.66655
Epoch: 3/20, step: 9400, training_loss: 1.94775
Epoch: 3/20, step: 9420, training_loss: 1.78758
Epoch: 3/20, step: 9440, training_loss: 1.86332
Epoch: 3/20, step: 9460, training_loss: 2.15704
Epoch: 3/20, step: 9480, training_loss: 2.26756
Epoch: 3/20, step: 9500, training_loss: 1.83133
Epoch: 3/20, step: 9520, training_loss: 2.52020
Epoch: 3/20, step: 9540, training_loss: 2.48919
Epoch: 3/20, step: 9560, training_loss: 1.76854
Epoch: 3/20, step: 9580, training_loss: 2.49441
Epoch: 3/20, step: 9600, training_loss: 2.31913
Epoch: 3/20, step: 9620, training_loss: 2.28972
Epoch: 3/20, step: 9640, training_loss: 2.31008
Epoch: 3/20, step: 9660, training_loss: 1.47166
Epoch: 3/20, step: 9680, training_loss: 2.14916
Epoch: 3/20, step: 9700, training_loss: 2.74815
Epoch: 3/20, step: 9720, training_loss: 2.15770
Epoch: 3/20, step: 9740, training_loss: 2.50203
Epoch: 3/20, step: 9760, training_loss: 2.64924
Epoch: 3/20, step: 9780, training_loss: 2.77324
Epoch: 3/20, step: 9800, training_loss: 1.64485
Epoch: 3/20, step: 9820, training_loss: 2.98215
Epoch: 3/20, step: 9840, training_loss: 2.53465
Epoch: 3/20, step: 9860, training_loss: 2.38456
Epoch: 3/20, step: 9880, training_loss: 2.31723
Epoch: 3/20, step: 9900, training_loss: 2.41488
Epoch: 3/20, step: 9920, training_loss: 2.18884
Epoch: 3/20, step: 9940, training_loss: 1.62023
Epoch: 3/20, step: 9960, training_loss: 1.16247
Epoch: 3/20, step: 9980, training_loss: 2.26914
Epoch: 3/20, step: 10000, training_loss: 2.25832
accuracy: 0.37, validation_loss: 2.011253833770752, num_samples: 100
Epoch: 3/20, step: 10020, training_loss: 0.70752
Epoch: 3/20, step: 10040, training_loss: 2.23791
Epoch: 3/20, step: 10060, training_loss: 1.44334
Epoch: 3/20, step: 10080, training_loss: 2.96459
Epoch: 3/20, step: 10100, training_loss: 1.51529
Epoch: 3/20, step: 10120, training_loss: 2.52897
Epoch: 3/20, step: 10140, training_loss: 2.06536
Epoch: 3/20, step: 10160, training_loss: 2.06725
Epoch: 3/20, step: 10180, training_loss: 2.64710
Epoch: 3/20, step: 10200, training_loss: 2.14748
Epoch: 3/20, step: 10220, training_loss: 2.36124
Epoch: 3/20, step: 10240, training_loss: 1.83925
Epoch: 3/20, step: 10260, training_loss: 2.09394
Epoch: 3/20, step: 10280, training_loss: 1.14333
Epoch: 3/20, step: 10300, training_loss: 2.01863
Epoch: 3/20, step: 10320, training_loss: 2.29445
Epoch: 3/20, step: 10340, training_loss: 1.73329
Epoch: 3/20, step: 10360, training_loss: 1.71788
Epoch: 3/20, step: 10380, training_loss: 1.60627
Epoch: 3/20, step: 10400, training_loss: 1.74057
Epoch: 3/20, step: 10420, training_loss: 2.19731
Epoch: 3/20, step: 10440, training_loss: 1.97224
Epoch: 3/20, step: 10460, training_loss: 2.73674
Epoch: 3/20, step: 10480, training_loss: 1.93394
Epoch: 3/20, step: 10500, training_loss: 2.39745
Epoch: 3/20, step: 10520, training_loss: 2.44471
Epoch: 3/20, step: 10540, training_loss: 2.55052
Epoch: 3/20, step: 10560, training_loss: 2.55942
Epoch: 3/20, step: 10580, training_loss: 2.15163
Epoch: 3/20, step: 10600, training_loss: 2.23143
Epoch: 3/20, step: 10620, training_loss: 2.35778
Epoch: 3/20, step: 10640, training_loss: 2.14781
Epoch: 3/20, step: 10660, training_loss: 1.91011
Epoch: 3/20, step: 10680, training_loss: 2.08634
Epoch: 3/20, step: 10700, training_loss: 2.05803
Epoch: 3/20, step: 10720, training_loss: 2.10936
Epoch: 3/20, step: 10740, training_loss: 2.30809
Epoch: 3/20, step: 10760, training_loss: 2.12300
Epoch: 3/20, step: 10780, training_loss: 1.93560
Epoch: 3/20, step: 10800, training_loss: 2.38134
Epoch: 3/20, step: 10820, training_loss: 2.48894
Epoch: 3/20, step: 10840, training_loss: 2.20788
Epoch: 3/20, step: 10860, training_loss: 2.02233
Epoch: 3/20, step: 10880, training_loss: 2.55581
Epoch: 3/20, step: 10900, training_loss: 1.82096
Epoch: 3/20, step: 10920, training_loss: 2.44241
Epoch: 3/20, step: 10940, training_loss: 2.02269
Epoch: 3/20, step: 10960, training_loss: 2.19834
Epoch: 3/20, step: 10980, training_loss: 2.73628
Epoch: 3/20, step: 11000, training_loss: 2.54761
accuracy: 0.34, validation_loss: 2.033143997192383, num_samples: 100
Epoch: 3/20, step: 11020, training_loss: 2.54645
Epoch: 3/20, step: 11040, training_loss: 2.33007
Epoch: 3/20, step: 11060, training_loss: 1.49209
Epoch: 3/20, step: 11080, training_loss: 2.45694
Epoch: 3/20, step: 11100, training_loss: 1.59336
Epoch: 3/20, step: 11120, training_loss: 2.40770
Epoch: 3/20, step: 11140, training_loss: 1.89913
Epoch: 3/20, step: 11160, training_loss: 1.78110
Epoch: 3/20, step: 11180, training_loss: 2.73380
Epoch: 3/20, step: 11200, training_loss: 2.54162
Epoch: 3/20, step: 11220, training_loss: 2.46533
Epoch: 3/20, step: 11240, training_loss: 2.75037
Epoch: 3/20, step: 11260, training_loss: 1.58176
Epoch: 3/20, step: 11280, training_loss: 2.24031
Epoch: 3/20, step: 11300, training_loss: 2.46157
Epoch: 3/20, step: 11320, training_loss: 2.19172
Epoch: 3/20, step: 11340, training_loss: 2.91630
Epoch: 3/20, step: 11360, training_loss: 1.88916
Epoch: 3/20, step: 11380, training_loss: 1.92391
Epoch: 3/20, step: 11400, training_loss: 1.26339
Epoch: 3/20, step: 11420, training_loss: 1.39172
Epoch: 3/20, step: 11440, training_loss: 2.23476
Epoch: 3/20, step: 11460, training_loss: 2.10644
Epoch: 3/20, step: 11480, training_loss: 1.73656
Epoch: 3/20, step: 11500, training_loss: 2.35794
Epoch: 3/20, step: 11520, training_loss: 2.55039
Epoch: 3/20, step: 11540, training_loss: 1.37161
Epoch: 3/20, step: 11560, training_loss: 1.49101
Epoch: 3/20, step: 11580, training_loss: 2.04222
Epoch: 3/20, step: 11600, training_loss: 2.84754
Epoch: 3/20, step: 11620, training_loss: 2.04477
Epoch: 3/20, step: 11640, training_loss: 2.39236
Epoch: 3/20, step: 11660, training_loss: 1.86224
Epoch: 3/20, step: 11680, training_loss: 1.93926
Epoch: 3/20, step: 11700, training_loss: 2.29839
Epoch: 3/20, step: 11720, training_loss: 2.31566
Epoch: 3/20, step: 11740, training_loss: 2.38535
Epoch: 3/20, step: 11760, training_loss: 2.40558
Epoch: 3/20, step: 11780, training_loss: 1.27930
Epoch: 3/20, step: 11800, training_loss: 2.09951
Epoch: 3/20, step: 11820, training_loss: 2.59994
Epoch: 3/20, step: 11840, training_loss: 1.75273
Epoch: 3/20, step: 11860, training_loss: 2.05680
Epoch: 3/20, step: 11880, training_loss: 2.79331
Epoch: 3/20, step: 11900, training_loss: 1.89098
Epoch: 3/20, step: 11920, training_loss: 1.58405
Epoch: 3/20, step: 11940, training_loss: 2.11563
Epoch: 3/20, step: 11960, training_loss: 2.18843
Epoch: 3/20, step: 11980, training_loss: 2.23989
Epoch: 3/20, step: 12000, training_loss: 1.25742
accuracy: 0.39, validation_loss: 2.3060953617095947, num_samples: 100
Epoch: 3/20, step: 12020, training_loss: 2.22303
Epoch: 3/20, step: 12040, training_loss: 1.96157
Epoch: 3/20, step: 12060, training_loss: 1.62160
Epoch: 3/20, step: 12080, training_loss: 2.71870
Epoch: 3/20, step: 12100, training_loss: 2.48720
Epoch: 3/20, step: 12120, training_loss: 1.41321
Epoch: 3/20, step: 12140, training_loss: 1.34298
Epoch: 3/20, step: 12160, training_loss: 1.59527
Epoch: 3/20, step: 12180, training_loss: 2.06228
Epoch: 3/20, step: 12200, training_loss: 2.06394
Epoch: 3/20, step: 12220, training_loss: 2.03760
Epoch: 3/20, step: 12240, training_loss: 2.36562
Epoch: 3/20, step: 12260, training_loss: 2.79351
Epoch: 3/20, step: 12280, training_loss: 2.21908
Epoch: 3/20, step: 12300, training_loss: 2.02388
Epoch: 3/20, step: 12320, training_loss: 2.32431
Epoch: 3/20, step: 12340, training_loss: 2.81340
Epoch: 3/20, step: 12360, training_loss: 2.18039
Epoch: 3/20, step: 12380, training_loss: 2.45942
Epoch: 3/20, step: 12400, training_loss: 2.82583
Epoch: 3/20, step: 12420, training_loss: 2.39522
Epoch: 3/20, step: 12440, training_loss: 2.94099
Epoch: 3/20, step: 12460, training_loss: 2.91068
Epoch: 3/20, step: 12480, training_loss: 2.24248
Epoch: 3/20, step: 12500, training_loss: 3.07065
Epoch: 3/20, step: 12520, training_loss: 1.89698
Epoch: 3/20, step: 12540, training_loss: 2.27524
Epoch: 3/20, step: 12560, training_loss: 1.28045
Epoch: 3/20, step: 12580, training_loss: 1.97126
Epoch: 3/20, step: 12600, training_loss: 1.83210
Epoch: 3/20, step: 12620, training_loss: 2.22538
Epoch: 3/20, step: 12640, training_loss: 2.44202
Epoch: 3/20, step: 12660, training_loss: 1.93865
Epoch: 3/20, step: 12680, training_loss: 2.04130
Epoch: 3/20, step: 12700, training_loss: 1.79977
Epoch: 3/20, step: 12720, training_loss: 2.06359
Epoch: 3/20, step: 12740, training_loss: 2.33760
Epoch: 3/20, step: 12760, training_loss: 1.93557
Epoch: 3/20, step: 12780, training_loss: 2.25255
Epoch: 3/20, step: 12800, training_loss: 1.97641
Epoch: 3/20, step: 12820, training_loss: 2.26705
Epoch: 3/20, step: 12840, training_loss: 1.88626
Epoch: 3/20, step: 12860, training_loss: 2.25910
Epoch: 3/20, step: 12880, training_loss: 2.11863
Epoch: 3/20, step: 12900, training_loss: 2.05486
Epoch: 3/20, step: 12920, training_loss: 2.48567
Epoch: 3/20, step: 12940, training_loss: 2.93310
Epoch: 3/20, step: 12960, training_loss: 2.08909
Epoch: 3/20, step: 12980, training_loss: 2.35018
Epoch: 3/20, step: 13000, training_loss: 1.76592
accuracy: 0.42, validation_loss: 2.095745801925659, num_samples: 100
Epoch: 3/20, step: 13020, training_loss: 2.27360
Epoch: 3/20, step: 13040, training_loss: 1.91254
Epoch: 3/20, step: 13060, training_loss: 3.18529
Epoch: 3/20, step: 13080, training_loss: 1.92155
Epoch: 3/20, step: 13100, training_loss: 1.89416
Epoch: 3/20, step: 13120, training_loss: 2.21291
Epoch: 3/20, step: 13140, training_loss: 2.11196
Epoch: 3/20, step: 13160, training_loss: 2.09144
Epoch: 3/20, step: 13180, training_loss: 2.35911
Epoch: 3/20, step: 13200, training_loss: 1.64035
Epoch: 3/20, step: 13220, training_loss: 1.93817
Epoch: 3/20, step: 13240, training_loss: 2.11008
Epoch: 3/20, step: 13260, training_loss: 1.93834
Epoch: 3/20, step: 13280, training_loss: 3.19336
Epoch: 3/20, step: 13300, training_loss: 2.92313
Epoch: 3/20, step: 13320, training_loss: 2.98704
Epoch: 3/20, step: 13340, training_loss: 1.43918
Epoch: 3/20, step: 13360, training_loss: 2.76240
Epoch: 3/20, step: 13380, training_loss: 2.14185
Epoch: 3/20, step: 13400, training_loss: 2.06379
Epoch: 3/20, step: 13420, training_loss: 1.34896
Epoch: 3/20, step: 13440, training_loss: 2.26699
Epoch: 3/20, step: 13460, training_loss: 2.22153
Epoch: 3/20, step: 13480, training_loss: 1.67694
Epoch: 3/20, step: 13500, training_loss: 2.04650
Epoch: 3/20, step: 13520, training_loss: 1.58284
Epoch: 3/20, step: 13540, training_loss: 2.34014
Epoch: 3/20, step: 13560, training_loss: 2.19427
Epoch: 3/20, step: 13580, training_loss: 1.76724
Epoch: 3/20, step: 13600, training_loss: 1.75547
Epoch: 3/20, step: 13620, training_loss: 2.60875
Epoch: 3/20, step: 13640, training_loss: 2.53656
Epoch: 3/20, step: 13660, training_loss: 1.77850
Epoch: 3/20, step: 13680, training_loss: 2.63012
Epoch: 3/20, step: 13700, training_loss: 2.52998
Epoch: 3/20, step: 13720, training_loss: 1.73662
Epoch: 3/20, step: 13740, training_loss: 2.03334
Epoch: 3/20, step: 13760, training_loss: 2.10364
Epoch: 3/20, step: 13780, training_loss: 2.21142
Epoch: 3/20, step: 13800, training_loss: 1.93348
Epoch: 3/20, step: 13820, training_loss: 2.60561
Epoch: 3/20, step: 13840, training_loss: 1.29269
Epoch: 3/20, step: 13860, training_loss: 1.96073
Epoch: 3/20, step: 13880, training_loss: 2.22492
Epoch: 3/20, step: 13900, training_loss: 2.52098
Epoch: 3/20, step: 13920, training_loss: 2.43177
Epoch: 3/20, step: 13940, training_loss: 2.50144
Epoch: 3/20, step: 13960, training_loss: 1.53434
Epoch: 3/20, step: 13980, training_loss: 3.16214
Epoch: 3/20, step: 14000, training_loss: 1.74659
accuracy: 0.47, validation_loss: 2.0700931549072266, num_samples: 100
Epoch: 3/20, step: 14020, training_loss: 2.83041
Epoch: 3/20, step: 14040, training_loss: 2.72481
Epoch: 3/20, step: 14060, training_loss: 2.28640
Epoch: 3/20, step: 14080, training_loss: 2.14526
Epoch: 3/20, step: 14100, training_loss: 2.39795
Epoch: 3/20, step: 14120, training_loss: 1.76071
Epoch: 3/20, step: 14140, training_loss: 2.28481
Epoch: 3/20, step: 14160, training_loss: 2.03287
Epoch: 3/20, step: 14180, training_loss: 2.46231
Epoch: 3/20, step: 14200, training_loss: 2.10077
Epoch: 3/20, step: 14220, training_loss: 1.53072
Epoch: 3/20, step: 14240, training_loss: 2.84990
Epoch: 3/20, step: 14260, training_loss: 2.44564
Epoch: 3/20, step: 14280, training_loss: 2.47655
Epoch: 3/20, step: 14300, training_loss: 1.44397
Epoch: 3/20, step: 14320, training_loss: 2.15586
Epoch: 3/20, step: 14340, training_loss: 2.37250
Epoch: 3/20, step: 14360, training_loss: 1.71885
Epoch: 3/20, step: 14380, training_loss: 1.85075
Epoch: 3/20, step: 14400, training_loss: 2.65522
Epoch: 3/20, step: 14420, training_loss: 2.66895
Epoch: 3/20, step: 14440, training_loss: 2.67325
Epoch: 3/20, step: 14460, training_loss: 1.95213
Epoch: 3/20, step: 14480, training_loss: 2.92404
Epoch: 3/20, step: 14500, training_loss: 1.98755
Epoch: 3/20, step: 14520, training_loss: 1.32743
Epoch: 3/20, step: 14540, training_loss: 2.02470
Epoch: 3/20, step: 14560, training_loss: 3.00006
Epoch: 3/20, step: 14580, training_loss: 1.14469
Epoch: 3/20, step: 14600, training_loss: 1.87686
Epoch: 3/20, step: 14620, training_loss: 2.67430
Epoch: 3/20, step: 14640, training_loss: 1.93777
Epoch: 3/20, step: 14660, training_loss: 1.94172
Epoch: 3/20, step: 14680, training_loss: 2.28603
Epoch: 3/20, step: 14700, training_loss: 1.99933
Epoch: 3/20, step: 14720, training_loss: 2.22322
Epoch: 3/20, step: 14740, training_loss: 2.25696
Epoch: 3/20, step: 14760, training_loss: 2.45516
Epoch: 3/20, step: 14780, training_loss: 1.87394
Epoch: 3/20, step: 14800, training_loss: 2.61867
Epoch: 3/20, step: 14820, training_loss: 2.27833
Epoch: 3/20, step: 14840, training_loss: 2.20012
Epoch: 3/20, step: 14860, training_loss: 1.35602
Epoch: 3/20, step: 14880, training_loss: 1.55037
Epoch: 3/20, step: 14900, training_loss: 2.04745
Epoch: 3/20, step: 14920, training_loss: 2.00052
Epoch: 3/20, step: 14940, training_loss: 2.21135
Epoch: 3/20, step: 14960, training_loss: 2.32227
Epoch: 3/20, step: 14980, training_loss: 1.80225
Epoch: 3/20, step: 15000, training_loss: 1.07955
accuracy: 0.44, validation_loss: 2.047931671142578, num_samples: 100
Epoch: 3/20, step: 15020, training_loss: 1.70694
Epoch: 3/20, step: 15040, training_loss: 1.79966
Epoch: 3/20, step: 15060, training_loss: 2.00243
Epoch: 3/20, step: 15080, training_loss: 1.68742
Epoch: 3/20, step: 15100, training_loss: 2.70612
Epoch: 3/20, step: 15120, training_loss: 2.35013
Epoch: 3/20, step: 15140, training_loss: 1.79086
Epoch: 3/20, step: 15160, training_loss: 2.71022
Epoch: 3/20, step: 15180, training_loss: 2.23945
Epoch: 3/20, step: 15200, training_loss: 2.21199
Epoch: 3/20, step: 15220, training_loss: 1.92966
Epoch: 3/20, step: 15240, training_loss: 1.61826
Epoch: 3/20, step: 15260, training_loss: 2.25612
Epoch: 3/20, step: 15280, training_loss: 1.99991
Epoch: 3/20, step: 15300, training_loss: 2.05537
Epoch: 3/20, step: 15320, training_loss: 1.82669
Epoch: 3/20, step: 15340, training_loss: 1.40976
Epoch: 3/20, step: 15360, training_loss: 2.18572
Epoch: 3/20, step: 15380, training_loss: 2.66453
Epoch: 3/20, step: 15400, training_loss: 2.25739
Epoch: 3/20, step: 15420, training_loss: 2.95865
Epoch: 3/20, step: 15440, training_loss: 2.33365
Epoch: 3/20, step: 15460, training_loss: 2.37814
Epoch: 3/20, step: 15480, training_loss: 1.37932
Epoch: 3/20, step: 15500, training_loss: 1.35707
Epoch: 3/20, step: 15520, training_loss: 2.69917
Epoch: 3/20, step: 15540, training_loss: 2.09157
Epoch: 3/20, step: 15560, training_loss: 1.37826
Epoch: 3/20, step: 15580, training_loss: 2.32578
Epoch: 3/20, step: 15600, training_loss: 2.66179
Epoch: 3/20, step: 15620, training_loss: 1.96770
Epoch: 3/20, step: 15640, training_loss: 2.25019
Epoch: 3/20, step: 15660, training_loss: 2.68056
Epoch: 3/20, step: 15680, training_loss: 1.96712
Epoch: 3/20, step: 15700, training_loss: 2.39442
Epoch: 3/20, step: 15720, training_loss: 2.22969
Epoch: 3/20, step: 15740, training_loss: 2.59210
Epoch: 3/20, step: 15760, training_loss: 2.28098
Epoch: 3/20, step: 15780, training_loss: 1.32698
Epoch: 3/20, step: 15800, training_loss: 2.34681
Epoch: 3/20, step: 15820, training_loss: 1.71598
Epoch: 3/20, step: 15840, training_loss: 1.85101
Epoch: 3/20, step: 15860, training_loss: 2.09006
Epoch: 3/20, step: 15880, training_loss: 1.41790
Epoch: 3/20, step: 15900, training_loss: 1.93521
Epoch: 3/20, step: 15920, training_loss: 2.37293
Epoch: 3/20, step: 15940, training_loss: 2.57418
Epoch: 3/20, step: 15960, training_loss: 1.76629
Epoch: 3/20, step: 15980, training_loss: 2.18536
Epoch: 3/20, step: 16000, training_loss: 1.67416
accuracy: 0.39, validation_loss: 2.2126283645629883, num_samples: 100
Epoch: 3/20, step: 16020, training_loss: 2.24580
Epoch: 3/20, step: 16040, training_loss: 1.64445
Epoch: 3/20, step: 16060, training_loss: 1.16203
Epoch: 3/20, step: 16080, training_loss: 2.00829
Epoch: 3/20, step: 16100, training_loss: 2.05666
Epoch: 3/20, step: 16120, training_loss: 1.99034
Epoch: 3/20, step: 16140, training_loss: 1.69689
Epoch: 3/20, step: 16160, training_loss: 2.19338
Epoch: 3/20, step: 16180, training_loss: 1.75606
Epoch: 3/20, step: 16200, training_loss: 1.32284
Epoch: 3/20, step: 16220, training_loss: 2.56925
Epoch: 3/20, step: 16240, training_loss: 2.61840
Epoch: 3/20, step: 16260, training_loss: 1.99302
Epoch: 3/20, step: 16280, training_loss: 2.88209
Epoch: 3/20, step: 16300, training_loss: 1.74069
Epoch: 3/20, step: 16320, training_loss: 1.50270
Epoch: 3/20, step: 16340, training_loss: 1.64640
Epoch: 3/20, step: 16360, training_loss: 2.06263
Epoch: 3/20, step: 16380, training_loss: 2.23224
Epoch: 3/20, step: 16400, training_loss: 1.97999
Epoch: 3/20, step: 16420, training_loss: 2.47734
Epoch: 3/20, step: 16440, training_loss: 2.11434
Epoch: 3/20, step: 16460, training_loss: 1.89010
Epoch: 3/20, step: 16480, training_loss: 2.98539
Epoch: 3/20, step: 16500, training_loss: 2.40860
Epoch: 3/20, step: 16520, training_loss: 1.84737
Epoch: 3/20, step: 16540, training_loss: 1.80281
Epoch: 3/20, step: 16560, training_loss: 2.09279
Epoch: 3/20, step: 16580, training_loss: 2.45667
Epoch: 3/20, step: 16600, training_loss: 2.95144
Epoch: 3/20, step: 16620, training_loss: 1.36216
Epoch: 3/20, step: 16640, training_loss: 2.31727
Epoch: 3/20, step: 16660, training_loss: 2.08851
Epoch: 3/20, step: 16680, training_loss: 1.82580
Epoch: 3/20, step: 16700, training_loss: 2.00702
Epoch: 3/20, step: 16720, training_loss: 1.89881
Epoch: 3/20, step: 16740, training_loss: 2.32278
Epoch: 3/20, step: 16760, training_loss: 1.86341
Epoch: 3/20, step: 16780, training_loss: 2.12629
Epoch: 3/20, step: 16800, training_loss: 3.34515
Epoch: 3/20, step: 16820, training_loss: 2.66705
Epoch: 3/20, step: 16840, training_loss: 1.99155
Epoch: 3/20, step: 16860, training_loss: 1.90849
Epoch: 3/20, step: 16880, training_loss: 2.23796
Epoch: 3/20, step: 16900, training_loss: 2.63673
Epoch: 3/20, step: 16920, training_loss: 2.09321
Epoch: 3/20, step: 16940, training_loss: 2.97850
Epoch: 3/20, step: 16960, training_loss: 2.67967
Epoch: 3/20, step: 16980, training_loss: 2.39857
Epoch: 3/20, step: 17000, training_loss: 2.35362
accuracy: 0.49, validation_loss: 1.8935717344284058, num_samples: 100
Epoch: 3/20, step: 17020, training_loss: 2.68439
Epoch: 3/20, step: 17040, training_loss: 1.93822
Epoch: 3/20, step: 17060, training_loss: 2.63905
Epoch: 3/20, step: 17080, training_loss: 1.92136
Epoch: 3/20, step: 17100, training_loss: 2.36292
Epoch: 3/20, step: 17120, training_loss: 1.40585
Epoch: 3/20, step: 17140, training_loss: 2.36821
Epoch: 3/20, step: 17160, training_loss: 2.70369
Epoch: 3/20, step: 17180, training_loss: 3.00647
Epoch: 3/20, step: 17200, training_loss: 2.22098
Epoch: 3/20, step: 17220, training_loss: 1.82859
Epoch: 3/20, step: 17240, training_loss: 1.59718
Epoch: 3/20, step: 17260, training_loss: 2.43357
Epoch: 3/20, step: 17280, training_loss: 2.11413
Epoch: 3/20, step: 17300, training_loss: 2.05210
Epoch: 3/20, step: 17320, training_loss: 2.53160
Epoch: 3/20, step: 17340, training_loss: 2.21918
Epoch: 3/20, step: 17360, training_loss: 2.17449
Epoch: 3/20, step: 17380, training_loss: 2.57616
Epoch: 3/20, step: 17400, training_loss: 2.12904
Epoch: 3/20, step: 17420, training_loss: 2.47942
Epoch: 3/20, step: 17440, training_loss: 2.79664
Epoch: 3/20, step: 17460, training_loss: 2.35216
Epoch: 3/20, step: 17480, training_loss: 2.38207
Epoch: 3/20, step: 17500, training_loss: 2.90534
Epoch: 3/20, step: 17520, training_loss: 2.23241
Epoch: 3/20, step: 17540, training_loss: 1.34092
Epoch: 3/20, step: 17560, training_loss: 2.55656
Epoch: 3/20, step: 17580, training_loss: 2.72241
Epoch: 3/20, step: 17600, training_loss: 1.41864
Epoch: 3/20, step: 17620, training_loss: 2.23449
Epoch: 3/20, step: 17640, training_loss: 2.84837
Epoch: 3/20, step: 17660, training_loss: 2.01521
Epoch: 3/20, step: 17680, training_loss: 1.31926
Epoch: 3/20, step: 17700, training_loss: 2.68149
Epoch: 3/20, step: 17720, training_loss: 2.68407
Epoch: 3/20, step: 17740, training_loss: 2.32244
Epoch: 3/20, step: 17760, training_loss: 2.17572
Epoch: 3/20, step: 17780, training_loss: 1.79837
Epoch: 3/20, step: 17800, training_loss: 3.23182
Epoch: 3/20, step: 17820, training_loss: 2.47484
Epoch: 3/20, step: 17840, training_loss: 1.31958
Epoch: 3/20, step: 17860, training_loss: 2.41063
Epoch: 3/20, step: 17880, training_loss: 2.96595
Epoch: 3/20, step: 17900, training_loss: 2.09103
Epoch: 3/20, step: 17920, training_loss: 1.68453
Epoch: 3/20, step: 17940, training_loss: 2.39490
Epoch: 3/20, step: 17960, training_loss: 2.04440
Epoch: 3/20, step: 17980, training_loss: 2.62978
Epoch: 3/20, step: 18000, training_loss: 3.04975
accuracy: 0.38, validation_loss: 2.3427717685699463, num_samples: 100
Epoch: 3/20, step: 18020, training_loss: 2.39165
Epoch: 3/20, step: 18040, training_loss: 1.88966
Epoch: 3/20, step: 18060, training_loss: 1.70194
Epoch: 3/20, step: 18080, training_loss: 1.61113
Epoch: 3/20, step: 18100, training_loss: 2.21015
Epoch: 3/20, step: 18120, training_loss: 2.55380
Epoch: 3/20, step: 18140, training_loss: 1.91861
Epoch: 3/20, step: 18160, training_loss: 2.35483
Epoch: 3/20, step: 18180, training_loss: 1.93356
Epoch: 3/20, step: 18200, training_loss: 2.47174
Epoch: 3/20, step: 18220, training_loss: 2.03262
Epoch: 3/20, step: 18240, training_loss: 2.88726
Epoch: 3/20, step: 18260, training_loss: 2.07091
Epoch: 3/20, step: 18280, training_loss: 1.97614
Epoch: 3/20, step: 18300, training_loss: 2.13321
Epoch: 3/20, step: 18320, training_loss: 2.11684
Epoch: 3/20, step: 18340, training_loss: 1.77354
Epoch: 3/20, step: 18360, training_loss: 2.57684
Epoch: 3/20, step: 18380, training_loss: 3.01911
Epoch: 3/20, step: 18400, training_loss: 2.67460
Epoch: 3/20, step: 18420, training_loss: 1.85717
Epoch: 3/20, step: 18440, training_loss: 1.81636
Epoch: 3/20, step: 18460, training_loss: 1.85000
Epoch: 3/20, step: 18480, training_loss: 2.52044
Epoch: 3/20, step: 18500, training_loss: 2.54365
Epoch: 3/20, step: 18520, training_loss: 1.74325
Epoch: 3/20, step: 18540, training_loss: 1.23316
Epoch: 3/20, step: 18560, training_loss: 2.49090
Epoch: 3/20, step: 18580, training_loss: 2.78692
Epoch: 3/20, step: 18600, training_loss: 1.20428
Epoch: 3/20, step: 18620, training_loss: 2.77993
Epoch: 3/20, step: 18640, training_loss: 2.49429
Epoch: 3/20, step: 18660, training_loss: 2.14406
Epoch: 3/20, step: 18680, training_loss: 2.10107
Epoch: 3/20, step: 18700, training_loss: 2.40559
Epoch: 3/20, step: 18720, training_loss: 2.05231
Epoch: 3/20, step: 18740, training_loss: 2.54707
Epoch: 3/20, step: 18760, training_loss: 2.73681
Epoch: 3/20, step: 18780, training_loss: 1.76576
Epoch: 3/20, step: 18800, training_loss: 2.42738
Epoch: 3/20, step: 18820, training_loss: 2.41110
Epoch: 3/20, step: 18840, training_loss: 2.44610
Epoch: 3/20, step: 18860, training_loss: 2.21907
Epoch: 3/20, step: 18880, training_loss: 1.75900
Epoch: 3/20, step: 18900, training_loss: 2.36400
Epoch: 3/20, step: 18920, training_loss: 2.34826
Epoch: 3/20, step: 18940, training_loss: 2.62964
Epoch: 3/20, step: 18960, training_loss: 1.54209
Epoch: 3/20, step: 18980, training_loss: 2.88153
Epoch: 3/20, step: 19000, training_loss: 2.68222
accuracy: 0.4, validation_loss: 2.2516143321990967, num_samples: 100
Epoch: 3/20, step: 19020, training_loss: 2.18811
Epoch: 3/20, step: 19040, training_loss: 2.36474
Epoch: 3/20, step: 19060, training_loss: 2.84206
Epoch: 3/20, step: 19080, training_loss: 2.93663
Epoch: 3/20, step: 19100, training_loss: 1.74894
Epoch: 3/20, step: 19120, training_loss: 2.56620
Epoch: 3/20, step: 19140, training_loss: 2.29955
Epoch: 3/20, step: 19160, training_loss: 2.03283
Epoch: 3/20, step: 19180, training_loss: 1.78526
Epoch: 3/20, step: 19200, training_loss: 1.83733
Epoch: 3/20, step: 19220, training_loss: 1.86522
Epoch: 3/20, step: 19240, training_loss: 1.61461
Epoch: 3/20, step: 19260, training_loss: 2.13129
Epoch: 3/20, step: 19280, training_loss: 1.87931
Epoch: 3/20, step: 19300, training_loss: 2.38791
Epoch: 3/20, step: 19320, training_loss: 2.39868
Epoch: 3/20, step: 19340, training_loss: 1.70069
Epoch: 3/20, step: 19360, training_loss: 2.12455
Epoch: 3/20, step: 19380, training_loss: 2.95517
Epoch: 3/20, step: 19400, training_loss: 2.52285
Epoch: 3/20, step: 19420, training_loss: 2.09931
Epoch: 3/20, step: 19440, training_loss: 2.35353
Epoch: 3/20, step: 19460, training_loss: 2.09979
Epoch: 3/20, step: 19480, training_loss: 1.63535
Epoch: 3/20, step: 19500, training_loss: 2.31698
Epoch: 3/20, step: 19520, training_loss: 1.87770
Epoch: 3/20, step: 19540, training_loss: 2.95661
Epoch: 3/20, step: 19560, training_loss: 1.87639
Epoch: 3/20, step: 19580, training_loss: 1.52047
Epoch: 3/20, step: 19600, training_loss: 1.74739
Epoch: 3/20, step: 19620, training_loss: 2.16400
Epoch: 3/20, step: 19640, training_loss: 2.89357
Epoch: 3/20, step: 19660, training_loss: 2.44503
Epoch: 3/20, step: 19680, training_loss: 2.15845
Epoch: 3/20, step: 19700, training_loss: 2.24516
Epoch: 3/20, step: 19720, training_loss: 1.67499
Epoch: 3/20, step: 19740, training_loss: 2.42206
Epoch: 3/20, step: 19760, training_loss: 2.75355
Epoch: 3/20, step: 19780, training_loss: 2.60438
Epoch: 3/20, step: 19800, training_loss: 2.46191
Epoch: 3/20, step: 19820, training_loss: 2.02823
Epoch: 3/20, step: 19840, training_loss: 1.43359
Epoch: 3/20, step: 19860, training_loss: 2.41376
Epoch: 3/20, step: 19880, training_loss: 2.07629
Epoch: 3/20, step: 19900, training_loss: 1.94972
Epoch: 3/20, step: 19920, training_loss: 2.07596
Epoch: 3/20, step: 19940, training_loss: 2.12480
Epoch: 3/20, step: 19960, training_loss: 1.91720
Epoch: 3/20, step: 19980, training_loss: 2.87612
Epoch: 3/20, step: 20000, training_loss: 2.37605
accuracy: 0.41, validation_loss: 2.1087822914123535, num_samples: 100
Epoch: 3/20, step: 20020, training_loss: 3.26021
Epoch: 3/20, step: 20040, training_loss: 2.24699
Epoch: 3/20, step: 20060, training_loss: 2.09903
Epoch: 3/20, step: 20080, training_loss: 1.63232
Epoch: 3/20, step: 20100, training_loss: 1.83213
Epoch: 3/20, step: 20120, training_loss: 2.15346
Epoch: 3/20, step: 20140, training_loss: 2.26843
Epoch: 3/20, step: 20160, training_loss: 2.48345
Epoch: 3/20, step: 20180, training_loss: 2.57565
Epoch: 3/20, step: 20200, training_loss: 2.40814
Epoch: 3/20, step: 20220, training_loss: 2.15721
Epoch: 3/20, step: 20240, training_loss: 1.97107
Epoch: 3/20, step: 20260, training_loss: 1.82958
Epoch: 3/20, step: 20280, training_loss: 3.15261
Epoch: 3/20, step: 20300, training_loss: 2.07863
Epoch: 3/20, step: 20320, training_loss: 2.33390
Epoch: 3/20, step: 20340, training_loss: 2.19693
Epoch: 3/20, step: 20360, training_loss: 1.78728
Epoch: 3/20, step: 20380, training_loss: 1.96041
Epoch: 3/20, step: 20400, training_loss: 2.52709
Epoch: 3/20, step: 20420, training_loss: 2.04215
Epoch: 3/20, step: 20440, training_loss: 1.75928
Epoch: 3/20, step: 20460, training_loss: 1.66921
Epoch: 3/20, step: 20480, training_loss: 2.22485
Epoch: 3/20, step: 20500, training_loss: 2.18732
Epoch: 3/20, step: 20520, training_loss: 2.06290
Epoch: 3/20, step: 20540, training_loss: 1.61108
Epoch: 3/20, step: 20560, training_loss: 2.18241
Epoch: 3/20, step: 20580, training_loss: 2.12403
Epoch: 3/20, step: 20600, training_loss: 1.67278
Epoch: 3/20, step: 20620, training_loss: 1.93696
Epoch: 3/20, step: 20640, training_loss: 2.42528
Epoch: 3/20, step: 20660, training_loss: 1.78502
Epoch: 3/20, step: 20680, training_loss: 2.43371
Epoch: 3/20, step: 20700, training_loss: 3.02175
Epoch: 3/20, step: 20720, training_loss: 1.68639
Epoch: 3/20, step: 20740, training_loss: 2.56647
Epoch: 3/20, step: 20760, training_loss: 2.18849
Epoch: 3/20, step: 20780, training_loss: 1.90178
Epoch: 3/20, step: 20800, training_loss: 2.45272
Epoch: 3/20, step: 20820, training_loss: 1.67519
Epoch: 3/20, step: 20840, training_loss: 1.86282
Epoch: 3/20, step: 20860, training_loss: 2.05263
Epoch: 3/20, step: 20880, training_loss: 2.65890
Epoch: 3/20, step: 20900, training_loss: 1.88980
Epoch: 3/20, step: 20920, training_loss: 2.45473
Epoch: 3/20, step: 20940, training_loss: 1.91032
Epoch: 3/20, step: 20960, training_loss: 2.20811
Epoch: 3/20, step: 20980, training_loss: 2.56434
Epoch: 3/20, step: 21000, training_loss: 1.54852
accuracy: 0.45, validation_loss: 1.9636584520339966, num_samples: 100
Epoch: 3/20, step: 21020, training_loss: 2.18730
Epoch: 3/20, step: 21040, training_loss: 2.37250
Epoch: 3/20, step: 21060, training_loss: 2.33136
Epoch: 3/20, step: 21080, training_loss: 1.44827
Epoch: 3/20, step: 21100, training_loss: 2.77834
Epoch: 3/20, step: 21120, training_loss: 2.14373
Epoch: 3/20, step: 21140, training_loss: 2.39830
Epoch: 3/20, step: 21160, training_loss: 1.92504
Epoch: 3/20, step: 21180, training_loss: 1.96404
Epoch: 3/20, step: 21200, training_loss: 1.97360
Epoch: 3/20, step: 21220, training_loss: 2.00426
Epoch: 3/20, step: 21240, training_loss: 1.95697
Epoch: 3/20, step: 21260, training_loss: 2.38154
Epoch: 3/20, step: 21280, training_loss: 2.11810
Epoch: 3/20, step: 21300, training_loss: 1.45625
Epoch: 3/20, step: 21320, training_loss: 1.98346
Epoch: 3/20, step: 21340, training_loss: 2.55823
Epoch: 3/20, step: 21360, training_loss: 2.59113
Epoch: 3/20, step: 21380, training_loss: 2.16698
Epoch: 3/20, step: 21400, training_loss: 2.06237
Epoch: 3/20, step: 21420, training_loss: 2.19933
Epoch: 3/20, step: 21440, training_loss: 2.03559
Epoch: 3/20, step: 21460, training_loss: 1.99940
Epoch: 3/20, step: 21480, training_loss: 3.17598
Epoch: 3/20, step: 21500, training_loss: 2.07416
Epoch: 3/20, step: 21520, training_loss: 2.50422
Epoch: 3/20, step: 21540, training_loss: 2.11011
Epoch: 3/20, step: 21560, training_loss: 1.76842
Epoch: 3/20, step: 21580, training_loss: 2.72566
Epoch: 3/20, step: 21600, training_loss: 1.97936
Epoch: 3/20, step: 21620, training_loss: 2.31233
Epoch: 3/20, step: 21640, training_loss: 1.75931
Epoch: 3/20, step: 21660, training_loss: 1.61119
Epoch: 3/20, step: 21680, training_loss: 2.30886
Epoch: 3/20, step: 21700, training_loss: 2.79482
Epoch: 3/20, step: 21720, training_loss: 2.50442
Epoch: 3/20, step: 21740, training_loss: 1.79427
Epoch: 3/20, step: 21760, training_loss: 2.10708
Epoch: 3/20, step: 21780, training_loss: 2.02202
Epoch: 3/20, step: 21800, training_loss: 2.51823
Epoch: 3/20, step: 21820, training_loss: 1.44922
Epoch: 3/20, step: 21840, training_loss: 2.32243
Epoch: 3/20, step: 21860, training_loss: 1.83306
Epoch: 3/20, step: 21880, training_loss: 2.99071
Epoch: 3/20, step: 21900, training_loss: 2.40124
Epoch: 3/20, step: 21920, training_loss: 1.71845
Epoch: 3/20, step: 21940, training_loss: 1.33886
Epoch: 3/20, step: 21960, training_loss: 2.39262
Epoch: 3/20, step: 21980, training_loss: 1.79232
Epoch: 3/20, step: 22000, training_loss: 2.46166
accuracy: 0.45, validation_loss: 1.9915282726287842, num_samples: 100
Epoch: 3/20, step: 22020, training_loss: 2.28648
Epoch: 3/20, step: 22040, training_loss: 2.15350
Epoch: 3/20, step: 22060, training_loss: 2.37519
Epoch: 3/20, step: 22080, training_loss: 2.25134
Epoch: 3/20, step: 22100, training_loss: 2.74888
Epoch: 3/20, step: 22120, training_loss: 2.22557
Epoch: 3/20, step: 22140, training_loss: 2.50363
Epoch: 3/20, step: 22160, training_loss: 2.48496
Epoch: 3/20, step: 22180, training_loss: 2.24074
Epoch: 3/20, step: 22200, training_loss: 1.76947
Epoch: 3/20, step: 22220, training_loss: 2.40454
Epoch: 3/20, step: 22240, training_loss: 3.13682
Epoch: 3/20, step: 22260, training_loss: 2.59976
Epoch: 3/20, step: 22280, training_loss: 1.90413
Epoch: 3/20, step: 22300, training_loss: 2.64961
Epoch: 3/20, step: 22320, training_loss: 2.29295
Epoch: 3/20, step: 22340, training_loss: 3.06644
Epoch: 3/20, step: 22360, training_loss: 1.85692
Epoch: 3/20, step: 22380, training_loss: 1.71512
Epoch: 3/20, step: 22400, training_loss: 1.64687
Epoch: 3/20, step: 22420, training_loss: 1.64669
Epoch: 3/20, step: 22440, training_loss: 1.86494
Epoch: 3/20, step: 22460, training_loss: 2.62074
Epoch: 3/20, step: 22480, training_loss: 1.81154
Epoch: 3/20, step: 22500, training_loss: 2.79350
Epoch: 3/20, step: 22520, training_loss: 1.83025
Epoch: 3/20, step: 22540, training_loss: 3.05622
Epoch: 3/20, step: 22560, training_loss: 2.18052
Epoch: 3/20, step: 22580, training_loss: 1.57679
Epoch: 3/20, step: 22600, training_loss: 1.95660
Epoch: 3/20, step: 22620, training_loss: 1.89308
Epoch: 3/20, step: 22640, training_loss: 2.07710
Epoch: 3/20, step: 22660, training_loss: 2.36664
Epoch: 3/20, step: 22680, training_loss: 1.96354
Epoch: 3/20, step: 22700, training_loss: 1.78112
Epoch: 3/20, step: 22720, training_loss: 1.77983
Epoch: 3/20, step: 22740, training_loss: 2.99577
Epoch: 3/20, step: 22760, training_loss: 2.60702
Epoch: 3/20, step: 22780, training_loss: 1.77753
Epoch: 3/20, step: 22800, training_loss: 2.07684
Epoch: 3/20, step: 22820, training_loss: 2.53178
Epoch: 3/20, step: 22840, training_loss: 2.16031
Epoch: 3/20, step: 22860, training_loss: 1.99749
Epoch: 3/20, step: 22880, training_loss: 2.65751
Epoch: 3/20, step: 22900, training_loss: 2.23439
Epoch: 3/20, step: 22920, training_loss: 2.16859
Epoch: 3/20, step: 22940, training_loss: 2.73743
Epoch: 3/20, step: 22960, training_loss: 2.46429
Epoch: 3/20, step: 22980, training_loss: 1.77706
Epoch: 3/20, step: 23000, training_loss: 1.64543
accuracy: 0.44, validation_loss: 1.9956268072128296, num_samples: 100
Epoch: 3/20, step: 23020, training_loss: 2.13719
Epoch: 3/20, step: 23040, training_loss: 2.05183
Epoch: 3/20, step: 23060, training_loss: 2.43862
Epoch: 3/20, step: 23080, training_loss: 2.03116
Epoch: 3/20, step: 23100, training_loss: 1.94708
Epoch: 3/20, step: 23120, training_loss: 2.53998
Epoch: 3/20, step: 23140, training_loss: 1.90193
Epoch: 3/20, step: 23160, training_loss: 1.42052
Epoch: 3/20, step: 23180, training_loss: 2.03113
Epoch: 3/20, step: 23200, training_loss: 2.57834
Epoch: 3/20, step: 23220, training_loss: 2.07930
Epoch: 3/20, step: 23240, training_loss: 3.01756
Epoch: 3/20, step: 23260, training_loss: 2.47282
Epoch: 3/20, step: 23280, training_loss: 1.48940
Epoch: 3/20, step: 23300, training_loss: 1.32445
Epoch: 3/20, step: 23320, training_loss: 2.63294
Epoch: 3/20, step: 23340, training_loss: 2.13160
Epoch: 3/20, step: 23360, training_loss: 2.41991
Epoch: 3/20, step: 23380, training_loss: 1.67223
Epoch: 3/20, step: 23400, training_loss: 2.61702
Epoch: 3/20, step: 23420, training_loss: 1.95103
Epoch: 3/20, step: 23440, training_loss: 2.10135
Epoch: 3/20, step: 23460, training_loss: 2.55093
Epoch: 3/20, step: 23480, training_loss: 2.21250
Epoch: 3/20, step: 23500, training_loss: 2.29578
Epoch: 3/20, step: 23520, training_loss: 2.36961
Epoch: 3/20, step: 23540, training_loss: 2.70090
Epoch: 3/20, step: 23560, training_loss: 1.91832
Epoch: 3/20, step: 23580, training_loss: 2.92024
Epoch: 3/20, step: 23600, training_loss: 2.10804
Epoch: 3/20, step: 23620, training_loss: 1.35666
Epoch: 3/20, step: 23640, training_loss: 2.25114
Epoch: 3/20, step: 23660, training_loss: 2.21979
Epoch: 3/20, step: 23680, training_loss: 2.13403
Epoch: 3/20, step: 23700, training_loss: 1.50750
Epoch: 3/20, step: 23720, training_loss: 2.46426
Epoch: 3/20, step: 23740, training_loss: 1.70623
Epoch: 3/20, step: 23760, training_loss: 2.01326
Epoch: 3/20, step: 23780, training_loss: 1.68806
Epoch: 3/20, step: 23800, training_loss: 2.55720
Epoch: 3/20, step: 23820, training_loss: 1.87038
Epoch: 3/20, step: 23840, training_loss: 1.75164
Epoch: 3/20, step: 23860, training_loss: 2.50032
Epoch: 3/20, step: 23880, training_loss: 2.90335
Epoch: 3/20, step: 23900, training_loss: 2.08314
Epoch: 3/20, step: 23920, training_loss: 1.67273
Epoch: 3/20, step: 23940, training_loss: 2.07137
Epoch: 3/20, step: 23960, training_loss: 2.12651
Epoch: 3/20, step: 23980, training_loss: 1.99729
Epoch: 3/20, step: 24000, training_loss: 2.38278
accuracy: 0.48, validation_loss: 1.996675729751587, num_samples: 100
Epoch: 3/20, step: 24020, training_loss: 2.51387
Epoch: 3/20, step: 24040, training_loss: 2.84660
Epoch: 3/20, step: 24060, training_loss: 2.82818
Epoch: 3/20, step: 24080, training_loss: 1.33756
Epoch: 3/20, step: 24100, training_loss: 2.19154
Epoch: 3/20, step: 24120, training_loss: 2.25040
Epoch: 3/20, step: 24140, training_loss: 2.19452
Epoch: 3/20, step: 24160, training_loss: 2.38992
Epoch: 3/20, step: 24180, training_loss: 1.97568
Epoch: 3/20, step: 24200, training_loss: 2.42983
Epoch: 3/20, step: 24220, training_loss: 2.66865
Epoch: 3/20, step: 24240, training_loss: 2.58908
Epoch: 3/20, step: 24260, training_loss: 2.41433
Epoch: 3/20, step: 24280, training_loss: 2.59575
Epoch: 3/20, step: 24300, training_loss: 2.28370
Epoch: 3/20, step: 24320, training_loss: 1.19158
Epoch: 3/20, step: 24340, training_loss: 2.68409
Epoch: 3/20, step: 24360, training_loss: 1.91108
Epoch: 3/20, step: 24380, training_loss: 2.08042
Epoch: 3/20, step: 24400, training_loss: 2.38210
Epoch: 3/20, step: 24420, training_loss: 1.84725
Epoch: 3/20, step: 24440, training_loss: 2.16393
Epoch: 3/20, step: 24460, training_loss: 1.92972
Epoch: 3/20, step: 24480, training_loss: 1.97384
Epoch: 3/20, step: 24500, training_loss: 1.88700
Epoch: 3/20, step: 24520, training_loss: 2.69545
Epoch: 3/20, step: 24540, training_loss: 2.67046
Epoch: 3/20, step: 24560, training_loss: 2.61262
Epoch: 3/20, step: 24580, training_loss: 1.98091
Epoch: 3/20, step: 24600, training_loss: 1.69325
Epoch: 3/20, step: 24620, training_loss: 2.41137
Epoch: 3/20, step: 24640, training_loss: 2.26748
Epoch: 3/20, step: 24660, training_loss: 1.70048
Epoch: 3/20, step: 24680, training_loss: 2.62581
Epoch: 3/20, step: 24700, training_loss: 2.12916
Epoch: 3/20, step: 24720, training_loss: 1.93002
Epoch: 3/20, step: 24740, training_loss: 1.98972
Epoch: 3/20, step: 24760, training_loss: 2.84804
Epoch: 3/20, step: 24780, training_loss: 2.13628
Epoch: 3/20, step: 24800, training_loss: 2.85647
Epoch: 3/20, step: 24820, training_loss: 2.44558
Epoch: 3/20, step: 24840, training_loss: 1.90179
Epoch: 3/20, step: 24860, training_loss: 2.34282
Epoch: 3/20, step: 24880, training_loss: 2.33054
Epoch: 3/20, step: 24900, training_loss: 1.75874
Epoch: 3/20, step: 24920, training_loss: 1.77553
Epoch: 3/20, step: 24940, training_loss: 2.85389
Epoch: 3/20, step: 24960, training_loss: 2.51138
Epoch: 3/20, step: 24980, training_loss: 3.25253
Epoch: 3/20, step: 25000, training_loss: 1.16619
accuracy: 0.43, validation_loss: 2.126539707183838, num_samples: 100
Epoch: 3/20, step: 25020, training_loss: 1.94966
Epoch: 3/20, step: 25040, training_loss: 2.50844
Epoch: 3/20, step: 25060, training_loss: 1.80933
Epoch: 3/20, step: 25080, training_loss: 1.77145
Epoch: 3/20, step: 25100, training_loss: 2.88771
Epoch: 3/20, step: 25120, training_loss: 2.21788
Epoch: 3/20, step: 25140, training_loss: 2.40757
Epoch: 3/20, step: 25160, training_loss: 2.15045
Epoch: 3/20, step: 25180, training_loss: 2.30486
Epoch: 3/20, step: 25200, training_loss: 1.97873
Epoch: 3/20, step: 25220, training_loss: 2.51520
Epoch: 3/20, step: 25240, training_loss: 2.73588
Epoch: 3/20, step: 25260, training_loss: 1.67337
Epoch: 3/20, step: 25280, training_loss: 1.53536
Epoch: 3/20, step: 25300, training_loss: 1.32479
Epoch: 3/20, step: 25320, training_loss: 2.28676
Epoch: 3/20, step: 25340, training_loss: 2.49953
Epoch: 3/20, step: 25360, training_loss: 1.70819
Epoch: 3/20, step: 25380, training_loss: 2.28918
Epoch: 3/20, step: 25400, training_loss: 2.48255
Epoch: 3/20, step: 25420, training_loss: 1.71461
Epoch: 3/20, step: 25440, training_loss: 2.78297
Epoch: 3/20, step: 25460, training_loss: 2.80444
Epoch: 3/20, step: 25480, training_loss: 2.18561
Epoch: 3/20, step: 25500, training_loss: 2.03039
Epoch: 3/20, step: 25520, training_loss: 2.10391
Epoch: 3/20, step: 25540, training_loss: 1.59060
Epoch: 3/20, step: 25560, training_loss: 1.96027
Epoch: 3/20, step: 25580, training_loss: 2.38228
Epoch: 3/20, step: 25600, training_loss: 2.34974
Epoch: 3/20, step: 25620, training_loss: 2.57540
Epoch: 3/20, step: 25640, training_loss: 2.02291
Epoch: 3/20, step: 25660, training_loss: 1.95844
Epoch: 3/20, step: 25680, training_loss: 2.49663
Epoch: 3/20, step: 25700, training_loss: 1.38613
Epoch: 3/20, step: 25720, training_loss: 2.36148
Epoch: 3/20, step: 25740, training_loss: 2.47421
Epoch: 3/20, step: 25760, training_loss: 2.19679
Epoch: 3/20, step: 25780, training_loss: 1.83790
Epoch: 3/20, step: 25800, training_loss: 2.47483
Epoch: 3/20, step: 25820, training_loss: 2.19242
Epoch: 3/20, step: 25840, training_loss: 3.02381
Epoch: 3/20, step: 25860, training_loss: 2.26193
Epoch: 3/20, step: 25880, training_loss: 2.29024
Epoch: 3/20, step: 25900, training_loss: 2.72596
Epoch: 3/20, step: 25920, training_loss: 1.87038
Epoch: 3/20, step: 25940, training_loss: 1.95471
Epoch: 3/20, step: 25960, training_loss: 2.08621
Epoch: 3/20, step: 25980, training_loss: 3.03307
Epoch: 3/20, step: 26000, training_loss: 2.05613
accuracy: 0.35, validation_loss: 2.3185150623321533, num_samples: 100
Epoch: 3/20, step: 26020, training_loss: 1.64844
Epoch: 3/20, step: 26040, training_loss: 1.93669
Epoch: 3/20, step: 26060, training_loss: 2.77636
Epoch: 3/20, step: 26080, training_loss: 1.01436
Epoch: 3/20, step: 26100, training_loss: 2.20526
Epoch: 3/20, step: 26120, training_loss: 2.37557
Epoch: 3/20, step: 26140, training_loss: 1.30255
Epoch: 3/20, step: 26160, training_loss: 2.06888
Epoch: 3/20, step: 26180, training_loss: 2.64843
Epoch: 3/20, step: 26200, training_loss: 2.63154
Epoch: 3/20, step: 26220, training_loss: 1.89745
Epoch: 3/20, step: 26240, training_loss: 2.08955
Epoch: 3/20, step: 26260, training_loss: 2.46718
Epoch: 3/20, step: 26280, training_loss: 2.24106
Epoch: 3/20, step: 26300, training_loss: 1.69833
Epoch: 3/20, step: 26320, training_loss: 1.78264
Epoch: 3/20, step: 26340, training_loss: 2.51716
Epoch: 3/20, step: 26360, training_loss: 2.27812
Epoch: 3/20, step: 26380, training_loss: 2.16688
Epoch: 3/20, step: 26400, training_loss: 2.36577
Epoch: 3/20, step: 26420, training_loss: 2.50066
Epoch: 3/20, step: 26440, training_loss: 2.36646
Epoch: 3/20, step: 26460, training_loss: 2.31317
Epoch: 3/20, step: 26480, training_loss: 2.23641
Epoch: 3/20, step: 26500, training_loss: 1.84589
Epoch: 3/20, step: 26520, training_loss: 1.69434
Epoch: 3/20, step: 26540, training_loss: 1.55558
Epoch: 3/20, step: 26560, training_loss: 2.32105
Epoch: 3/20, step: 26580, training_loss: 2.81526
Epoch: 3/20, step: 26600, training_loss: 1.80879
Epoch: 3/20, step: 26620, training_loss: 2.86276
Epoch: 3/20, step: 26640, training_loss: 2.92251
Epoch: 3/20, step: 26660, training_loss: 2.37810
Epoch: 3/20, step: 26680, training_loss: 2.30444
Epoch: 3/20, step: 26700, training_loss: 2.45055
Epoch: 3/20, step: 26720, training_loss: 2.28646
Epoch: 3/20, step: 26740, training_loss: 2.06447
Epoch: 3/20, step: 26760, training_loss: 2.23978
Epoch: 3/20, step: 26780, training_loss: 2.14506
Epoch: 3/20, step: 26800, training_loss: 1.84815
Epoch: 3/20, step: 26820, training_loss: 1.90927
Epoch: 3/20, step: 26840, training_loss: 2.81740
Epoch: 3/20, step: 26860, training_loss: 1.47311
Epoch: 3/20, step: 26880, training_loss: 1.94001
Epoch: 3/20, step: 26900, training_loss: 1.90364
Epoch: 3/20, step: 26920, training_loss: 1.53312
Epoch: 3/20, step: 26940, training_loss: 2.04832
Epoch: 3/20, step: 26960, training_loss: 2.31416
Epoch: 3/20, step: 26980, training_loss: 2.83040
Epoch: 3/20, step: 27000, training_loss: 2.08442
accuracy: 0.41, validation_loss: 2.1466786861419678, num_samples: 100
Epoch: 3/20, step: 27020, training_loss: 2.25133
Epoch: 3/20, step: 27040, training_loss: 2.63585
Epoch: 3/20, step: 27060, training_loss: 1.26077
Epoch: 3/20, step: 27080, training_loss: 2.03189
Epoch: 3/20, step: 27100, training_loss: 2.25398
Epoch: 3/20, step: 27120, training_loss: 1.51671
Epoch: 3/20, step: 27140, training_loss: 2.02809
Epoch: 3/20, step: 27160, training_loss: 2.44342
Epoch: 3/20, step: 27180, training_loss: 1.72110
Epoch: 3/20, step: 27200, training_loss: 2.49148
Epoch: 3/20, step: 27220, training_loss: 1.67330
Epoch: 3/20, step: 27240, training_loss: 2.49874
Epoch: 3/20, step: 27260, training_loss: 2.09529
Epoch: 3/20, step: 27280, training_loss: 1.66895
Epoch: 3/20, step: 27300, training_loss: 2.81997
Epoch: 3/20, step: 27320, training_loss: 2.13399
Epoch: 3/20, step: 27340, training_loss: 2.50670
Epoch: 3/20, step: 27360, training_loss: 2.07342
Epoch: 3/20, step: 27380, training_loss: 1.67766
Epoch: 3/20, step: 27400, training_loss: 3.07250
Epoch: 3/20, step: 27420, training_loss: 1.51886
Epoch: 3/20, step: 27440, training_loss: 2.67772
Epoch: 3/20, step: 27460, training_loss: 1.91383
Epoch: 3/20, step: 27480, training_loss: 2.35699
Epoch: 3/20, step: 27500, training_loss: 2.39537
Epoch: 3/20, step: 27520, training_loss: 2.14429
Epoch: 3/20, step: 27540, training_loss: 2.77149
Epoch: 3/20, step: 27560, training_loss: 2.24451
Epoch: 3/20, step: 27580, training_loss: 1.33477
Epoch: 3/20, step: 27600, training_loss: 2.43693
Epoch: 3/20, step: 27620, training_loss: 1.98622
Epoch: 3/20, step: 27640, training_loss: 1.90902
Epoch: 3/20, step: 27660, training_loss: 1.81887
Epoch: 3/20, step: 27680, training_loss: 2.13910
Epoch: 3/20, step: 27700, training_loss: 2.22044
Epoch: 3/20, step: 27720, training_loss: 2.07506
Epoch: 3/20, step: 27740, training_loss: 2.50473
Epoch: 3/20, step: 27760, training_loss: 1.37568
Epoch: 3/20, step: 27780, training_loss: 1.88604
Epoch: 3/20, step: 27800, training_loss: 2.08738
Epoch: 3/20, step: 27820, training_loss: 2.02895
Epoch: 3/20, step: 27840, training_loss: 2.78240
Epoch: 3/20, step: 27860, training_loss: 1.88242
Epoch: 3/20, step: 27880, training_loss: 2.32111
Epoch: 3/20, step: 27900, training_loss: 2.18403
Epoch: 3/20, step: 27920, training_loss: 2.30365
Epoch: 3/20, step: 27940, training_loss: 2.80658
Epoch: 3/20, step: 27960, training_loss: 2.02215
Epoch: 3/20, step: 27980, training_loss: 2.39845
Epoch: 3/20, step: 28000, training_loss: 2.40257
accuracy: 0.51, validation_loss: 1.8940002918243408, num_samples: 100
Epoch: 3/20, step: 28020, training_loss: 2.56597
Epoch: 3/20, step: 28040, training_loss: 2.14810
Epoch: 3/20, step: 28060, training_loss: 2.20851
Epoch: 3/20, step: 28080, training_loss: 2.14739
Epoch: 3/20, step: 28100, training_loss: 2.79314
Epoch: 3/20, step: 28120, training_loss: 2.19888
Epoch: 3/20, step: 28140, training_loss: 2.41395
Epoch: 3/20, step: 28160, training_loss: 2.21056
Epoch: 3/20, step: 28180, training_loss: 1.81202
Epoch: 3/20, step: 28200, training_loss: 2.70092
Epoch: 3/20, step: 28220, training_loss: 2.02963
Epoch: 3/20, step: 28240, training_loss: 2.23268
Epoch: 3/20, step: 28260, training_loss: 2.06718
Epoch: 3/20, step: 28280, training_loss: 2.52473
Epoch: 3/20, step: 28300, training_loss: 1.62596
Epoch: 3/20, step: 28320, training_loss: 1.29200
Epoch: 3/20, step: 28340, training_loss: 1.82747
Epoch: 3/20, step: 28360, training_loss: 2.59726
Epoch: 3/20, step: 28380, training_loss: 2.38349
Epoch: 3/20, step: 28400, training_loss: 1.89368
Epoch: 3/20, step: 28420, training_loss: 1.60774
Epoch: 3/20, step: 28440, training_loss: 2.62615
Epoch: 3/20, step: 28460, training_loss: 1.37821
Epoch: 3/20, step: 28480, training_loss: 2.45097
Epoch: 3/20, step: 28500, training_loss: 1.89781
Epoch: 3/20, step: 28520, training_loss: 1.48550
Epoch: 3/20, step: 28540, training_loss: 2.27240
Epoch: 3/20, step: 28560, training_loss: 2.06283
Epoch: 3/20, step: 28580, training_loss: 2.07067
Epoch: 3/20, step: 28600, training_loss: 2.32774
Epoch: 3/20, step: 28620, training_loss: 2.75524
Epoch: 3/20, step: 28640, training_loss: 2.56614
Epoch: 3/20, step: 28660, training_loss: 2.39201
Epoch: 3/20, step: 28680, training_loss: 2.57431
Epoch: 3/20, step: 28700, training_loss: 1.69178
Epoch: 3/20, step: 28720, training_loss: 2.43647
Epoch: 3/20, step: 28740, training_loss: 2.35883
Epoch: 3/20, step: 28760, training_loss: 1.59539
Epoch: 3/20, step: 28780, training_loss: 2.30896
Epoch: 3/20, step: 28800, training_loss: 1.29879
Epoch: 3/20, step: 28820, training_loss: 2.70025
Epoch: 3/20, step: 28840, training_loss: 2.23868
Epoch: 3/20, step: 28860, training_loss: 2.54047
Epoch: 3/20, step: 28880, training_loss: 2.01039
Epoch: 3/20, step: 28900, training_loss: 2.31873
Epoch: 3/20, step: 28920, training_loss: 1.85876
Epoch: 3/20, step: 28940, training_loss: 1.79366
Epoch: 3/20, step: 28960, training_loss: 2.47484
Epoch: 3/20, step: 28980, training_loss: 2.45282
Epoch: 3/20, step: 29000, training_loss: 1.75107
accuracy: 0.41, validation_loss: 2.239037275314331, num_samples: 100
Epoch: 3/20, step: 29020, training_loss: 2.51230
Epoch: 3/20, step: 29040, training_loss: 2.68762
Epoch: 3/20, step: 29060, training_loss: 3.18700
Epoch: 3/20, step: 29080, training_loss: 2.87376
Epoch: 3/20, step: 29100, training_loss: 1.35093
Epoch: 3/20, step: 29120, training_loss: 1.51403
Epoch: 3/20, step: 29140, training_loss: 1.48711
Epoch: 3/20, step: 29160, training_loss: 1.11807
Epoch: 3/20, step: 29180, training_loss: 2.76217
Epoch: 3/20, step: 29200, training_loss: 2.64740
Epoch: 3/20, step: 29220, training_loss: 2.66028
Epoch: 3/20, step: 29240, training_loss: 2.20263
Epoch: 3/20, step: 29260, training_loss: 2.28081
Epoch: 3/20, step: 29280, training_loss: 1.85073
Epoch: 3/20, step: 29300, training_loss: 2.73241
Epoch: 3/20, step: 29320, training_loss: 2.39338
Epoch: 3/20, step: 29340, training_loss: 3.20330
Epoch: 3/20, step: 29360, training_loss: 2.21929
Epoch: 3/20, step: 29380, training_loss: 2.65514
Epoch: 3/20, step: 29400, training_loss: 1.85694
Epoch: 3/20, step: 29420, training_loss: 3.79031
Epoch: 3/20, step: 29440, training_loss: 2.44719
Epoch: 3/20, step: 29460, training_loss: 2.10941
Epoch: 3/20, step: 29480, training_loss: 3.43646
Epoch: 3/20, step: 29500, training_loss: 1.74962
Epoch: 3/20, step: 29520, training_loss: 1.38040
Epoch: 3/20, step: 29540, training_loss: 1.63815
Epoch: 3/20, step: 29560, training_loss: 1.34636
Epoch: 3/20, step: 29580, training_loss: 2.52615
Epoch: 3/20, step: 29600, training_loss: 2.64010
Epoch: 3/20, step: 29620, training_loss: 1.93196
Epoch: 3/20, step: 29640, training_loss: 2.26447
Epoch: 3/20, step: 29660, training_loss: 1.80343
Epoch: 3/20, step: 29680, training_loss: 3.47604
Epoch: 3/20, step: 29700, training_loss: 1.73798
Epoch: 3/20, step: 29720, training_loss: 2.25004
Epoch: 3/20, step: 29740, training_loss: 2.69320
Epoch: 3/20, step: 29760, training_loss: 2.21142
Epoch: 3/20, step: 29780, training_loss: 1.85410
Epoch: 3/20, step: 29800, training_loss: 2.06913
Epoch: 3/20, step: 29820, training_loss: 1.88486
Epoch: 3/20, step: 29840, training_loss: 3.12995
Epoch: 3/20, step: 29860, training_loss: 2.32998
Epoch: 3/20, step: 29880, training_loss: 1.70360
Epoch: 3/20, step: 29900, training_loss: 2.74069
Epoch: 3/20, step: 29920, training_loss: 1.58703
Epoch: 3/20, step: 29940, training_loss: 2.23553
Epoch: 3/20, step: 29960, training_loss: 2.74373
Epoch: 3/20, step: 29980, training_loss: 1.91530
Epoch: 3/20, step: 30000, training_loss: 1.50928
accuracy: 0.4, validation_loss: 2.2932517528533936, num_samples: 100
Epoch: 3/20, step: 30020, training_loss: 2.02955
Epoch: 3/20, step: 30040, training_loss: 1.67401
Epoch: 3/20, step: 30060, training_loss: 2.10735
Epoch: 3/20, step: 30080, training_loss: 1.99710
Epoch: 3/20, step: 30100, training_loss: 3.07684
Epoch: 3/20, step: 30120, training_loss: 2.46999
Epoch: 3/20, step: 30140, training_loss: 2.54405
Epoch: 3/20, step: 30160, training_loss: 2.13669
Epoch: 3/20, step: 30180, training_loss: 1.98446
Epoch: 3/20, step: 30200, training_loss: 2.15272
Epoch: 3/20, step: 30220, training_loss: 1.15772
Epoch: 3/20, step: 30240, training_loss: 1.86536
Epoch: 3/20, step: 30260, training_loss: 1.72107
Epoch: 3/20, step: 30280, training_loss: 2.03463
Epoch: 3/20, step: 30300, training_loss: 2.11448
Epoch: 3/20, step: 30320, training_loss: 2.20166
Epoch: 3/20, step: 30340, training_loss: 2.57937
Epoch: 3/20, step: 30360, training_loss: 2.20063
Epoch: 3/20, step: 30380, training_loss: 2.88395
Epoch: 3/20, step: 30400, training_loss: 2.09085
Epoch: 3/20, step: 30420, training_loss: 2.94438
Epoch: 3/20, step: 30440, training_loss: 3.00437
Epoch: 3/20, step: 30460, training_loss: 2.16275
Epoch: 3/20, step: 30480, training_loss: 2.46420
Epoch: 3/20, step: 30500, training_loss: 1.99679
Epoch: 3/20, step: 30520, training_loss: 1.68198
Epoch: 3/20, step: 30540, training_loss: 2.41404
Epoch: 3/20, step: 30560, training_loss: 1.96084
Epoch: 3/20, step: 30580, training_loss: 2.15079
Epoch: 3/20, step: 30600, training_loss: 2.51346
Epoch: 3/20, step: 30620, training_loss: 3.32912
Epoch: 3/20, step: 30640, training_loss: 1.63266
Epoch: 3/20, step: 30660, training_loss: 2.39542
Epoch: 3/20, step: 30680, training_loss: 3.27986
Epoch: 3/20, step: 30700, training_loss: 1.82867
Epoch: 3/20, step: 30720, training_loss: 2.32281
Epoch: 3/20, step: 30740, training_loss: 1.74543
Epoch: 3/20, step: 30760, training_loss: 2.27183
Epoch: 3/20, step: 30780, training_loss: 1.83062
Epoch: 3/20, step: 30800, training_loss: 1.38840
Epoch: 3/20, step: 30820, training_loss: 1.63129
Epoch: 3/20, step: 30840, training_loss: 1.89226
Epoch: 3/20, step: 30860, training_loss: 1.44672
Epoch: 3/20, step: 30880, training_loss: 1.67198
Epoch: 3/20, step: 30900, training_loss: 2.95569
Epoch: 3/20, step: 30920, training_loss: 1.88823
Epoch: 3/20, step: 30940, training_loss: 2.00731
Epoch: 3/20, step: 30960, training_loss: 2.27564
Epoch: 3/20, step: 30980, training_loss: 2.20804
Epoch: 3/20, step: 31000, training_loss: 2.22177
accuracy: 0.41, validation_loss: 2.0778844356536865, num_samples: 100
Epoch: 3/20, step: 31020, training_loss: 1.72552
Epoch: 3/20, step: 31040, training_loss: 2.59153
Epoch: 3/20, step: 31060, training_loss: 1.37457
Epoch: 3/20, step: 31080, training_loss: 2.72549
Epoch: 3/20, step: 31100, training_loss: 2.43340
Epoch: 3/20, step: 31120, training_loss: 2.71649
Epoch: 3/20, step: 31140, training_loss: 2.27471
Epoch: 3/20, step: 31160, training_loss: 2.06839
Epoch: 3/20, step: 31180, training_loss: 1.67799
Epoch: 3/20, step: 31200, training_loss: 2.50973
Epoch: 3/20, step: 31220, training_loss: 2.52573
Epoch: 3/20, step: 31240, training_loss: 1.81698
Epoch: 3/20, step: 31260, training_loss: 1.46151
Epoch: 3/20, step: 31280, training_loss: 1.92960
Epoch: 3/20, step: 31300, training_loss: 1.42577
Epoch: 3/20, step: 31320, training_loss: 2.24438
Epoch: 3/20, step: 31340, training_loss: 2.01779
Epoch: 3/20, step: 31360, training_loss: 2.30383
Epoch: 3/20, step: 31380, training_loss: 2.89630
Epoch: 3/20, step: 31400, training_loss: 2.16482
Epoch: 3/20, step: 31420, training_loss: 2.86660
Epoch: 3/20, step: 31440, training_loss: 1.47432
Epoch: 3/20, step: 31460, training_loss: 2.30204
Epoch: 3/20, step: 31480, training_loss: 2.24550
Epoch: 3/20, step: 31500, training_loss: 2.05521
Epoch: 3/20, step: 31520, training_loss: 2.05395
Epoch: 3/20, step: 31540, training_loss: 2.32789
Epoch: 3/20, step: 31560, training_loss: 1.94380
Epoch: 3/20, step: 31580, training_loss: 2.67320
Epoch: 3/20, step: 31600, training_loss: 2.52074
Epoch: 3/20, step: 31620, training_loss: 1.69768
Epoch: 3/20, step: 31640, training_loss: 1.72330
Epoch: 3/20, step: 31660, training_loss: 2.82524
Epoch: 3/20, step: 31680, training_loss: 1.85515
Epoch: 3/20, step: 31700, training_loss: 1.63002
Epoch: 3/20, step: 31720, training_loss: 2.32845
Epoch: 3/20, step: 31740, training_loss: 1.78296
Epoch: 3/20, step: 31760, training_loss: 2.04751
Epoch: 3/20, step: 31780, training_loss: 1.75974
Epoch: 3/20, step: 31800, training_loss: 1.57666
Epoch: 3/20, step: 31820, training_loss: 2.21997
Epoch: 3/20, step: 31840, training_loss: 2.44779
Epoch: 3/20, step: 31860, training_loss: 1.92967
Epoch: 3/20, step: 31880, training_loss: 2.63097
Epoch: 3/20, step: 31900, training_loss: 2.26538
Epoch: 3/20, step: 31920, training_loss: 2.14719
Epoch: 3/20, step: 31940, training_loss: 1.86201
Epoch: 3/20, step: 31960, training_loss: 1.76154
Epoch: 3/20, step: 31980, training_loss: 1.48635
Epoch: 3/20, step: 32000, training_loss: 3.11410
accuracy: 0.43, validation_loss: 1.9327396154403687, num_samples: 100
Epoch: 3/20, step: 32020, training_loss: 3.02346
Epoch: 3/20, step: 32040, training_loss: 1.66562
Epoch: 3/20, step: 32060, training_loss: 2.43273
Epoch: 3/20, step: 32080, training_loss: 2.12326
Epoch: 3/20, step: 32100, training_loss: 2.26899
Epoch: 3/20, step: 32120, training_loss: 2.51916
Epoch: 3/20, step: 32140, training_loss: 2.84497
Epoch: 3/20, step: 32160, training_loss: 2.64430
Epoch: 3/20, step: 32180, training_loss: 2.43105
Epoch: 3/20, step: 32200, training_loss: 2.14748
Epoch: 3/20, step: 32220, training_loss: 2.11083
Epoch: 3/20, step: 32240, training_loss: 1.90081
Epoch: 3/20, step: 32260, training_loss: 2.63650
Epoch: 3/20, step: 32280, training_loss: 2.46624
Epoch: 3/20, step: 32300, training_loss: 2.06872
Epoch: 3/20, step: 32320, training_loss: 2.48002
Epoch: 3/20, step: 32340, training_loss: 2.51640
Epoch: 3/20, step: 32360, training_loss: 2.12152
Epoch: 3/20, step: 32380, training_loss: 1.78666
Epoch: 3/20, step: 32400, training_loss: 2.92291
Epoch: 3/20, step: 32420, training_loss: 2.64714
Epoch: 3/20, step: 32440, training_loss: 2.36148
Epoch: 3/20, step: 32460, training_loss: 1.96085
Epoch: 3/20, step: 32480, training_loss: 2.10794
Epoch: 3/20, step: 32500, training_loss: 2.11886
Epoch: 3/20, step: 32520, training_loss: 1.54960
Epoch: 3/20, step: 32540, training_loss: 1.96203
Epoch: 3/20, step: 32560, training_loss: 2.97238
Epoch: 3/20, step: 32580, training_loss: 2.99246
Epoch: 3/20, step: 32600, training_loss: 1.77671
Epoch: 3/20, step: 32620, training_loss: 2.57222
Epoch: 3/20, step: 32640, training_loss: 2.34755
Epoch: 3/20, step: 32660, training_loss: 2.09471
Epoch: 3/20, step: 32680, training_loss: 1.87481
Epoch: 3/20, step: 32700, training_loss: 1.54112
Epoch: 3/20, step: 32720, training_loss: 2.36966
Epoch: 3/20, step: 32740, training_loss: 3.05495
Epoch: 3/20, step: 32760, training_loss: 2.41000
Epoch: 3/20, step: 32780, training_loss: 2.56967
Epoch: 3/20, step: 32800, training_loss: 2.37091
Epoch: 3/20, step: 32820, training_loss: 1.60561
Epoch: 3/20, step: 32840, training_loss: 2.62798
Epoch: 3/20, step: 32860, training_loss: 2.02102
Epoch: 3/20, step: 32880, training_loss: 2.18554
Epoch: 3/20, step: 32900, training_loss: 1.26844
Epoch: 3/20, step: 32920, training_loss: 1.83117
Epoch: 3/20, step: 32940, training_loss: 2.76755
Epoch: 3/20, step: 32960, training_loss: 2.29275
Epoch: 3/20, step: 32980, training_loss: 1.84435
Epoch: 3/20, step: 33000, training_loss: 2.23617
accuracy: 0.35, validation_loss: 2.197476863861084, num_samples: 100
Epoch: 3/20, step: 33020, training_loss: 1.72310
Epoch: 3/20, step: 33040, training_loss: 2.10527
Epoch: 3/20, step: 33060, training_loss: 2.37150
Epoch: 3/20, step: 33080, training_loss: 1.06118
Epoch: 3/20, step: 33100, training_loss: 2.15161
Epoch: 3/20, step: 33120, training_loss: 1.96505
Epoch: 3/20, step: 33140, training_loss: 2.33343
Epoch: 3/20, step: 33160, training_loss: 2.14411
Epoch: 3/20, step: 33180, training_loss: 2.14870
Epoch: 3/20, step: 33200, training_loss: 1.77246
Epoch: 3/20, step: 33220, training_loss: 1.21872
Epoch: 3/20, step: 33240, training_loss: 2.40587
Epoch: 3/20, step: 33260, training_loss: 1.96155
Epoch: 3/20, step: 33280, training_loss: 1.66994
Epoch: 3/20, step: 33300, training_loss: 2.97340
Epoch: 3/20, step: 33320, training_loss: 1.74645
Epoch: 3/20, step: 33340, training_loss: 3.12404
Epoch: 3/20, step: 33360, training_loss: 2.92685
Epoch: 3/20, step: 33380, training_loss: 2.82020
Epoch: 3/20, step: 33400, training_loss: 2.23313
Epoch: 3/20, step: 33420, training_loss: 2.48846
Epoch: 3/20, step: 33440, training_loss: 2.03961
Epoch: 3/20, step: 33460, training_loss: 1.82283
Epoch: 3/20, step: 33480, training_loss: 2.30146
Epoch: 3/20, step: 33500, training_loss: 1.24225
Epoch: 3/20, step: 33520, training_loss: 2.72092
Epoch: 3/20, step: 33540, training_loss: 2.20005
Epoch: 3/20, step: 33560, training_loss: 2.03768
Epoch: 3/20, step: 33580, training_loss: 2.67848
Epoch: 3/20, step: 33600, training_loss: 1.63861
Epoch: 3/20, step: 33620, training_loss: 2.33535
Epoch: 3/20, step: 33640, training_loss: 1.99970
Epoch: 3/20, step: 33660, training_loss: 2.08189
Epoch: 3/20, step: 33680, training_loss: 1.82013
Epoch: 3/20, step: 33700, training_loss: 1.15825
Epoch: 3/20, step: 33720, training_loss: 2.14109
Epoch: 3/20, step: 33740, training_loss: 1.80897
Epoch: 3/20, step: 33760, training_loss: 2.33499
Epoch: 3/20, step: 33780, training_loss: 2.47358
Epoch: 3/20, step: 33800, training_loss: 1.44425
Epoch: 3/20, step: 33820, training_loss: 1.97511
Epoch: 3/20, step: 33840, training_loss: 2.55887
Epoch: 3/20, step: 33860, training_loss: 1.64164
Epoch: 3/20, step: 33880, training_loss: 2.51565
Epoch: 3/20, step: 33900, training_loss: 1.68627
Epoch: 3/20, step: 33920, training_loss: 2.46610
Epoch: 3/20, step: 33940, training_loss: 2.15372
Epoch: 3/20, step: 33960, training_loss: 2.01904
Epoch: 3/20, step: 33980, training_loss: 2.03207
Epoch: 3/20, step: 34000, training_loss: 1.78148
accuracy: 0.43, validation_loss: 2.351919174194336, num_samples: 100
Epoch: 3/20, step: 34020, training_loss: 2.12440
Epoch: 3/20, step: 34040, training_loss: 1.94766
Epoch: 3/20, step: 34060, training_loss: 2.11164
Epoch: 3/20, step: 34080, training_loss: 2.47780
Epoch: 3/20, step: 34100, training_loss: 2.59062
Epoch: 3/20, step: 34120, training_loss: 1.75851
Epoch: 3/20, step: 34140, training_loss: 2.81011
Epoch: 3/20, step: 34160, training_loss: 2.22581
Epoch: 3/20, step: 34180, training_loss: 1.90247
Epoch: 3/20, step: 34200, training_loss: 2.64023
Epoch: 3/20, step: 34220, training_loss: 2.15738
Epoch: 3/20, step: 34240, training_loss: 1.86758
Epoch: 3/20, step: 34260, training_loss: 1.84704
Epoch: 3/20, step: 34280, training_loss: 2.07334
Epoch: 3/20, step: 34300, training_loss: 2.42533
Epoch: 3/20, step: 34320, training_loss: 2.02960
Epoch: 3/20, step: 34340, training_loss: 2.59337
Epoch: 3/20, step: 34360, training_loss: 1.48272
Epoch: 3/20, step: 34380, training_loss: 1.61518
Epoch: 3/20, step: 34400, training_loss: 2.91445
Epoch: 3/20, step: 34420, training_loss: 2.66840
Epoch: 3/20, step: 34440, training_loss: 1.82976
Epoch: 3/20, step: 34460, training_loss: 2.53285
Epoch: 3/20, step: 34480, training_loss: 2.70273
Epoch: 3/20, step: 34500, training_loss: 2.75420
Epoch: 3/20, step: 34520, training_loss: 2.02994
Epoch: 3/20, step: 34540, training_loss: 1.81371
Epoch: 3/20, step: 34560, training_loss: 2.13712
Epoch: 3/20, step: 34580, training_loss: 1.49133
Epoch: 3/20, step: 34600, training_loss: 3.12053
Epoch: 3/20, step: 34620, training_loss: 1.71935
Epoch: 3/20, step: 34640, training_loss: 2.69005
Epoch: 3/20, step: 34660, training_loss: 2.30390
Epoch: 3/20, step: 34680, training_loss: 2.45084
Epoch: 3/20, step: 34700, training_loss: 2.51045
Epoch: 3/20, step: 34720, training_loss: 2.06723
Epoch: 3/20, step: 34740, training_loss: 2.32687
Epoch: 3/20, step: 34760, training_loss: 1.83416
Epoch: 3/20, step: 34780, training_loss: 2.80937
Epoch: 3/20, step: 34800, training_loss: 1.93631
Epoch: 3/20, step: 34820, training_loss: 1.59733
Epoch: 3/20, step: 34840, training_loss: 2.34814
Epoch: 3/20, step: 34860, training_loss: 2.31846
Epoch: 3/20, step: 34880, training_loss: 1.56159
Epoch: 3/20, step: 34900, training_loss: 2.26510
Epoch: 3/20, step: 34920, training_loss: 2.97761
Epoch: 3/20, step: 34940, training_loss: 2.30263
Epoch: 3/20, step: 34960, training_loss: 1.19131
Epoch: 3/20, step: 34980, training_loss: 2.09842
Epoch: 3/20, step: 35000, training_loss: 2.47165
accuracy: 0.49, validation_loss: 2.0650339126586914, num_samples: 100
Epoch: 3/20, step: 35020, training_loss: 1.89558
Epoch: 3/20, step: 35040, training_loss: 1.45561
Epoch: 3/20, step: 35060, training_loss: 2.15444
Epoch: 3/20, step: 35080, training_loss: 2.11501
Epoch: 3/20, step: 35100, training_loss: 2.47947
Epoch: 3/20, step: 35120, training_loss: 1.24274
Epoch: 3/20, step: 35140, training_loss: 2.56602
Epoch: 3/20, step: 35160, training_loss: 1.70920
Epoch: 3/20, step: 35180, training_loss: 1.40408
Epoch: 3/20, step: 35200, training_loss: 2.13400
Epoch: 3/20, step: 35220, training_loss: 2.32200
Epoch: 3/20, step: 35240, training_loss: 2.86043
Epoch: 3/20, step: 35260, training_loss: 2.31241
Epoch: 3/20, step: 35280, training_loss: 2.51497
Epoch: 3/20, step: 35300, training_loss: 2.35036
Epoch: 3/20, step: 35320, training_loss: 2.43188
Epoch: 3/20, step: 35340, training_loss: 2.61090
Epoch: 3/20, step: 35360, training_loss: 2.48549
Epoch: 3/20, step: 35380, training_loss: 2.45581
Epoch: 3/20, step: 35400, training_loss: 1.08448
Epoch: 3/20, step: 35420, training_loss: 2.26902
Epoch: 3/20, step: 35440, training_loss: 2.03238
Epoch: 3/20, step: 35460, training_loss: 2.93999
Epoch: 3/20, step: 35480, training_loss: 2.77347
Epoch: 3/20, step: 35500, training_loss: 1.86030
Epoch: 3/20, step: 35520, training_loss: 3.15719
Epoch: 3/20, step: 35540, training_loss: 2.49685
Epoch: 3/20, step: 35560, training_loss: 2.42148
Epoch: 3/20, step: 35580, training_loss: 2.52926
Epoch: 3/20, step: 35600, training_loss: 1.88358
Epoch: 3/20, step: 35620, training_loss: 1.80682
Epoch: 3/20, step: 35640, training_loss: 2.49500
Epoch: 3/20, step: 35660, training_loss: 2.41829
Epoch: 3/20, step: 35680, training_loss: 2.12044
Epoch: 3/20, step: 35700, training_loss: 2.23385
Epoch: 3/20, step: 35720, training_loss: 2.40997
Epoch: 3/20, step: 35740, training_loss: 2.31349
Epoch: 3/20, step: 35760, training_loss: 2.27726
Epoch: 3/20, step: 35780, training_loss: 2.24484
Epoch: 3/20, step: 35800, training_loss: 2.92122
Epoch: 3/20, step: 35820, training_loss: 2.37975
Epoch: 3/20, step: 35840, training_loss: 1.77274
Epoch: 3/20, step: 35860, training_loss: 1.97682
Epoch: 3/20, step: 35880, training_loss: 2.68613
Epoch: 3/20, step: 35900, training_loss: 2.01237
Epoch: 3/20, step: 35920, training_loss: 2.77498
Epoch: 3/20, step: 35940, training_loss: 2.63677
Epoch: 3/20, step: 35960, training_loss: 2.20539
Epoch: 3/20, step: 35980, training_loss: 2.55670
Epoch: 3/20, step: 36000, training_loss: 1.31809
accuracy: 0.39, validation_loss: 2.2237048149108887, num_samples: 100
Epoch: 3/20, step: 36020, training_loss: 1.97698
Epoch: 3/20, step: 36040, training_loss: 1.55079
Epoch: 3/20, step: 36060, training_loss: 2.63136
Epoch: 3/20, step: 36080, training_loss: 2.13932
Epoch: 3/20, step: 36100, training_loss: 3.14071
Epoch: 3/20, step: 36120, training_loss: 2.63299
Epoch: 3/20, step: 36140, training_loss: 3.03524
Epoch: 3/20, step: 36160, training_loss: 2.08868
Epoch: 3/20, step: 36180, training_loss: 1.80908
Epoch: 3/20, step: 36200, training_loss: 1.77623
Epoch: 3/20, step: 36220, training_loss: 1.95561
Epoch: 3/20, step: 36240, training_loss: 1.41673
Epoch: 3/20, step: 36260, training_loss: 2.42796
Epoch: 3/20, step: 36280, training_loss: 2.35315
Epoch: 3/20, step: 36300, training_loss: 1.79992
Epoch: 3/20, step: 36320, training_loss: 1.58499
Epoch: 3/20, step: 36340, training_loss: 2.27935
Epoch: 3/20, step: 36360, training_loss: 2.38403
Epoch: 3/20, step: 36380, training_loss: 2.59083
Epoch: 3/20, step: 36400, training_loss: 1.51200
Epoch: 3/20, step: 36420, training_loss: 3.26723
Epoch: 3/20, step: 36440, training_loss: 1.88866
Epoch: 3/20, step: 36460, training_loss: 1.67704
Epoch: 3/20, step: 36480, training_loss: 2.59422
Epoch: 3/20, step: 36500, training_loss: 2.53438
Epoch: 3/20, step: 36520, training_loss: 2.44063
Epoch: 3/20, step: 36540, training_loss: 1.45824
Epoch: 3/20, step: 36560, training_loss: 2.12099
Epoch: 3/20, step: 36580, training_loss: 2.27036
Epoch: 3/20, step: 36600, training_loss: 2.17212
Epoch: 3/20, step: 36620, training_loss: 2.42922
Epoch: 3/20, step: 36640, training_loss: 2.45130
Epoch: 3/20, step: 36660, training_loss: 1.79625
Epoch: 3/20, step: 36680, training_loss: 1.96331
Epoch: 3/20, step: 36700, training_loss: 1.79108
Epoch: 3/20, step: 36720, training_loss: 2.63349
Epoch: 3/20, step: 36740, training_loss: 2.15426
Epoch: 3/20, step: 36760, training_loss: 2.67894
Epoch: 3/20, step: 36780, training_loss: 1.54701
Epoch: 3/20, step: 36800, training_loss: 1.97550
Epoch: 3/20, step: 36820, training_loss: 1.66761
Epoch: 3/20, step: 36840, training_loss: 2.60412
Epoch: 3/20, step: 36860, training_loss: 2.37104
Epoch: 3/20, step: 36880, training_loss: 2.33075
Epoch: 3/20, step: 36900, training_loss: 2.57694
Epoch: 3/20, step: 36920, training_loss: 2.01862
Epoch: 3/20, step: 36940, training_loss: 2.39822
Epoch: 3/20, step: 36960, training_loss: 2.71259
Epoch: 3/20, step: 36980, training_loss: 2.32923
Epoch: 3/20, step: 37000, training_loss: 2.36812
accuracy: 0.37, validation_loss: 2.262347459793091, num_samples: 100
Epoch: 3/20, step: 37020, training_loss: 1.97336
Epoch: 3/20, step: 37040, training_loss: 1.96929
Epoch: 3/20, step: 37060, training_loss: 2.34262
Epoch: 3/20, step: 37080, training_loss: 3.31242
Epoch: 3/20, step: 37100, training_loss: 1.82829
Epoch: 3/20, step: 37120, training_loss: 3.06944
Epoch: 3/20, step: 37140, training_loss: 1.58904
Epoch: 3/20, step: 37160, training_loss: 2.90691
Epoch: 3/20, step: 37180, training_loss: 2.45246
Epoch: 3/20, step: 37200, training_loss: 2.04045
Epoch: 3/20, step: 37220, training_loss: 2.16047
Epoch: 3/20, step: 37240, training_loss: 1.64992
Epoch: 3/20, step: 37260, training_loss: 1.63012
Epoch: 3/20, step: 37280, training_loss: 1.75966
Epoch: 3/20, step: 37300, training_loss: 1.55748
Epoch: 3/20, step: 37320, training_loss: 2.92114
Epoch: 3/20, step: 37340, training_loss: 2.60669
Epoch: 3/20, step: 37360, training_loss: 1.86994
Epoch: 3/20, step: 37380, training_loss: 2.92649
Epoch: 3/20, step: 37400, training_loss: 2.73445
Epoch: 3/20, step: 37420, training_loss: 2.64693
Epoch: 3/20, step: 37440, training_loss: 2.84994
Epoch: 3/20, step: 37460, training_loss: 2.55370
Epoch: 3/20, step: 37480, training_loss: 2.17682
Epoch: 3/20, step: 37500, training_loss: 2.26480
Epoch: 3/20, step: 37520, training_loss: 1.67741
Epoch: 3/20, step: 37540, training_loss: 2.43047
Epoch: 3/20, step: 37560, training_loss: 1.75451
Epoch: 3/20, step: 37580, training_loss: 2.53662
Epoch: 3/20, step: 37600, training_loss: 1.77329
Epoch: 3/20, step: 37620, training_loss: 1.62392
Epoch: 3/20, step: 37640, training_loss: 2.15034
Epoch: 3/20, step: 37660, training_loss: 3.04237
Epoch: 3/20, step: 37680, training_loss: 2.38078
Epoch: 3/20, step: 37700, training_loss: 2.22213
Epoch: 3/20, step: 37720, training_loss: 2.00838
Epoch: 3/20, step: 37740, training_loss: 1.89203
Epoch: 3/20, step: 37760, training_loss: 2.68384
Epoch: 3/20, step: 37780, training_loss: 1.85422
Epoch: 3/20, step: 37800, training_loss: 2.94411
Epoch: 3/20, step: 37820, training_loss: 2.64339
Epoch: 3/20, step: 37840, training_loss: 2.07422
Epoch: 3/20, step: 37860, training_loss: 2.80197
Epoch: 3/20, step: 37880, training_loss: 2.37513
Epoch: 3/20, step: 37900, training_loss: 1.84057
Epoch: 3/20, step: 37920, training_loss: 1.68388
Epoch: 3/20, step: 37940, training_loss: 2.50254
Epoch: 3/20, step: 37960, training_loss: 2.13452
Epoch: 3/20, step: 37980, training_loss: 2.42564
Epoch: 3/20, step: 38000, training_loss: 2.12788
accuracy: 0.33, validation_loss: 2.4306719303131104, num_samples: 100
Epoch: 3/20, step: 38020, training_loss: 2.15900
Epoch: 3/20, step: 38040, training_loss: 1.57857
Epoch: 3/20, step: 38060, training_loss: 2.63098
Epoch: 3/20, step: 38080, training_loss: 1.75977
Epoch: 3/20, step: 38100, training_loss: 2.32084
Epoch: 3/20, step: 38120, training_loss: 2.06305
Epoch: 3/20, step: 38140, training_loss: 2.04688
Epoch: 3/20, step: 38160, training_loss: 1.68315
Epoch: 3/20, step: 38180, training_loss: 2.37645
Epoch: 3/20, step: 38200, training_loss: 2.30183
Epoch: 3/20, step: 38220, training_loss: 2.06141
Epoch: 3/20, step: 38240, training_loss: 2.16174
Epoch: 3/20, step: 38260, training_loss: 1.97928
Epoch: 3/20, step: 38280, training_loss: 2.84400
Epoch: 3/20, step: 38300, training_loss: 1.79038
Epoch: 3/20, step: 38320, training_loss: 3.05949
Epoch: 3/20, step: 38340, training_loss: 2.74602
Epoch: 3/20, step: 38360, training_loss: 2.62194
Epoch: 3/20, step: 38380, training_loss: 2.37314
Epoch: 3/20, step: 38400, training_loss: 2.58061
Epoch: 3/20, step: 38420, training_loss: 2.38775
Epoch: 3/20, step: 38440, training_loss: 2.57346
Epoch: 3/20, step: 38460, training_loss: 2.44646
Epoch: 3/20, step: 38480, training_loss: 2.04439
Epoch: 3/20, step: 38500, training_loss: 2.72148
Epoch: 3/20, step: 38520, training_loss: 2.78331
Epoch: 3/20, step: 38540, training_loss: 1.52461
Epoch: 3/20, step: 38560, training_loss: 2.42533
Epoch: 3/20, step: 38580, training_loss: 2.23799
Epoch: 3/20, step: 38600, training_loss: 2.12243
Epoch: 3/20, step: 38620, training_loss: 2.45613
Epoch: 3/20, step: 38640, training_loss: 2.88537
Epoch: 3/20, step: 38660, training_loss: 1.86032
Epoch: 3/20, step: 38680, training_loss: 2.32723
Epoch: 3/20, step: 38700, training_loss: 2.38724
Epoch: 3/20, step: 38720, training_loss: 2.30884
Epoch: 3/20, step: 38740, training_loss: 2.34234
Epoch: 3/20, step: 38760, training_loss: 2.21972
Epoch: 3/20, step: 38780, training_loss: 1.33704
Epoch: 3/20, step: 38800, training_loss: 2.66712
Epoch: 3/20, step: 38820, training_loss: 2.64054
Epoch: 3/20, step: 38840, training_loss: 1.93846
Epoch: 3/20, step: 38860, training_loss: 2.33210
Epoch: 3/20, step: 38880, training_loss: 2.39726
Epoch: 3/20, step: 38900, training_loss: 2.26487
Epoch: 3/20, step: 38920, training_loss: 2.31848
Epoch: 3/20, step: 38940, training_loss: 2.25284
Epoch: 3/20, step: 38960, training_loss: 2.66642
Epoch: 3/20, step: 38980, training_loss: 2.62990
Epoch: 3/20, step: 39000, training_loss: 2.07680
accuracy: 0.4, validation_loss: 2.0631802082061768, num_samples: 100
Epoch: 3/20, step: 39020, training_loss: 2.23541
Epoch: 3/20, step: 39040, training_loss: 2.37110
Epoch: 3/20, step: 39060, training_loss: 2.54280
Epoch: 3/20, step: 39080, training_loss: 2.18813
Epoch: 3/20, step: 39100, training_loss: 1.88380
Epoch: 3/20, step: 39120, training_loss: 2.83193
Epoch: 3/20, step: 39140, training_loss: 1.66926
Epoch: 3/20, step: 39160, training_loss: 1.83161
Epoch: 3/20, step: 39180, training_loss: 2.16607
Epoch: 3/20, step: 39200, training_loss: 1.38859
Epoch: 3/20, step: 39220, training_loss: 1.91945
Epoch: 3/20, step: 39240, training_loss: 2.28253
Epoch: 3/20, step: 39260, training_loss: 2.79364
Epoch: 3/20, step: 39280, training_loss: 1.40958
Epoch: 3/20, step: 39300, training_loss: 2.30715
Epoch: 3/20, step: 39320, training_loss: 2.15124
Epoch: 3/20, step: 39340, training_loss: 1.99735
Epoch: 3/20, step: 39360, training_loss: 2.89985
Epoch: 3/20, step: 39380, training_loss: 1.83583
Epoch: 3/20, step: 39400, training_loss: 2.59714
Epoch: 3/20, step: 39420, training_loss: 1.52783
Epoch: 3/20, step: 39440, training_loss: 2.77505
Epoch: 3/20, step: 39460, training_loss: 2.30025
Epoch: 3/20, step: 39480, training_loss: 2.43836
Epoch: 3/20, step: 39500, training_loss: 2.43993
Epoch: 3/20, step: 39520, training_loss: 2.20438
Epoch: 3/20, step: 39540, training_loss: 2.53616
Epoch: 3/20, step: 39560, training_loss: 2.01084
Epoch: 3/20, step: 39580, training_loss: 1.58178
Epoch: 3/20, step: 39600, training_loss: 2.17457
Epoch: 3/20, step: 39620, training_loss: 2.19677
Epoch: 3/20, step: 39640, training_loss: 2.02540
Epoch: 3/20, step: 39660, training_loss: 2.13719
Epoch: 3/20, step: 39680, training_loss: 2.43377
Epoch: 3/20, step: 39700, training_loss: 2.97162
Epoch: 3/20, step: 39720, training_loss: 1.98148
Epoch: 3/20, step: 39740, training_loss: 2.14037
Epoch: 3/20, step: 39760, training_loss: 1.93785
Epoch: 3/20, step: 39780, training_loss: 2.60387
Epoch: 3/20, step: 39800, training_loss: 1.94785
Epoch: 3/20, step: 39820, training_loss: 2.37538
Epoch: 3/20, step: 39840, training_loss: 2.70111
Epoch: 3/20, step: 39860, training_loss: 1.95154
Epoch: 3/20, step: 39880, training_loss: 1.70993
Epoch: 3/20, step: 39900, training_loss: 2.06693
Epoch: 3/20, step: 39920, training_loss: 3.25201
Epoch: 3/20, step: 39940, training_loss: 1.85597
Epoch: 3/20, step: 39960, training_loss: 2.42213
Epoch: 3/20, step: 39980, training_loss: 1.59811
Epoch: 3/20, step: 40000, training_loss: 1.34491
accuracy: 0.43, validation_loss: 1.955803632736206, num_samples: 100
Epoch: 3/20, step: 40020, training_loss: 2.27945
Epoch: 3/20, step: 40040, training_loss: 2.27048
Epoch: 3/20, step: 40060, training_loss: 2.20179
Epoch: 3/20, step: 40080, training_loss: 2.59760
Epoch: 3/20, step: 40100, training_loss: 1.93005
Epoch: 3/20, step: 40120, training_loss: 2.51845
Epoch: 3/20, step: 40140, training_loss: 2.71453
Epoch: 3/20, step: 40160, training_loss: 2.39395
Epoch: 3/20, step: 40180, training_loss: 2.90504
Epoch: 3/20, step: 40200, training_loss: 2.30842
Epoch: 3/20, step: 40220, training_loss: 2.13311
Epoch: 3/20, step: 40240, training_loss: 1.82725
Epoch: 3/20, step: 40260, training_loss: 2.26334
Epoch: 3/20, step: 40280, training_loss: 2.11336
Epoch: 3/20, step: 40300, training_loss: 2.06813
Epoch: 3/20, step: 40320, training_loss: 2.62855
Epoch: 3/20, step: 40340, training_loss: 0.90234
Epoch: 3/20, step: 40360, training_loss: 2.01704
Epoch: 3/20, step: 40380, training_loss: 2.72436
Epoch: 3/20, step: 40400, training_loss: 3.11634
Epoch: 3/20, step: 40420, training_loss: 2.69497
Epoch: 3/20, step: 40440, training_loss: 2.76129
Epoch: 3/20, step: 40460, training_loss: 1.88668
Epoch: 3/20, step: 40480, training_loss: 1.30814
Epoch: 3/20, step: 40500, training_loss: 1.96822
Epoch: 3/20, step: 40520, training_loss: 2.10476
Epoch: 3/20, step: 40540, training_loss: 1.42320
Epoch: 3/20, step: 40560, training_loss: 3.22460
Epoch: 3/20, step: 40580, training_loss: 2.51561
Epoch: 3/20, step: 40600, training_loss: 2.02764
Epoch: 3/20, step: 40620, training_loss: 2.34185
Epoch: 3/20, step: 40640, training_loss: 1.89282
Epoch: 3/20, step: 40660, training_loss: 2.21478
Epoch: 3/20, step: 40680, training_loss: 1.82812
Epoch: 3/20, step: 40700, training_loss: 2.10256
Epoch: 3/20, step: 40720, training_loss: 2.39675
Epoch: 3/20, step: 40740, training_loss: 2.61368
Epoch: 3/20, step: 40760, training_loss: 2.23402
Epoch: 3/20, step: 40780, training_loss: 2.64836
Epoch: 3/20, step: 40800, training_loss: 1.61696
Epoch: 3/20, step: 40820, training_loss: 2.80914
Epoch: 3/20, step: 40840, training_loss: 2.09407
Epoch: 3/20, step: 40860, training_loss: 1.84092
Epoch: 3/20, step: 40880, training_loss: 2.58955
Epoch: 3/20, step: 40900, training_loss: 2.11645
Epoch: 3/20, step: 40920, training_loss: 1.74252
Epoch: 3/20, step: 40940, training_loss: 2.46282
Epoch: 3/20, step: 40960, training_loss: 2.26254
Epoch: 3/20, step: 40980, training_loss: 1.75800
Epoch: 3/20, step: 41000, training_loss: 2.46690
accuracy: 0.41, validation_loss: 2.1143798828125, num_samples: 100
Epoch: 3/20, step: 41020, training_loss: 2.04796
Epoch: 3/20, step: 41040, training_loss: 2.49277
Epoch: 3/20, step: 41060, training_loss: 2.51883
Epoch: 3/20, step: 41080, training_loss: 2.16343
Epoch: 3/20, step: 41100, training_loss: 1.61554
Epoch: 3/20, step: 41120, training_loss: 1.92806
Epoch: 3/20, step: 41140, training_loss: 1.90505
Epoch: 3/20, step: 41160, training_loss: 1.55676
Epoch: 3/20, step: 41180, training_loss: 2.51671
Epoch: 3/20, step: 41200, training_loss: 2.82675
Epoch: 3/20, step: 41220, training_loss: 2.48111
Epoch: 3/20, step: 41240, training_loss: 2.04810
Epoch: 3/20, step: 41260, training_loss: 1.87712
Epoch: 3/20, step: 41280, training_loss: 2.13777
Epoch: 3/20, step: 41300, training_loss: 2.25312
Epoch: 3/20, step: 41320, training_loss: 1.52162
Epoch: 3/20, step: 41340, training_loss: 2.42296
Epoch: 3/20, step: 41360, training_loss: 1.95421
Epoch: 3/20, step: 41380, training_loss: 1.77780
Epoch: 3/20, step: 41400, training_loss: 2.42041
Epoch: 3/20, step: 41420, training_loss: 2.04038
Epoch: 3/20, step: 41440, training_loss: 1.99732
Epoch: 3/20, step: 41460, training_loss: 1.50536
Epoch: 3/20, step: 41480, training_loss: 2.30665
Epoch: 3/20, step: 41500, training_loss: 2.31409
Epoch: 3/20, step: 41520, training_loss: 2.09669
Epoch: 3/20, step: 41540, training_loss: 2.94898
Epoch: 3/20, step: 41560, training_loss: 1.59947
Epoch: 3/20, step: 41580, training_loss: 1.35276
Epoch: 3/20, step: 41600, training_loss: 1.71349
Epoch: 3/20, step: 41620, training_loss: 2.12758
Epoch: 3/20, step: 41640, training_loss: 2.72130
Epoch: 3/20, step: 41660, training_loss: 1.85706
Epoch: 3/20, step: 41680, training_loss: 1.88009
Epoch: 3/20, step: 41700, training_loss: 1.72196
Epoch: 3/20, step: 41720, training_loss: 1.74474
Epoch: 3/20, step: 41740, training_loss: 2.04613
Epoch: 3/20, step: 41760, training_loss: 2.06266
Epoch: 3/20, step: 41780, training_loss: 1.95799
Epoch: 3/20, step: 41800, training_loss: 2.71709
Epoch: 3/20, step: 41820, training_loss: 2.90111
Epoch: 3/20, step: 41840, training_loss: 2.63188
Epoch: 3/20, step: 41860, training_loss: 2.74499
Epoch: 3/20, step: 41880, training_loss: 1.54461
Epoch: 3/20, step: 41900, training_loss: 1.72576
Epoch: 3/20, step: 41920, training_loss: 2.42884
Epoch: 3/20, step: 41940, training_loss: 2.48166
Epoch: 3/20, step: 41960, training_loss: 1.48964
Epoch: 3/20, step: 41980, training_loss: 1.69484
Epoch: 3/20, step: 42000, training_loss: 2.85708
accuracy: 0.45, validation_loss: 1.845149040222168, num_samples: 100
Epoch: 3/20, step: 42020, training_loss: 2.71498
Epoch: 3/20, step: 42040, training_loss: 2.00208
Epoch: 3/20, step: 42060, training_loss: 2.33899
Epoch: 3/20, step: 42080, training_loss: 2.25647
Epoch: 3/20, step: 42100, training_loss: 2.57926
Epoch: 3/20, step: 42120, training_loss: 1.60543
Epoch: 3/20, step: 42140, training_loss: 2.03687
Epoch: 3/20, step: 42160, training_loss: 1.90897
Epoch: 3/20, step: 42180, training_loss: 1.39418
Epoch: 3/20, step: 42200, training_loss: 2.35550
Epoch: 3/20, step: 42220, training_loss: 2.43730
Epoch: 3/20, step: 42240, training_loss: 2.32589
Epoch: 3/20, step: 42260, training_loss: 2.85773
Epoch: 3/20, step: 42280, training_loss: 2.08815
Epoch: 3/20, step: 42300, training_loss: 2.05036
Epoch: 3/20, step: 42320, training_loss: 2.00344
Epoch: 3/20, step: 42340, training_loss: 2.29997
Epoch: 3/20, step: 42360, training_loss: 1.01096
Epoch: 3/20, step: 42380, training_loss: 1.89819
Epoch: 3/20, step: 42400, training_loss: 2.76278
Epoch: 3/20, step: 42420, training_loss: 1.38526
Epoch: 3/20, step: 42440, training_loss: 1.66356
Epoch: 3/20, step: 42460, training_loss: 2.17549
Epoch: 3/20, step: 42480, training_loss: 1.62510
Epoch: 3/20, step: 42500, training_loss: 1.67033
Epoch: 3/20, step: 42520, training_loss: 2.35069
Epoch: 3/20, step: 42540, training_loss: 2.28104
Epoch: 3/20, step: 42560, training_loss: 2.10457
Epoch: 3/20, step: 42580, training_loss: 2.95588
Epoch: 3/20, step: 42600, training_loss: 3.01212
Epoch: 3/20, step: 42620, training_loss: 2.11798
Epoch: 3/20, step: 42640, training_loss: 1.45077
Epoch: 3/20, step: 42660, training_loss: 2.09481
Epoch: 3/20, step: 42680, training_loss: 2.54617
Epoch: 3/20, step: 42700, training_loss: 2.49481
Epoch: 3/20, step: 42720, training_loss: 2.07754
Epoch: 3/20, step: 42740, training_loss: 2.90710
Epoch: 3/20, step: 42760, training_loss: 3.07799
Epoch: 3/20, step: 42780, training_loss: 1.29703
Epoch: 3/20, step: 42800, training_loss: 2.44363
Epoch: 3/20, step: 42820, training_loss: 2.46137
Epoch: 3/20, step: 42840, training_loss: 1.90338
Epoch: 3/20, step: 42860, training_loss: 2.37564
Epoch: 3/20, step: 42880, training_loss: 2.74082
Epoch: 3/20, step: 42900, training_loss: 1.77065
Epoch: 3/20, step: 42920, training_loss: 2.02478
Epoch: 3/20, step: 42940, training_loss: 1.75451
Epoch: 3/20, step: 42960, training_loss: 1.90234
Epoch: 3/20, step: 42980, training_loss: 3.23054
Epoch: 3/20, step: 43000, training_loss: 2.20913
accuracy: 0.32, validation_loss: 2.433354139328003, num_samples: 100
Epoch: 3/20, step: 43020, training_loss: 2.15244
Epoch: 3/20, step: 43040, training_loss: 2.39726
Epoch: 3/20, step: 43060, training_loss: 1.93279
Epoch: 3/20, step: 43080, training_loss: 1.67936
Epoch: 3/20, step: 43100, training_loss: 2.41865
Epoch: 3/20, step: 43120, training_loss: 2.79133
Epoch: 3/20, step: 43140, training_loss: 2.08720
Epoch: 3/20, step: 43160, training_loss: 1.93531
Epoch: 3/20, step: 43180, training_loss: 2.73666
Epoch: 3/20, step: 43200, training_loss: 1.82999
Epoch: 3/20, step: 43220, training_loss: 1.81900
Epoch: 3/20, step: 43240, training_loss: 1.96940
Epoch: 3/20, step: 43260, training_loss: 2.02551
Epoch: 3/20, step: 43280, training_loss: 2.75361
Epoch: 3/20, step: 43300, training_loss: 2.22676
Epoch: 3/20, step: 43320, training_loss: 1.57270
Epoch: 3/20, step: 43340, training_loss: 2.31552
Epoch: 3/20, step: 43360, training_loss: 2.66627
Epoch: 3/20, step: 43380, training_loss: 2.10636
Epoch: 3/20, step: 43400, training_loss: 1.93947
Epoch: 3/20, step: 43420, training_loss: 1.82247
Epoch: 3/20, step: 43440, training_loss: 2.95216
Epoch: 3/20, step: 43460, training_loss: 2.32614
Epoch: 3/20, step: 43480, training_loss: 2.44297
Epoch: 3/20, step: 43500, training_loss: 2.18609
Epoch: 3/20, step: 43520, training_loss: 2.43924
Epoch: 3/20, step: 43540, training_loss: 1.97734
Epoch: 3/20, step: 43560, training_loss: 2.18477
Epoch: 3/20, step: 43580, training_loss: 1.94294
Epoch: 3/20, step: 43600, training_loss: 1.90516
Epoch: 3/20, step: 43620, training_loss: 2.22450
Epoch: 3/20, step: 43640, training_loss: 2.96345
Epoch: 3/20, step: 43660, training_loss: 1.97495
Epoch: 3/20, step: 43680, training_loss: 2.77828
Epoch: 3/20, step: 43700, training_loss: 1.83971
Epoch: 3/20, step: 43720, training_loss: 1.29817
Epoch: 3/20, step: 43740, training_loss: 2.04125
Epoch: 3/20, step: 43760, training_loss: 1.64816
Epoch: 3/20, step: 43780, training_loss: 2.02748
Epoch: 3/20, step: 43800, training_loss: 0.88370
Epoch: 3/20, step: 43820, training_loss: 1.84890
Epoch: 3/20, step: 43840, training_loss: 1.77238
Epoch: 3/20, step: 43860, training_loss: 2.07411
Epoch: 3/20, step: 43880, training_loss: 2.78794
Epoch: 3/20, step: 43900, training_loss: 2.04767
Epoch: 3/20, step: 43920, training_loss: 2.21267
Epoch: 3/20, step: 43940, training_loss: 2.61170
Epoch: 3/20, step: 43960, training_loss: 2.76331
Epoch: 3/20, step: 43980, training_loss: 2.64890
Epoch: 3/20, step: 44000, training_loss: 2.27567
accuracy: 0.48, validation_loss: 1.9864449501037598, num_samples: 100
Epoch: 3/20, step: 44020, training_loss: 1.82178
Epoch: 3/20, step: 44040, training_loss: 2.40210
Epoch: 3/20, step: 44060, training_loss: 2.25479
Epoch: 3/20, step: 44080, training_loss: 2.08930
Epoch: 3/20, step: 44100, training_loss: 2.53889
Epoch: 3/20, step: 44120, training_loss: 1.52895
Epoch: 3/20, step: 44140, training_loss: 2.32839
Epoch: 3/20, step: 44160, training_loss: 1.91199
Epoch: 3/20, step: 44180, training_loss: 2.59460
Epoch: 3/20, step: 44200, training_loss: 1.09004
Epoch: 3/20, step: 44220, training_loss: 2.68208
Epoch: 3/20, step: 44240, training_loss: 1.31390
Epoch: 3/20, step: 44260, training_loss: 2.05962
Epoch: 3/20, step: 44280, training_loss: 2.30802
Epoch: 3/20, step: 44300, training_loss: 2.62756
Epoch: 3/20, step: 44320, training_loss: 1.85001
Epoch: 3/20, step: 44340, training_loss: 2.40016
Epoch: 3/20, step: 44360, training_loss: 2.18979
Epoch: 3/20, step: 44380, training_loss: 2.54059
Epoch: 3/20, step: 44400, training_loss: 1.24837
Epoch: 3/20, step: 44420, training_loss: 1.99473
Epoch: 3/20, step: 44440, training_loss: 2.44901
Epoch: 3/20, step: 44460, training_loss: 2.35033
Epoch: 3/20, step: 44480, training_loss: 2.21682
Epoch: 3/20, step: 44500, training_loss: 1.52630
Epoch: 3/20, step: 44520, training_loss: 1.99931
Epoch: 3/20, step: 44540, training_loss: 2.23645
Epoch: 3/20, step: 44560, training_loss: 2.31289
Epoch: 3/20, step: 44580, training_loss: 2.42174
Epoch: 3/20, step: 44600, training_loss: 1.79317
Epoch: 3/20, step: 44620, training_loss: 2.06730
Epoch: 3/20, step: 44640, training_loss: 2.09980
Epoch: 3/20, step: 44660, training_loss: 2.79212
Epoch: 3/20, step: 44680, training_loss: 1.67460
Epoch: 3/20, step: 44700, training_loss: 2.72343
Epoch: 3/20, step: 44720, training_loss: 2.63381
Epoch: 3/20, step: 44740, training_loss: 1.16261
Epoch: 3/20, step: 44760, training_loss: 2.41833
Epoch: 3/20, step: 44780, training_loss: 1.74208
Epoch: 3/20, step: 44800, training_loss: 2.40344
Epoch: 3/20, step: 44820, training_loss: 2.99295
Epoch: 3/20, step: 44840, training_loss: 1.98281
Epoch: 3/20, step: 44860, training_loss: 2.51661
Epoch: 3/20, step: 44880, training_loss: 1.81917
Epoch: 3/20, step: 44900, training_loss: 2.23452
Epoch: 3/20, step: 44920, training_loss: 1.90263
Epoch: 3/20, step: 44940, training_loss: 2.33790
Epoch: 3/20, step: 44960, training_loss: 3.29668
Epoch: 3/20, step: 44980, training_loss: 2.50579
Epoch: 3/20, step: 45000, training_loss: 2.30126
accuracy: 0.43, validation_loss: 2.1001787185668945, num_samples: 100
Epoch: 3/20, step: 45020, training_loss: 1.62732
Epoch: 3/20, step: 45040, training_loss: 2.17662
Epoch: 3/20, step: 45060, training_loss: 2.46136
Epoch: 3/20, step: 45080, training_loss: 2.58274
Epoch: 3/20, step: 45100, training_loss: 2.52715
Epoch: 3/20, step: 45120, training_loss: 1.82712
Epoch: 3/20, step: 45140, training_loss: 2.10695
Epoch: 3/20, step: 45160, training_loss: 2.30696
Epoch: 3/20, step: 45180, training_loss: 1.24037
Epoch: 3/20, step: 45200, training_loss: 2.03500
Epoch: 3/20, step: 45220, training_loss: 2.37871
Epoch: 3/20, step: 45240, training_loss: 2.52502
Epoch: 3/20, step: 45260, training_loss: 2.29213
Epoch: 3/20, step: 45280, training_loss: 2.71942
Epoch: 3/20, step: 45300, training_loss: 2.02583
Epoch: 3/20, step: 45320, training_loss: 1.87893
Epoch: 3/20, step: 45340, training_loss: 2.11959
Epoch: 3/20, step: 45360, training_loss: 1.62665
Epoch: 3/20, step: 45380, training_loss: 2.17220
Epoch: 3/20, step: 45400, training_loss: 2.95618
Epoch: 3/20, step: 45420, training_loss: 1.96616
Epoch: 3/20, step: 45440, training_loss: 2.20353
Epoch: 3/20, step: 45460, training_loss: 2.55299
Epoch: 3/20, step: 45480, training_loss: 2.56887
Epoch: 3/20, step: 45500, training_loss: 2.87576
Epoch: 3/20, step: 45520, training_loss: 2.63637
Epoch: 3/20, step: 45540, training_loss: 1.21462
Epoch: 3/20, step: 45560, training_loss: 2.26896
Epoch: 3/20, step: 45580, training_loss: 2.56574
Epoch: 3/20, step: 45600, training_loss: 3.04782
Epoch: 3/20, step: 45620, training_loss: 2.40634
Epoch: 3/20, step: 45640, training_loss: 2.39735
Epoch: 3/20, step: 45660, training_loss: 1.70230
Epoch: 3/20, step: 45680, training_loss: 2.15736
Epoch: 3/20, step: 45700, training_loss: 2.21564
Epoch: 3/20, step: 45720, training_loss: 1.95702
Epoch: 3/20, step: 45740, training_loss: 1.85047
Epoch: 3/20, step: 45760, training_loss: 2.84140
Epoch: 3/20, step: 45780, training_loss: 1.65891
Epoch: 3/20, step: 45800, training_loss: 2.28461
Epoch: 3/20, step: 45820, training_loss: 1.98674
Epoch: 3/20, step: 45840, training_loss: 1.38767
Epoch: 3/20, step: 45860, training_loss: 2.14758
Epoch: 3/20, step: 45880, training_loss: 1.53383
Epoch: 3/20, step: 45900, training_loss: 1.96711
Epoch: 3/20, step: 45920, training_loss: 2.15556
Epoch: 3/20, step: 45940, training_loss: 2.41572
Epoch: 3/20, step: 45960, training_loss: 2.42270
Epoch: 3/20, step: 45980, training_loss: 2.04111
Epoch: 3/20, step: 46000, training_loss: 1.66584
accuracy: 0.38, validation_loss: 2.2529687881469727, num_samples: 100
Epoch: 3/20, step: 46020, training_loss: 2.32488
Epoch: 3/20, step: 46040, training_loss: 2.33479
Epoch: 3/20, step: 46060, training_loss: 2.18651
Epoch: 3/20, step: 46080, training_loss: 2.59033
Epoch: 3/20, step: 46100, training_loss: 2.37299
Epoch: 3/20, step: 46120, training_loss: 2.28870
Epoch: 3/20, step: 46140, training_loss: 1.48576
Epoch: 3/20, step: 46160, training_loss: 1.56323
Epoch: 3/20, step: 46180, training_loss: 2.20022
Epoch: 3/20, step: 46200, training_loss: 3.07309
Epoch: 3/20, step: 46220, training_loss: 1.86465
Epoch: 3/20, step: 46240, training_loss: 3.38578
Epoch: 3/20, step: 46260, training_loss: 1.65167
Epoch: 3/20, step: 46280, training_loss: 1.84972
Epoch: 3/20, step: 46300, training_loss: 2.92612
Epoch: 3/20, step: 46320, training_loss: 1.71967
Epoch: 3/20, step: 46340, training_loss: 2.08952
Epoch: 3/20, step: 46360, training_loss: 1.64857
Epoch: 3/20, step: 46380, training_loss: 1.97655
Epoch: 3/20, step: 46400, training_loss: 2.24811
Epoch: 3/20, step: 46420, training_loss: 1.99422
Epoch: 3/20, step: 46440, training_loss: 2.14996
Epoch: 3/20, step: 46460, training_loss: 1.09611
Epoch: 3/20, step: 46480, training_loss: 1.58646
Epoch: 3/20, step: 46500, training_loss: 2.97540
Epoch: 3/20, step: 46520, training_loss: 2.60583
Epoch: 3/20, step: 46540, training_loss: 2.97409
Epoch: 3/20, step: 46560, training_loss: 2.25703
Epoch: 3/20, step: 46580, training_loss: 1.84937
Epoch: 3/20, step: 46600, training_loss: 2.22657
Epoch: 3/20, step: 46620, training_loss: 1.95128
Epoch: 3/20, step: 46640, training_loss: 2.12025
Epoch: 3/20, step: 46660, training_loss: 1.96999
Epoch: 3/20, step: 46680, training_loss: 1.73726
Epoch: 3/20, step: 46700, training_loss: 2.83113
Epoch: 3/20, step: 46720, training_loss: 1.91213
Epoch: 3/20, step: 46740, training_loss: 2.15103
Epoch: 3/20, step: 46760, training_loss: 1.79256
Epoch: 3/20, step: 46780, training_loss: 1.93634
Epoch: 3/20, step: 46800, training_loss: 2.36408
Epoch: 3/20, step: 46820, training_loss: 2.22404
Epoch: 3/20, step: 46840, training_loss: 2.43164
Epoch: 3/20, step: 46860, training_loss: 2.23477
Epoch: 3/20, step: 46880, training_loss: 1.52937
Epoch: 3/20, step: 46900, training_loss: 1.90249
Epoch: 3/20, step: 46920, training_loss: 1.18798
Epoch: 3/20, step: 46940, training_loss: 2.22708
Epoch: 3/20, step: 46960, training_loss: 2.31585
Epoch: 3/20, step: 46980, training_loss: 1.85531
Epoch: 3/20, step: 47000, training_loss: 2.84968
accuracy: 0.5, validation_loss: 2.0696420669555664, num_samples: 100
Epoch: 3/20, step: 47020, training_loss: 2.94733
Epoch: 3/20, step: 47040, training_loss: 2.40099
Epoch: 3/20, step: 47060, training_loss: 1.63734
Epoch: 3/20, step: 47080, training_loss: 2.81905
Epoch: 3/20, step: 47100, training_loss: 1.87126
Epoch: 3/20, step: 47120, training_loss: 1.84661
Epoch: 3/20, step: 47140, training_loss: 1.71857
Epoch: 3/20, step: 47160, training_loss: 2.18892
Epoch: 3/20, step: 47180, training_loss: 1.98312
Epoch: 3/20, step: 47200, training_loss: 1.68378
Epoch: 3/20, step: 47220, training_loss: 2.78740
Epoch: 3/20, step: 47240, training_loss: 2.55191
Epoch: 3/20, step: 47260, training_loss: 3.20399
Epoch: 3/20, step: 47280, training_loss: 2.55328
Epoch: 3/20, step: 47300, training_loss: 3.12838
Epoch: 3/20, step: 47320, training_loss: 1.75210
Epoch: 3/20, step: 47340, training_loss: 2.03520
Epoch: 3/20, step: 47360, training_loss: 2.42337
Epoch: 3/20, step: 47380, training_loss: 2.98138
Epoch: 3/20, step: 47400, training_loss: 2.01725
Epoch: 3/20, step: 47420, training_loss: 1.95054
Epoch: 3/20, step: 47440, training_loss: 2.22126
Epoch: 3/20, step: 47460, training_loss: 1.84710
Epoch: 3/20, step: 47480, training_loss: 2.62612
Epoch: 3/20, step: 47500, training_loss: 2.09793
Epoch: 3/20, step: 47520, training_loss: 2.42000
Epoch: 3/20, step: 47540, training_loss: 1.66053
Epoch: 3/20, step: 47560, training_loss: 1.44495
Epoch: 3/20, step: 47580, training_loss: 2.28514
Epoch: 3/20, step: 47600, training_loss: 1.41282
Epoch: 3/20, step: 47620, training_loss: 2.42260
Epoch: 3/20, step: 47640, training_loss: 1.76123
Epoch: 3/20, step: 47660, training_loss: 2.10825
Epoch: 3/20, step: 47680, training_loss: 1.37825
Epoch: 3/20, step: 47700, training_loss: 3.30606
Epoch: 3/20, step: 47720, training_loss: 1.33413
Epoch: 3/20, step: 47740, training_loss: 2.37258
Epoch: 3/20, step: 47760, training_loss: 2.83612
Epoch: 3/20, step: 47780, training_loss: 2.20708
Epoch: 3/20, step: 47800, training_loss: 1.55354
Epoch: 3/20, step: 47820, training_loss: 1.78224
Epoch: 3/20, step: 47840, training_loss: 1.65258
Epoch: 3/20, step: 47860, training_loss: 1.74854
Epoch: 3/20, step: 47880, training_loss: 1.81125
Epoch: 3/20, step: 47900, training_loss: 2.39732
Epoch: 3/20, step: 47920, training_loss: 1.84343
Epoch: 3/20, step: 47940, training_loss: 2.36226
Epoch: 3/20, step: 47960, training_loss: 2.18684
Epoch: 3/20, step: 47980, training_loss: 2.86578
Epoch: 3/20, step: 48000, training_loss: 2.47998
accuracy: 0.45, validation_loss: 1.931769847869873, num_samples: 100
Epoch: 3/20, step: 48020, training_loss: 1.23441
Epoch: 3/20, step: 48040, training_loss: 2.30231
Epoch: 3/20, step: 48060, training_loss: 2.20343
Epoch: 3/20, step: 48080, training_loss: 2.03410
Epoch: 3/20, step: 48100, training_loss: 2.48892
Epoch: 3/20, step: 48120, training_loss: 1.91800
Epoch: 3/20, step: 48140, training_loss: 1.68141
Epoch: 3/20, step: 48160, training_loss: 2.15400
Epoch: 3/20, step: 48180, training_loss: 2.63329
Epoch: 3/20, step: 48200, training_loss: 2.41020
Epoch: 3/20, step: 48220, training_loss: 2.00213
Epoch: 3/20, step: 48240, training_loss: 2.19544
Epoch: 3/20, step: 48260, training_loss: 2.05050
Epoch: 3/20, step: 48280, training_loss: 1.52553
Epoch: 3/20, step: 48300, training_loss: 2.86940
Epoch: 3/20, step: 48320, training_loss: 2.13276
Epoch: 3/20, step: 48340, training_loss: 2.53136
Epoch: 3/20, step: 48360, training_loss: 2.23168
Epoch: 3/20, step: 48380, training_loss: 2.30409
Epoch: 3/20, step: 48400, training_loss: 2.01087
Epoch: 3/20, step: 48420, training_loss: 1.20501
Epoch: 3/20, step: 48440, training_loss: 2.42858
Epoch: 3/20, step: 48460, training_loss: 1.57007
Epoch: 3/20, step: 48480, training_loss: 2.86081
Epoch: 3/20, step: 48500, training_loss: 2.12959
Epoch: 3/20, step: 48520, training_loss: 1.47808
Epoch: 3/20, step: 48540, training_loss: 2.01702
Epoch: 3/20, step: 48560, training_loss: 2.95904
Epoch: 3/20, step: 48580, training_loss: 1.58633
Epoch: 3/20, step: 48600, training_loss: 2.20499
Epoch: 3/20, step: 48620, training_loss: 2.16081
Epoch: 3/20, step: 48640, training_loss: 2.11017
Epoch: 3/20, step: 48660, training_loss: 2.36258
Epoch: 3/20, step: 48680, training_loss: 2.58866
Epoch: 3/20, step: 48700, training_loss: 1.79355
Epoch: 3/20, step: 48720, training_loss: 2.31731
Epoch: 3/20, step: 48740, training_loss: 2.49185
Epoch: 3/20, step: 48760, training_loss: 1.85463
Epoch: 3/20, step: 48780, training_loss: 2.30838
Epoch: 3/20, step: 48800, training_loss: 3.28971
Epoch: 3/20, step: 48820, training_loss: 2.86771
Epoch: 3/20, step: 48840, training_loss: 1.73363
Epoch: 3/20, step: 48860, training_loss: 2.77578
Epoch: 3/20, step: 48880, training_loss: 2.25143
Epoch: 3/20, step: 48900, training_loss: 2.07900
Epoch: 3/20, step: 48920, training_loss: 2.40233
Epoch: 3/20, step: 48940, training_loss: 2.27250
Epoch: 3/20, step: 48960, training_loss: 3.05049
Epoch: 3/20, step: 48980, training_loss: 2.68494
Epoch: 3/20, step: 49000, training_loss: 2.16499
accuracy: 0.36, validation_loss: 2.115138530731201, num_samples: 100
Epoch: 3/20, step: 49020, training_loss: 2.55169
Epoch: 3/20, step: 49040, training_loss: 1.10620
Epoch: 3/20, step: 49060, training_loss: 1.74711
Epoch: 3/20, step: 49080, training_loss: 2.51676
Epoch: 3/20, step: 49100, training_loss: 1.97953
Epoch: 3/20, step: 49120, training_loss: 2.37551
Epoch: 3/20, step: 49140, training_loss: 1.54191
Epoch: 3/20, step: 49160, training_loss: 2.66182
Epoch: 3/20, step: 49180, training_loss: 1.96568
Epoch: 3/20, step: 49200, training_loss: 2.20645
Epoch: 3/20, step: 49220, training_loss: 1.57308
Epoch: 3/20, step: 49240, training_loss: 1.73918
Epoch: 3/20, step: 49260, training_loss: 2.02914
Epoch: 3/20, step: 49280, training_loss: 1.64307
Epoch: 3/20, step: 49300, training_loss: 1.76387
Epoch: 3/20, step: 49320, training_loss: 2.25236
Epoch: 3/20, step: 49340, training_loss: 1.64695
Epoch: 3/20, step: 49360, training_loss: 2.73597
Epoch: 3/20, step: 49380, training_loss: 2.25525
Epoch: 3/20, step: 49400, training_loss: 2.37043
Epoch: 3/20, step: 49420, training_loss: 1.94238
Epoch: 3/20, step: 49440, training_loss: 2.06147
Epoch: 3/20, step: 49460, training_loss: 1.38285
Epoch: 3/20, step: 49480, training_loss: 1.67432
Epoch: 3/20, step: 49500, training_loss: 1.99423
Epoch: 3/20, step: 49520, training_loss: 1.97801
Epoch: 3/20, step: 49540, training_loss: 2.75444
Epoch: 3/20, step: 49560, training_loss: 2.05060
Epoch: 3/20, step: 49580, training_loss: 2.79731
Epoch: 3/20, step: 49600, training_loss: 2.70230
Epoch: 3/20, step: 49620, training_loss: 1.72839
Epoch: 3/20, step: 49640, training_loss: 1.78220
Epoch: 3/20, step: 49660, training_loss: 2.33611
Epoch: 3/20, step: 49680, training_loss: 2.22161
Epoch: 3/20, step: 49700, training_loss: 1.81772
Epoch: 3/20, step: 49720, training_loss: 2.49323
Epoch: 3/20, step: 49740, training_loss: 2.74353
Epoch: 3/20, step: 49760, training_loss: 2.44776
Epoch: 3/20, step: 49780, training_loss: 2.05735
Epoch: 3/20, step: 49800, training_loss: 1.81218
Epoch: 3/20, step: 49820, training_loss: 2.19668
Epoch: 3/20, step: 49840, training_loss: 1.65056
Epoch: 3/20, step: 49860, training_loss: 1.71051
Epoch: 3/20, step: 49880, training_loss: 2.27857
Epoch: 3/20, step: 49900, training_loss: 2.78754
Epoch: 3/20, step: 49920, training_loss: 1.93997
Epoch: 3/20, step: 49940, training_loss: 1.87352
Epoch: 3/20, step: 49960, training_loss: 1.87796
Epoch: 3/20, step: 49980, training_loss: 2.57163
Epoch: 3/20, step: 50000, training_loss: 1.79317
accuracy: 0.39, validation_loss: 2.243129253387451, num_samples: 100
Epoch: 3/20, step: 50020, training_loss: 1.47488
Epoch: 3/20, step: 50040, training_loss: 1.68275
Epoch: 3/20, step: 50060, training_loss: 2.28730
Epoch: 3/20, step: 50080, training_loss: 2.05409
Epoch: 3/20, step: 50100, training_loss: 2.26418
Epoch: 3/20, step: 50120, training_loss: 2.05047
Epoch: 3/20, step: 50140, training_loss: 2.63856
Epoch: 3/20, step: 50160, training_loss: 2.69917
Epoch: 3/20, step: 50180, training_loss: 2.19294
Epoch: 3/20, step: 50200, training_loss: 1.79359
Epoch: 3/20, step: 50220, training_loss: 1.91950
Epoch: 3/20, step: 50240, training_loss: 1.92097
Epoch: 3/20, step: 50260, training_loss: 1.52670
Epoch: 3/20, step: 50280, training_loss: 3.10696
Epoch: 3/20, step: 50300, training_loss: 2.44548
Epoch: 3/20, step: 50320, training_loss: 2.40133
Epoch: 3/20, step: 50340, training_loss: 2.07878
Epoch: 3/20, step: 50360, training_loss: 2.40799
Epoch: 3/20, step: 50380, training_loss: 1.75647
Epoch: 3/20, step: 50400, training_loss: 2.40420
Epoch: 3/20, step: 50420, training_loss: 1.47267
Epoch: 3/20, step: 50440, training_loss: 2.04700
Epoch: 3/20, step: 50460, training_loss: 2.67512
Epoch: 3/20, step: 50480, training_loss: 1.91855
Epoch: 3/20, step: 50500, training_loss: 2.04905
Epoch: 3/20, step: 50520, training_loss: 2.43638
Epoch: 3/20, step: 50540, training_loss: 2.07606
Epoch: 3/20, step: 50560, training_loss: 2.58287
Epoch: 3/20, step: 50580, training_loss: 2.63007
Epoch: 3/20, step: 50600, training_loss: 1.83385
Epoch: 3/20, step: 50620, training_loss: 2.14679
Epoch: 3/20, step: 50640, training_loss: 2.44113
Epoch: 3/20, step: 50660, training_loss: 2.15500
Epoch: 3/20, step: 50680, training_loss: 2.33953
Epoch: 3/20, step: 50700, training_loss: 2.07335
Epoch: 3/20, step: 50720, training_loss: 2.32312
Epoch: 3/20, step: 50740, training_loss: 2.84014
Epoch: 3/20, step: 50760, training_loss: 2.19360
Epoch: 3/20, step: 50780, training_loss: 2.25439
Epoch: 3/20, step: 50800, training_loss: 2.96605
Epoch: 3/20, step: 50820, training_loss: 1.98419
Epoch: 3/20, step: 50840, training_loss: 2.31175
Epoch: 3/20, step: 50860, training_loss: 2.53132
Epoch: 3/20, step: 50880, training_loss: 2.57154
Epoch: 3/20, step: 50900, training_loss: 1.93809
Epoch: 3/20, step: 50920, training_loss: 2.83307
Epoch: 3/20, step: 50940, training_loss: 2.71005
Epoch: 3/20, step: 50960, training_loss: 2.02443
Epoch: 3/20, step: 50980, training_loss: 2.06925
Epoch: 3/20, step: 51000, training_loss: 2.12347
accuracy: 0.43, validation_loss: 2.1445090770721436, num_samples: 100
Epoch: 3/20, step: 51020, training_loss: 2.71024
Epoch: 3/20, step: 51040, training_loss: 2.80654
Epoch: 3/20, step: 51060, training_loss: 2.14749
Epoch: 3/20, step: 51080, training_loss: 1.67423
Epoch: 3/20, step: 51100, training_loss: 2.13694
Epoch: 3/20, step: 51120, training_loss: 1.82258
Epoch: 3/20, step: 51140, training_loss: 2.80446
Epoch: 3/20, step: 51160, training_loss: 1.46232
Epoch: 3/20, step: 51180, training_loss: 2.32240
Epoch: 3/20, step: 51200, training_loss: 2.48696
Epoch: 3/20, step: 51220, training_loss: 2.12979
Epoch: 3/20, step: 51240, training_loss: 1.94158
Epoch: 3/20, step: 51260, training_loss: 1.61176
Epoch: 3/20, step: 51280, training_loss: 1.94673
Epoch: 3/20, step: 51300, training_loss: 1.97681
Epoch: 3/20, step: 51320, training_loss: 1.38510
Epoch: 3/20, step: 51340, training_loss: 1.92731
Epoch: 3/20, step: 51360, training_loss: 2.13400
Epoch: 3/20, step: 51380, training_loss: 2.54147
Epoch: 3/20, step: 51400, training_loss: 2.55526
Epoch: 3/20, step: 51420, training_loss: 2.11654
Epoch: 3/20, step: 51440, training_loss: 2.31229
Epoch: 3/20, step: 51460, training_loss: 2.75359
Epoch: 3/20, step: 51480, training_loss: 2.26418
Epoch: 3/20, step: 51500, training_loss: 2.62582
Epoch: 3/20, step: 51520, training_loss: 2.53814
Epoch: 3/20, step: 51540, training_loss: 1.80319
Epoch: 3/20, step: 51560, training_loss: 1.96935
Epoch: 3/20, step: 51580, training_loss: 1.51376
Epoch: 3/20, step: 51600, training_loss: 2.03557
Epoch: 3/20, step: 51620, training_loss: 2.28513
Epoch: 3/20, step: 51640, training_loss: 1.76491
Epoch: 3/20, step: 51660, training_loss: 1.69768
Epoch: 3/20, step: 51680, training_loss: 2.39648
Epoch: 3/20, step: 51700, training_loss: 3.15620
Epoch: 3/20, step: 51720, training_loss: 1.64375
Epoch: 3/20, step: 51740, training_loss: 2.09902
Epoch: 3/20, step: 51760, training_loss: 2.31310
Epoch: 3/20, step: 51780, training_loss: 2.41969
Epoch: 3/20, step: 51800, training_loss: 2.33646
Epoch: 3/20, step: 51820, training_loss: 1.76993
Epoch: 3/20, step: 51840, training_loss: 2.33857
Epoch: 3/20, step: 51860, training_loss: 2.21698
Epoch: 3/20, step: 51880, training_loss: 2.29937
Epoch: 3/20, step: 51900, training_loss: 2.59260
Epoch: 3/20, step: 51920, training_loss: 2.15557
Epoch: 3/20, step: 51940, training_loss: 1.60386
Epoch: 3/20, step: 51960, training_loss: 1.45332
Epoch: 3/20, step: 51980, training_loss: 2.04456
Epoch: 3/20, step: 52000, training_loss: 1.34789
accuracy: 0.4, validation_loss: 2.241781234741211, num_samples: 100
Epoch: 3/20, step: 52020, training_loss: 2.51998
Epoch: 3/20, step: 52040, training_loss: 2.56351
Epoch: 3/20, step: 52060, training_loss: 3.18725
Epoch: 3/20, step: 52080, training_loss: 2.53366
Epoch: 3/20, step: 52100, training_loss: 2.23981
Epoch: 3/20, step: 52120, training_loss: 2.60201
Epoch: 3/20, step: 52140, training_loss: 1.37429
Epoch: 3/20, step: 52160, training_loss: 2.07190
Epoch: 3/20, step: 52180, training_loss: 2.03132
Epoch: 3/20, step: 52200, training_loss: 3.02162
Epoch: 3/20, step: 52220, training_loss: 1.55918
Epoch: 3/20, step: 52240, training_loss: 3.04836
Epoch: 3/20, step: 52260, training_loss: 3.25414
Epoch: 3/20, step: 52280, training_loss: 2.15089
Epoch: 3/20, step: 52300, training_loss: 2.28794
Epoch: 3/20, step: 52320, training_loss: 1.63672
Epoch: 3/20, step: 52340, training_loss: 2.22824
Epoch: 3/20, step: 52360, training_loss: 2.64222
Epoch: 3/20, step: 52380, training_loss: 1.85418
Epoch: 3/20, step: 52400, training_loss: 2.76879
Epoch: 3/20, step: 52420, training_loss: 2.82675
Epoch: 3/20, step: 52440, training_loss: 1.25914
Epoch: 3/20, step: 52460, training_loss: 2.59140
Epoch: 3/20, step: 52480, training_loss: 1.64311
Epoch: 3/20, step: 52500, training_loss: 1.75029
Epoch: 3/20, step: 52520, training_loss: 2.41800
Epoch: 3/20, step: 52540, training_loss: 2.46432
Epoch: 3/20, step: 52560, training_loss: 2.65605
Epoch: 3/20, step: 52580, training_loss: 1.94359
Epoch: 3/20, step: 52600, training_loss: 2.77015
Epoch: 3/20, step: 52620, training_loss: 1.86005
Epoch: 3/20, step: 52640, training_loss: 1.65237
Epoch: 3/20, step: 52660, training_loss: 1.76470
Epoch: 3/20, step: 52680, training_loss: 2.09114
Epoch: 3/20, step: 52700, training_loss: 2.21759
Epoch: 3/20, step: 52720, training_loss: 2.35120
Epoch: 3/20, step: 52740, training_loss: 2.63128
Epoch: 3/20, step: 52760, training_loss: 1.77445
Epoch: 3/20, step: 52780, training_loss: 2.05836
Epoch: 3/20, step: 52800, training_loss: 1.75127
Epoch: 3/20, step: 52820, training_loss: 1.31277
Epoch: 3/20, step: 52840, training_loss: 2.04585
Epoch: 3/20, step: 52860, training_loss: 2.27502
Epoch: 3/20, step: 52880, training_loss: 1.58952
Epoch: 3/20, step: 52900, training_loss: 2.01577
Epoch: 3/20, step: 52920, training_loss: 1.63351
Epoch: 3/20, step: 52940, training_loss: 3.17463
Epoch: 3/20, step: 52960, training_loss: 2.44428
Epoch: 3/20, step: 52980, training_loss: 1.70163
Epoch: 3/20, step: 53000, training_loss: 1.27233
accuracy: 0.43, validation_loss: 2.095353841781616, num_samples: 100
Epoch: 3/20, step: 53020, training_loss: 2.35893
Epoch: 3/20, step: 53040, training_loss: 1.97215
Epoch: 3/20, step: 53060, training_loss: 2.53997
Epoch: 3/20, step: 53080, training_loss: 2.19045
Epoch: 3/20, step: 53100, training_loss: 2.48158
Epoch: 3/20, step: 53120, training_loss: 2.44198
Epoch: 3/20, step: 53140, training_loss: 1.29391
Epoch: 3/20, step: 53160, training_loss: 2.28774
Epoch: 3/20, step: 53180, training_loss: 2.51653
Epoch: 3/20, step: 53200, training_loss: 1.81701
Epoch: 3/20, step: 53220, training_loss: 2.54650
Epoch: 3/20, step: 53240, training_loss: 1.70929
Epoch: 3/20, step: 53260, training_loss: 1.78747
Epoch: 3/20, step: 53280, training_loss: 2.18033
Epoch: 3/20, step: 53300, training_loss: 2.13178
Epoch: 3/20, step: 53320, training_loss: 2.22978
Epoch: 3/20, step: 53340, training_loss: 2.18441
Epoch: 3/20, step: 53360, training_loss: 2.50338
Epoch: 3/20, step: 53380, training_loss: 2.27218
Epoch: 3/20, step: 53400, training_loss: 1.61213
Epoch: 3/20, step: 53420, training_loss: 2.24244
Epoch: 3/20, step: 53440, training_loss: 2.36902
Epoch: 3/20, step: 53460, training_loss: 2.21345
Epoch: 3/20, step: 53480, training_loss: 2.10214
Epoch: 3/20, step: 53500, training_loss: 2.02951
Epoch: 3/20, step: 53520, training_loss: 2.55292
Epoch: 3/20, step: 53540, training_loss: 2.12789
Epoch: 3/20, step: 53560, training_loss: 2.36976
Epoch: 3/20, step: 53580, training_loss: 1.97147
Epoch: 3/20, step: 53600, training_loss: 2.22606
Epoch: 3/20, step: 53620, training_loss: 2.44073
Epoch: 3/20, step: 53640, training_loss: 2.01226
Epoch: 3/20, step: 53660, training_loss: 2.47200
Epoch: 3/20, step: 53680, training_loss: 2.85806
Epoch: 3/20, step: 53700, training_loss: 1.97713
Epoch: 3/20, step: 53720, training_loss: 1.69308
Epoch: 3/20, step: 53740, training_loss: 2.84678
Epoch: 3/20, step: 53760, training_loss: 1.93188
Epoch: 3/20, step: 53780, training_loss: 1.61006
Epoch: 3/20, step: 53800, training_loss: 1.91022
Epoch: 3/20, step: 53820, training_loss: 2.01792
Epoch: 3/20, step: 53840, training_loss: 1.65971
Epoch: 3/20, step: 53860, training_loss: 2.68605
Epoch: 3/20, step: 53880, training_loss: 1.53183
Epoch: 3/20, step: 53900, training_loss: 2.10928
Epoch: 3/20, step: 53920, training_loss: 1.71046
Epoch: 3/20, step: 53940, training_loss: 2.64824
Epoch: 3/20, step: 53960, training_loss: 2.03226
Epoch: 3/20, step: 53980, training_loss: 1.70842
Epoch: 3/20, step: 54000, training_loss: 1.61670
accuracy: 0.45, validation_loss: 2.1476523876190186, num_samples: 100
Epoch: 3/20, step: 54020, training_loss: 1.90462
Epoch: 3/20, step: 54040, training_loss: 1.70887
Epoch: 3/20, step: 54060, training_loss: 2.02858
Epoch: 3/20, step: 54080, training_loss: 1.69014
Epoch: 3/20, step: 54100, training_loss: 2.48495
Epoch: 3/20, step: 54120, training_loss: 1.95682
Epoch: 3/20, step: 54140, training_loss: 1.83400
Epoch: 3/20, step: 54160, training_loss: 2.13939
Epoch: 3/20, step: 54180, training_loss: 2.12196
Epoch: 3/20, step: 54200, training_loss: 1.95757
Epoch: 3/20, step: 54220, training_loss: 1.92235
Epoch: 3/20, step: 54240, training_loss: 2.18728
Epoch: 3/20, step: 54260, training_loss: 2.12300
Epoch: 3/20, step: 54280, training_loss: 1.82460
Epoch: 3/20, step: 54300, training_loss: 2.24836
Epoch: 3/20, step: 54320, training_loss: 2.31294
Epoch: 3/20, step: 54340, training_loss: 2.16978
Epoch: 3/20, step: 54360, training_loss: 1.63673
Epoch: 3/20, step: 54380, training_loss: 2.36716
Epoch: 3/20, step: 54400, training_loss: 1.84698
Epoch: 3/20, step: 54420, training_loss: 1.80035
Epoch: 3/20, step: 54440, training_loss: 1.84488
Epoch: 3/20, step: 54460, training_loss: 1.83424
Epoch: 3/20, step: 54480, training_loss: 2.99237
Epoch: 3/20, step: 54500, training_loss: 2.33202
Epoch: 3/20, step: 54520, training_loss: 2.74374
Epoch: 3/20, step: 54540, training_loss: 2.80013
Epoch: 3/20, step: 54560, training_loss: 2.73478
Epoch: 3/20, step: 54580, training_loss: 1.28420
Epoch: 3/20, step: 54600, training_loss: 2.00407
Epoch: 3/20, step: 54620, training_loss: 2.02933
Epoch: 3/20, step: 54640, training_loss: 2.26737
Epoch: 3/20, step: 54660, training_loss: 2.36368
Epoch: 3/20, step: 54680, training_loss: 2.66179
Epoch: 3/20, step: 54700, training_loss: 2.60600
Epoch: 3/20, step: 54720, training_loss: 1.97516
Epoch: 3/20, step: 54740, training_loss: 2.41011
Epoch: 3/20, step: 54760, training_loss: 2.72594
Epoch: 3/20, step: 54780, training_loss: 1.67819
Epoch: 3/20, step: 54800, training_loss: 1.56150
Epoch: 3/20, step: 54820, training_loss: 1.65581
Epoch: 3/20, step: 54840, training_loss: 2.29784
Epoch: 3/20, step: 54860, training_loss: 2.82887
Epoch: 3/20, step: 54880, training_loss: 1.89435
Epoch: 3/20, step: 54900, training_loss: 2.71537
Epoch: 3/20, step: 54920, training_loss: 1.73792
Epoch: 3/20, step: 54940, training_loss: 3.35447
Epoch: 3/20, step: 54960, training_loss: 1.62280
Epoch: 3/20, step: 54980, training_loss: 2.65350
Epoch: 3/20, step: 55000, training_loss: 1.90474
accuracy: 0.38, validation_loss: 2.188892126083374, num_samples: 100
Epoch: 3/20, step: 55020, training_loss: 3.65115
Epoch: 3/20, step: 55040, training_loss: 2.31161
Epoch: 3/20, step: 55060, training_loss: 2.06595
Epoch: 3/20, step: 55080, training_loss: 2.61694
Epoch: 3/20, step: 55100, training_loss: 3.00884
Epoch: 3/20, step: 55120, training_loss: 2.91193
Epoch: 3/20, step: 55140, training_loss: 2.96252
Epoch: 3/20, step: 55160, training_loss: 3.13327
Epoch: 3/20, step: 55180, training_loss: 2.01686
Epoch: 3/20, step: 55200, training_loss: 2.42954
Epoch: 3/20, step: 55220, training_loss: 2.03422
Epoch: 3/20, step: 55240, training_loss: 2.51864
Epoch: 3/20, step: 55260, training_loss: 1.46894
Epoch: 3/20, step: 55280, training_loss: 1.62361
Epoch: 3/20, step: 55300, training_loss: 2.07177
Epoch: 3/20, step: 55320, training_loss: 2.29267
Epoch: 3/20, step: 55340, training_loss: 2.32953
Epoch: 3/20, step: 55360, training_loss: 2.57813
Epoch: 3/20, step: 55380, training_loss: 2.80707
Epoch: 3/20, step: 55400, training_loss: 1.88574
Epoch: 3/20, step: 55420, training_loss: 1.57258
Epoch: 3/20, step: 55440, training_loss: 2.76487
Epoch: 3/20, step: 55460, training_loss: 2.65075
Epoch: 3/20, step: 55480, training_loss: 2.36542
Epoch: 3/20, step: 55500, training_loss: 2.20506
Epoch: 3/20, step: 55520, training_loss: 1.44478
Epoch: 3/20, step: 55540, training_loss: 2.08120
Epoch: 3/20, step: 55560, training_loss: 2.52477
Epoch: 3/20, step: 55580, training_loss: 2.09572
Epoch: 3/20, step: 55600, training_loss: 2.47784
Epoch: 3/20, step: 55620, training_loss: 2.58079
Epoch: 3/20, step: 55640, training_loss: 2.45001
Epoch: 3/20, step: 55660, training_loss: 2.00826
Epoch: 3/20, step: 55680, training_loss: 1.66479
Epoch: 3/20, step: 55700, training_loss: 2.06498
Epoch: 3/20, step: 55720, training_loss: 2.40526
Epoch: 3/20, step: 55740, training_loss: 2.25462
Epoch: 3/20, step: 55760, training_loss: 2.96357
Epoch: 3/20, step: 55780, training_loss: 2.12409
Epoch: 3/20, step: 55800, training_loss: 2.03206
Epoch: 3/20, step: 55820, training_loss: 2.17721
Epoch: 3/20, step: 55840, training_loss: 1.91770
Epoch: 3/20, step: 55860, training_loss: 2.24169
Epoch: 3/20, step: 55880, training_loss: 2.67831
Epoch: 3/20, step: 55900, training_loss: 1.46749
Epoch: 3/20, step: 55920, training_loss: 1.50353
Epoch: 3/20, step: 55940, training_loss: 2.38386
Epoch: 3/20, step: 55960, training_loss: 2.70444
Epoch: 3/20, step: 55980, training_loss: 2.59967
Epoch: 3/20, step: 56000, training_loss: 2.67453
accuracy: 0.34, validation_loss: 2.3111727237701416, num_samples: 100
Epoch: 3/20, step: 56020, training_loss: 2.59093
Epoch: 3/20, step: 56040, training_loss: 2.90712
Epoch: 3/20, step: 56060, training_loss: 1.80992
Epoch: 3/20, step: 56080, training_loss: 2.22084
Epoch: 3/20, step: 56100, training_loss: 2.90440
Epoch: 3/20, step: 56120, training_loss: 2.25374
Epoch: 3/20, step: 56140, training_loss: 2.13581
Epoch: 3/20, step: 56160, training_loss: 1.79605
Epoch: 3/20, step: 56180, training_loss: 1.64586
Epoch: 3/20, step: 56200, training_loss: 2.77575
Epoch: 3/20, step: 56220, training_loss: 2.27531
Epoch: 3/20, step: 56240, training_loss: 2.24171
Epoch: 3/20, step: 56260, training_loss: 1.90057
Epoch: 3/20, step: 56280, training_loss: 2.69614
Epoch: 3/20, step: 56300, training_loss: 2.03030
Epoch: 3/20, step: 56320, training_loss: 1.51533
Epoch: 3/20, step: 56340, training_loss: 1.85241
Epoch: 3/20, step: 56360, training_loss: 2.10580
Epoch: 3/20, step: 56380, training_loss: 2.19613
Epoch: 3/20, step: 56400, training_loss: 2.31554
Epoch: 3/20, step: 56420, training_loss: 1.91194
Epoch: 3/20, step: 56440, training_loss: 1.29227
Epoch: 3/20, step: 56460, training_loss: 1.78105
Epoch: 3/20, step: 56480, training_loss: 1.89913
Epoch: 3/20, step: 56500, training_loss: 2.38866
Epoch: 3/20, step: 56520, training_loss: 2.62479
Epoch: 3/20, step: 56540, training_loss: 2.09053
Epoch: 3/20, step: 56560, training_loss: 2.67284
Epoch: 3/20, step: 56580, training_loss: 2.22190
Epoch: 3/20, step: 56600, training_loss: 2.30301
Epoch: 3/20, step: 56620, training_loss: 1.97958
Epoch: 3/20, step: 56640, training_loss: 2.37542
Epoch: 3/20, step: 56660, training_loss: 1.57135
Epoch: 3/20, step: 56680, training_loss: 2.15467
Epoch: 3/20, step: 56700, training_loss: 2.47453
Epoch: 3/20, step: 56720, training_loss: 3.00348
Epoch: 3/20, step: 56740, training_loss: 2.48686
Epoch: 3/20, step: 56760, training_loss: 1.70492
Epoch: 3/20, step: 56780, training_loss: 2.98799
Epoch: 3/20, step: 56800, training_loss: 2.88782
Epoch: 3/20, step: 56820, training_loss: 2.50328
Epoch: 3/20, step: 56840, training_loss: 2.38186
Epoch: 3/20, step: 56860, training_loss: 2.21976
Epoch: 3/20, step: 56880, training_loss: 2.33277
Epoch: 3/20, step: 56900, training_loss: 2.58511
Epoch: 3/20, step: 56920, training_loss: 2.67105
Epoch: 3/20, step: 56940, training_loss: 2.12169
Epoch: 3/20, step: 56960, training_loss: 2.16646
Epoch: 3/20, step: 56980, training_loss: 2.70057
Epoch: 3/20, step: 57000, training_loss: 1.88112
accuracy: 0.42, validation_loss: 2.1190052032470703, num_samples: 100
Epoch: 3/20, step: 57020, training_loss: 2.63704
Epoch: 3/20, step: 57040, training_loss: 2.49413
Epoch: 3/20, step: 57060, training_loss: 2.59597
Epoch: 3/20, step: 57080, training_loss: 2.23826
Epoch: 3/20, step: 57100, training_loss: 2.07975
Epoch: 3/20, step: 57120, training_loss: 2.39817
Epoch: 3/20, step: 57140, training_loss: 1.63839
Epoch: 3/20, step: 57160, training_loss: 2.06984
Epoch: 3/20, step: 57180, training_loss: 2.11007
Epoch: 3/20, step: 57200, training_loss: 1.51968
Epoch: 3/20, step: 57220, training_loss: 2.58722
Epoch: 3/20, step: 57240, training_loss: 2.45372
Epoch: 3/20, step: 57260, training_loss: 1.87899
Epoch: 3/20, step: 57280, training_loss: 3.31341
Epoch: 3/20, step: 57300, training_loss: 2.18174
Epoch: 3/20, step: 57320, training_loss: 2.73057
Epoch: 3/20, step: 57340, training_loss: 1.90829
Epoch: 3/20, step: 57360, training_loss: 1.77470
Epoch: 3/20, step: 57380, training_loss: 2.03564
Epoch: 3/20, step: 57400, training_loss: 1.80892
Epoch: 3/20, step: 57420, training_loss: 2.72066
Epoch: 3/20, step: 57440, training_loss: 2.38718
Epoch: 3/20, step: 57460, training_loss: 2.43226
Epoch: 3/20, step: 57480, training_loss: 1.64961
Epoch: 3/20, step: 57500, training_loss: 2.43203
Epoch: 3/20, step: 57520, training_loss: 2.29381
Epoch: 3/20, step: 57540, training_loss: 2.68740
Epoch: 3/20, step: 57560, training_loss: 1.84317
Epoch: 3/20, step: 57580, training_loss: 1.38964
Epoch: 3/20, step: 57600, training_loss: 3.16082
Epoch: 3/20, step: 57620, training_loss: 1.97996
Epoch: 3/20, step: 57640, training_loss: 1.57084
Epoch: 3/20, step: 57660, training_loss: 2.77515
Epoch: 3/20, step: 57680, training_loss: 1.94060
Epoch: 3/20, step: 57700, training_loss: 1.87088
Epoch: 3/20, step: 57720, training_loss: 2.17706
Epoch: 3/20, step: 57740, training_loss: 2.76110
Epoch: 3/20, step: 57760, training_loss: 1.42398
Epoch: 3/20, step: 57780, training_loss: 1.64735
Epoch: 3/20, step: 57800, training_loss: 1.69605
Epoch: 3/20, step: 57820, training_loss: 2.58775
Epoch: 3/20, step: 57840, training_loss: 1.92365
Epoch: 3/20, step: 57860, training_loss: 2.33065
Epoch: 3/20, step: 57880, training_loss: 2.48705
Epoch: 3/20, step: 57900, training_loss: 2.00497
Epoch: 3/20, step: 57920, training_loss: 1.94594
Epoch: 3/20, step: 57940, training_loss: 1.30090
Epoch: 3/20, step: 57960, training_loss: 1.66496
Epoch: 3/20, step: 57980, training_loss: 2.18087
Epoch: 3/20, step: 58000, training_loss: 1.97433
accuracy: 0.37, validation_loss: 2.197361946105957, num_samples: 100
Epoch: 3/20, step: 58020, training_loss: 2.32721
Epoch: 3/20, step: 58040, training_loss: 1.88130
Epoch: 3/20, step: 58060, training_loss: 1.84184
Epoch: 3/20, step: 58080, training_loss: 1.84952
Epoch: 3/20, step: 58100, training_loss: 1.60444
Epoch: 3/20, step: 58120, training_loss: 2.10100
Epoch: 3/20, step: 58140, training_loss: 2.41605
Epoch: 3/20, step: 58160, training_loss: 1.50477
Epoch: 3/20, step: 58180, training_loss: 2.71490
Epoch: 3/20, step: 58200, training_loss: 2.11545
Epoch: 3/20, step: 58220, training_loss: 2.59751
Epoch: 3/20, step: 58240, training_loss: 1.90073
Epoch: 3/20, step: 58260, training_loss: 1.43721
Epoch: 3/20, step: 58280, training_loss: 1.67099
Epoch: 3/20, step: 58300, training_loss: 2.50797
Epoch: 3/20, step: 58320, training_loss: 2.40023
Epoch: 3/20, step: 58340, training_loss: 2.91767
Epoch: 3/20, step: 58360, training_loss: 2.54753
Epoch: 3/20, step: 58380, training_loss: 2.05172
Epoch: 3/20, step: 58400, training_loss: 2.32159
Epoch: 3/20, step: 58420, training_loss: 2.19823
Epoch: 3/20, step: 58440, training_loss: 1.70646
Epoch: 3/20, step: 58460, training_loss: 3.04554
Epoch: 3/20, step: 58480, training_loss: 1.86841
Epoch: 3/20, step: 58500, training_loss: 2.04624
Epoch: 3/20, step: 58520, training_loss: 2.62031
Epoch: 3/20, step: 58540, training_loss: 2.91831
Epoch: 3/20, step: 58560, training_loss: 2.74327
Epoch: 3/20, step: 58580, training_loss: 1.86778
Epoch: 3/20, step: 58600, training_loss: 2.34851
Epoch: 3/20, step: 58620, training_loss: 2.56653
Epoch: 3/20, step: 58640, training_loss: 1.58957
Epoch: 3/20, step: 58660, training_loss: 2.29810
Epoch: 3/20, step: 58680, training_loss: 2.61579
Epoch: 3/20, step: 58700, training_loss: 2.60060
Epoch: 3/20, step: 58720, training_loss: 1.42590
Epoch: 3/20, step: 58740, training_loss: 2.03258
Epoch: 3/20, step: 58760, training_loss: 2.47110
Epoch: 3/20, step: 58780, training_loss: 2.16039
Epoch: 3/20, step: 58800, training_loss: 2.05869
Epoch: 3/20, step: 58820, training_loss: 1.94696
Epoch: 3/20, step: 58840, training_loss: 2.48140
Epoch: 3/20, step: 58860, training_loss: 2.21928
Epoch: 3/20, step: 58880, training_loss: 3.03693
Epoch: 3/20, step: 58900, training_loss: 2.08135
Epoch: 3/20, step: 58920, training_loss: 2.40731
Epoch: 3/20, step: 58940, training_loss: 2.98846
Epoch: 3/20, step: 58960, training_loss: 2.26456
Epoch: 3/20, step: 58980, training_loss: 2.50946
Epoch: 3/20, step: 59000, training_loss: 1.82750
accuracy: 0.37, validation_loss: 2.0961039066314697, num_samples: 100
Epoch: 3/20, step: 59020, training_loss: 1.36588
Epoch: 3/20, step: 59040, training_loss: 2.29069
Epoch: 3/20, step: 59060, training_loss: 1.90921
Epoch: 3/20, step: 59080, training_loss: 1.87929
Epoch: 3/20, step: 59100, training_loss: 1.92998
Epoch: 3/20, step: 59120, training_loss: 1.90633
Epoch: 3/20, step: 59140, training_loss: 2.62855
Epoch: 3/20, step: 59160, training_loss: 1.64564
Epoch: 3/20, step: 59180, training_loss: 1.95724
Epoch: 3/20, step: 59200, training_loss: 2.14948
Epoch: 3/20, step: 59220, training_loss: 3.13160
Epoch: 3/20, step: 59240, training_loss: 2.40603
Epoch: 3/20, step: 59260, training_loss: 2.58384
Epoch: 3/20, step: 59280, training_loss: 2.12440
Epoch: 3/20, step: 59300, training_loss: 2.59271
Epoch: 3/20, step: 59320, training_loss: 1.86955
Epoch: 3/20, step: 59340, training_loss: 1.94456
Epoch: 3/20, step: 59360, training_loss: 2.40268
Epoch: 3/20, step: 59380, training_loss: 2.46261
Epoch: 3/20, step: 59400, training_loss: 1.72174
Epoch: 3/20, step: 59420, training_loss: 1.73738
Epoch: 3/20, step: 59440, training_loss: 2.04945
Epoch: 3/20, step: 59460, training_loss: 2.53661
Epoch: 3/20, step: 59480, training_loss: 2.73998
Epoch: 3/20, step: 59500, training_loss: 1.53898
Epoch: 3/20, step: 59520, training_loss: 2.29829
Epoch: 3/20, step: 59540, training_loss: 2.55700
Epoch: 3/20, step: 59560, training_loss: 1.63903
Epoch: 3/20, step: 59580, training_loss: 1.84012
Epoch: 3/20, step: 59600, training_loss: 1.64922
Epoch: 3/20, step: 59620, training_loss: 1.91236
Epoch: 3/20, step: 59640, training_loss: 2.09602
Epoch: 3/20, step: 59660, training_loss: 1.39454
Epoch: 3/20, step: 59680, training_loss: 2.15343
Epoch: 3/20, step: 59700, training_loss: 2.17043
Epoch: 3/20, step: 59720, training_loss: 2.72420
Epoch: 3/20, step: 59740, training_loss: 2.59879
Epoch: 3/20, step: 59760, training_loss: 1.45201
Epoch: 3/20, step: 59780, training_loss: 2.38101
Epoch: 3/20, step: 59800, training_loss: 2.34579
Epoch: 3/20, step: 59820, training_loss: 1.92694
Epoch: 3/20, step: 59840, training_loss: 1.95042
Epoch: 3/20, step: 59860, training_loss: 2.76760
Epoch: 3/20, step: 59880, training_loss: 2.21189
Epoch: 3/20, step: 59900, training_loss: 1.86035
Epoch: 3/20, step: 59920, training_loss: 1.48146
Epoch: 3/20, step: 59940, training_loss: 1.81978
Epoch: 3/20, step: 59960, training_loss: 1.57585
Epoch: 3/20, step: 59980, training_loss: 2.26010
Epoch: 3/20, step: 60000, training_loss: 1.73055
accuracy: 0.5, validation_loss: 1.9252979755401611, num_samples: 100
Epoch: 3/20, step: 60020, training_loss: 2.02769
Epoch: 3/20, step: 60040, training_loss: 2.97781
Epoch: 3/20, step: 60060, training_loss: 2.92955
Epoch: 3/20, step: 60080, training_loss: 2.50770
Epoch: 3/20, step: 60100, training_loss: 2.23022
Epoch: 3/20, step: 60120, training_loss: 1.88427
Epoch: 3/20, step: 60140, training_loss: 2.68401
Epoch: 3/20, step: 60160, training_loss: 2.70260
Epoch: 3/20, step: 60180, training_loss: 2.15969
Epoch: 3/20, step: 60200, training_loss: 2.07167
Epoch: 3/20, step: 60220, training_loss: 1.85668
Epoch: 3/20, step: 60240, training_loss: 2.72723
Epoch: 3/20, step: 60260, training_loss: 2.17434
Epoch: 3/20, step: 60280, training_loss: 1.83825
Epoch: 3/20, step: 60300, training_loss: 2.67536
Epoch: 3/20, step: 60320, training_loss: 2.24207
Epoch: 3/20, step: 60340, training_loss: 2.48216
Epoch: 3/20, step: 60360, training_loss: 2.68367
Epoch: 3/20, step: 60380, training_loss: 1.90509
Epoch: 3/20, step: 60400, training_loss: 2.23831
Epoch: 3/20, step: 60420, training_loss: 2.88852
Epoch: 3/20, step: 60440, training_loss: 2.34940
Epoch: 3/20, step: 60460, training_loss: 2.29555
Epoch: 3/20, step: 60480, training_loss: 1.90283
Epoch: 3/20, step: 60500, training_loss: 1.71846
Epoch: 3/20, step: 60520, training_loss: 2.73864
Epoch: 3/20, step: 60540, training_loss: 1.89153
Epoch: 3/20, step: 60560, training_loss: 1.82114
Epoch: 3/20, step: 60580, training_loss: 1.77115
Epoch: 3/20, step: 60600, training_loss: 2.51124
Epoch: 3/20, step: 60620, training_loss: 2.67590
Epoch: 3/20, step: 60640, training_loss: 2.05763
Epoch: 3/20, step: 60660, training_loss: 1.38348
Epoch: 3/20, step: 60680, training_loss: 1.55692
Epoch: 3/20, step: 60700, training_loss: 1.57760
Epoch: 3/20, step: 60720, training_loss: 2.29811
Epoch: 3/20, step: 60740, training_loss: 1.78450
Epoch: 3/20, step: 60760, training_loss: 1.97796
Epoch: 3/20, step: 60780, training_loss: 2.42943
Epoch: 3/20, step: 60800, training_loss: 1.79487
Epoch: 3/20, step: 60820, training_loss: 2.30784
Epoch: 3/20, step: 60840, training_loss: 3.08452
Epoch: 3/20, step: 60860, training_loss: 2.76809
Epoch: 3/20, step: 60880, training_loss: 1.65743
Epoch: 3/20, step: 60900, training_loss: 1.86383
Epoch: 3/20, step: 60920, training_loss: 1.96783
Epoch: 3/20, step: 60940, training_loss: 2.00217
Epoch: 3/20, step: 60960, training_loss: 2.73876
Epoch: 3/20, step: 60980, training_loss: 1.58812
Epoch: 3/20, step: 61000, training_loss: 1.85253
accuracy: 0.41, validation_loss: 2.025444507598877, num_samples: 100
Epoch: 3/20, step: 61020, training_loss: 2.01770
Epoch: 3/20, step: 61040, training_loss: 2.18048
Epoch: 3/20, step: 61060, training_loss: 1.72037
Epoch: 3/20, step: 61080, training_loss: 2.47910
Epoch: 3/20, step: 61100, training_loss: 2.02691
Epoch: 3/20, step: 61120, training_loss: 1.87675
Epoch: 3/20, step: 61140, training_loss: 1.12339
Epoch: 3/20, step: 61160, training_loss: 2.93019
Epoch: 3/20, step: 61180, training_loss: 1.85025
Epoch: 3/20, step: 61200, training_loss: 2.02592
Epoch: 3/20, step: 61220, training_loss: 2.23489
Epoch: 3/20, step: 61240, training_loss: 2.96107
Epoch: 3/20, step: 61260, training_loss: 1.89700
Epoch: 3/20, step: 61280, training_loss: 2.88614
Epoch: 3/20, step: 61300, training_loss: 1.91203
Epoch: 3/20, step: 61320, training_loss: 2.60354
Epoch: 3/20, step: 61340, training_loss: 2.52695
Epoch: 3/20, step: 61360, training_loss: 1.37082
Epoch: 3/20, step: 61380, training_loss: 2.16854
Epoch: 3/20, step: 61400, training_loss: 1.67672
Epoch: 3/20, step: 61420, training_loss: 2.08382
Epoch: 3/20, step: 61440, training_loss: 2.38078
Epoch: 3/20, step: 61460, training_loss: 2.24917
Epoch: 3/20, step: 61480, training_loss: 1.24064
Epoch: 3/20, step: 61500, training_loss: 2.63244
Epoch: 3/20, step: 61520, training_loss: 1.61409
Epoch: 3/20, step: 61540, training_loss: 1.85612
Epoch: 3/20, step: 61560, training_loss: 2.50066
Epoch: 3/20, step: 61580, training_loss: 2.48692
Epoch: 3/20, step: 61600, training_loss: 2.57142
Epoch: 3/20, step: 61620, training_loss: 1.92413
Epoch: 3/20, step: 61640, training_loss: 1.77418
Epoch: 3/20, step: 61660, training_loss: 2.24904
Epoch: 3/20, step: 61680, training_loss: 2.75782
Epoch: 3/20, step: 61700, training_loss: 1.46644
Epoch: 3/20, step: 61720, training_loss: 2.54318
Epoch: 3/20, step: 61740, training_loss: 3.32614
Epoch: 3/20, step: 61760, training_loss: 1.84189
Epoch: 3/20, step: 61780, training_loss: 1.95172
Epoch: 3/20, step: 61800, training_loss: 2.09387
Epoch: 3/20, step: 61820, training_loss: 2.58410
Epoch: 3/20, step: 61840, training_loss: 2.80550
Epoch: 3/20, step: 61860, training_loss: 2.05235
Epoch: 3/20, step: 61880, training_loss: 1.93092
Epoch: 3/20, step: 61900, training_loss: 1.62542
Epoch: 3/20, step: 61920, training_loss: 2.41876
Epoch: 3/20, step: 61940, training_loss: 2.07807
Epoch: 3/20, step: 61960, training_loss: 3.12664
Epoch: 3/20, step: 61980, training_loss: 2.55158
Epoch: 3/20, step: 62000, training_loss: 1.88949
accuracy: 0.53, validation_loss: 1.7750885486602783, num_samples: 100
Epoch: 3/20, step: 62020, training_loss: 1.02497
Epoch: 3/20, step: 62040, training_loss: 2.13126
Epoch: 3/20, step: 62060, training_loss: 2.78910
Epoch: 3/20, step: 62080, training_loss: 2.48379
Epoch: 3/20, step: 62100, training_loss: 2.38852
Epoch: 3/20, step: 62120, training_loss: 3.24215
Epoch: 3/20, step: 62140, training_loss: 1.93706
Epoch: 3/20, step: 62160, training_loss: 2.30930
Epoch: 3/20, step: 62180, training_loss: 1.80603
Epoch: 3/20, step: 62200, training_loss: 1.44180
Epoch: 3/20, step: 62220, training_loss: 1.79728
Epoch: 3/20, step: 62240, training_loss: 1.97917
Epoch: 3/20, step: 62260, training_loss: 2.74561
Epoch: 3/20, step: 62280, training_loss: 2.08844
Epoch: 3/20, step: 62300, training_loss: 2.32741
Epoch: 3/20, step: 62320, training_loss: 1.65582
Epoch: 3/20, step: 62340, training_loss: 2.40591
Epoch: 3/20, step: 62360, training_loss: 2.82335
Epoch: 3/20, step: 62380, training_loss: 2.34560
Epoch: 3/20, step: 62400, training_loss: 2.51745
Epoch: 3/20, step: 62420, training_loss: 2.42388
Epoch: 3/20, step: 62440, training_loss: 1.81273
Epoch: 3/20, step: 62460, training_loss: 2.25320
Epoch: 3/20, step: 62480, training_loss: 1.82768
Epoch: 3/20, step: 62500, training_loss: 2.43614
Epoch: 3/20, step: 62520, training_loss: 2.18871
Epoch: 3/20, step: 62540, training_loss: 3.27535
Epoch: 3/20, step: 62560, training_loss: 2.79298
Epoch: 3/20, step: 62580, training_loss: 2.71747
Epoch: 3/20, step: 62600, training_loss: 2.55724
Epoch: 3/20, step: 62620, training_loss: 1.27768
Epoch: 3/20, step: 62640, training_loss: 2.44680
Epoch: 3/20, step: 62660, training_loss: 1.24417
Epoch: 3/20, step: 62680, training_loss: 2.45079
Epoch: 3/20, step: 62700, training_loss: 1.99008
Epoch: 3/20, step: 62720, training_loss: 2.15825
Epoch: 3/20, step: 62740, training_loss: 2.70164
Epoch: 3/20, step: 62760, training_loss: 2.60701
Epoch: 3/20, step: 62780, training_loss: 2.22351
Epoch: 3/20, step: 62800, training_loss: 2.35991
Epoch: 3/20, step: 62820, training_loss: 2.59980
Epoch: 3/20, step: 62840, training_loss: 2.55451
Epoch: 3/20, step: 62860, training_loss: 2.19674
Epoch: 3/20, step: 62880, training_loss: 2.39031
Epoch: 3/20, step: 62900, training_loss: 2.03998
Epoch: 3/20, step: 62920, training_loss: 2.46366
Epoch: 3/20, step: 62940, training_loss: 2.89752
Epoch: 3/20, step: 62960, training_loss: 1.23486
Epoch: 3/20, step: 62980, training_loss: 2.39784
Epoch: 3/20, step: 63000, training_loss: 1.83975
accuracy: 0.35, validation_loss: 2.4643027782440186, num_samples: 100
Epoch: 3/20, step: 63020, training_loss: 2.02565
Epoch: 3/20, step: 63040, training_loss: 1.43879
Epoch: 3/20, step: 63060, training_loss: 1.83168
Epoch: 3/20, step: 63080, training_loss: 2.61297
Epoch: 3/20, step: 63100, training_loss: 2.40353
Epoch: 3/20, step: 63120, training_loss: 2.35650
Epoch: 3/20, step: 63140, training_loss: 2.34443
Epoch: 3/20, step: 63160, training_loss: 1.94532
Epoch: 3/20, step: 63180, training_loss: 2.31119
Epoch: 3/20, step: 63200, training_loss: 2.47269
Epoch: 3/20, step: 63220, training_loss: 1.72614
Epoch: 3/20, step: 63240, training_loss: 1.96252
Epoch: 3/20, step: 63260, training_loss: 1.42607
Epoch: 3/20, step: 63280, training_loss: 2.58706
Epoch: 3/20, step: 63300, training_loss: 2.07203
Epoch: 3/20, step: 63320, training_loss: 2.27875
Epoch: 3/20, step: 63340, training_loss: 1.53894
Epoch: 3/20, step: 63360, training_loss: 2.47383
Epoch: 3/20, step: 63380, training_loss: 1.62949
Epoch: 3/20, step: 63400, training_loss: 2.51626
Epoch: 3/20, step: 63420, training_loss: 2.16466
Epoch: 3/20, step: 63440, training_loss: 2.21332
Epoch: 3/20, step: 63460, training_loss: 2.08837
Epoch: 3/20, step: 63480, training_loss: 1.96332
Epoch: 3/20, step: 63500, training_loss: 2.28027
Epoch: 3/20, step: 63520, training_loss: 2.81870
Epoch: 3/20, step: 63540, training_loss: 2.77836
Epoch: 3/20, step: 63560, training_loss: 1.26595
Epoch: 3/20, step: 63580, training_loss: 1.96085
Epoch: 3/20, step: 63600, training_loss: 2.48532
Epoch: 3/20, step: 63620, training_loss: 1.93832
Epoch: 3/20, step: 63640, training_loss: 2.11105
Epoch: 3/20, step: 63660, training_loss: 1.88261
Epoch: 3/20, step: 63680, training_loss: 1.85236
Epoch: 3/20, step: 63700, training_loss: 2.82774
Epoch: 3/20, step: 63720, training_loss: 1.35362
Epoch: 3/20, step: 63740, training_loss: 1.70623
Epoch: 3/20, step: 63760, training_loss: 2.25367
Epoch: 3/20, step: 63780, training_loss: 2.97598
Epoch: 3/20, step: 63800, training_loss: 2.56799
Epoch: 3/20, step: 63820, training_loss: 2.32853
Epoch: 3/20, step: 63840, training_loss: 1.43991
Epoch: 3/20, step: 63860, training_loss: 2.68648
Epoch: 3/20, step: 63880, training_loss: 1.69258
Epoch: 3/20, step: 63900, training_loss: 1.68499
Epoch: 3/20, step: 63920, training_loss: 1.82523
Epoch: 3/20, step: 63940, training_loss: 1.78108
Epoch: 3/20, step: 63960, training_loss: 1.66591
Epoch: 3/20, step: 63980, training_loss: 2.68055
Epoch: 3/20, step: 64000, training_loss: 1.43967
accuracy: 0.33, validation_loss: 2.3403518199920654, num_samples: 100
Epoch: 3/20, step: 64020, training_loss: 1.79394
Epoch: 3/20, step: 64040, training_loss: 1.80907
Epoch: 3/20, step: 64060, training_loss: 2.01937
Epoch: 3/20, step: 64080, training_loss: 1.99997
Epoch: 3/20, step: 64100, training_loss: 2.37245
Epoch: 3/20, step: 64120, training_loss: 2.20384
Epoch: 3/20, step: 64140, training_loss: 2.42970
Epoch: 3/20, step: 64160, training_loss: 2.23068
Epoch: 3/20, step: 64180, training_loss: 2.83514
Epoch: 3/20, step: 64200, training_loss: 2.26648
Epoch: 3/20, step: 64220, training_loss: 1.42613
Epoch: 3/20, step: 64240, training_loss: 1.88115
Epoch: 3/20, step: 64260, training_loss: 2.43949
Epoch: 3/20, step: 64280, training_loss: 2.14508
Epoch: 3/20, step: 64300, training_loss: 2.10404
Epoch: 3/20, step: 64320, training_loss: 1.27941
Epoch: 3/20, step: 64340, training_loss: 2.82253
Epoch: 3/20, step: 64360, training_loss: 1.70710
Epoch: 3/20, step: 64380, training_loss: 2.21718
Epoch: 3/20, step: 64400, training_loss: 1.78728
Epoch: 3/20, step: 64420, training_loss: 1.62549
Epoch: 3/20, step: 64440, training_loss: 3.09162
Epoch: 3/20, step: 64460, training_loss: 1.98865
Epoch: 3/20, step: 64480, training_loss: 2.78962
Epoch: 3/20, step: 64500, training_loss: 2.14567
Epoch: 3/20, step: 64520, training_loss: 1.59945
Epoch: 3/20, step: 64540, training_loss: 2.09213
Epoch: 3/20, step: 64560, training_loss: 2.25904
Epoch: 3/20, step: 64580, training_loss: 1.16765
Epoch: 3/20, step: 64600, training_loss: 2.00846
Epoch: 3/20, step: 64620, training_loss: 1.91152
Epoch: 3/20, step: 64640, training_loss: 2.44136
Epoch: 3/20, step: 64660, training_loss: 1.99272
Epoch: 3/20, step: 64680, training_loss: 1.94585
Epoch: 3/20, step: 64700, training_loss: 2.16512
Epoch: 3/20, step: 64720, training_loss: 2.77763
Epoch: 3/20, step: 64740, training_loss: 2.37309
Epoch: 3/20, step: 64760, training_loss: 2.49127
Epoch: 3/20, step: 64780, training_loss: 2.35985
Epoch: 3/20, step: 64800, training_loss: 1.62298
Epoch: 3/20, step: 64820, training_loss: 2.02281
Epoch: 3/20, step: 64840, training_loss: 1.80787
Epoch: 3/20, step: 64860, training_loss: 2.20700
Epoch: 3/20, step: 64880, training_loss: 2.15351
Epoch: 3/20, step: 64900, training_loss: 1.88066
Epoch: 3/20, step: 64920, training_loss: 1.65666
Epoch: 3/20, step: 64940, training_loss: 2.70925
Epoch: 3/20, step: 64960, training_loss: 2.14337
Epoch: 3/20, step: 64980, training_loss: 1.85495
Epoch: 3/20, step: 65000, training_loss: 2.13287
accuracy: 0.38, validation_loss: 2.0408225059509277, num_samples: 100
Epoch: 3/20, step: 65020, training_loss: 2.09347
Epoch: 3/20, step: 65040, training_loss: 2.71888
Epoch: 3/20, step: 65060, training_loss: 2.18864
Epoch: 3/20, step: 65080, training_loss: 2.17062
Epoch: 3/20, step: 65100, training_loss: 2.52800
Epoch: 3/20, step: 65120, training_loss: 2.04405
Epoch: 3/20, step: 65140, training_loss: 1.92990
Epoch: 3/20, step: 65160, training_loss: 2.14978
Epoch: 3/20, step: 65180, training_loss: 2.60047
Epoch: 3/20, step: 65200, training_loss: 2.51887
Epoch: 3/20, step: 65220, training_loss: 1.61117
Epoch: 3/20, step: 65240, training_loss: 2.17982
Epoch: 3/20, step: 65260, training_loss: 1.78418
Epoch: 3/20, step: 65280, training_loss: 2.18253
Epoch: 3/20, step: 65300, training_loss: 2.30525
Epoch: 3/20, step: 65320, training_loss: 2.60306
Epoch: 3/20, step: 65340, training_loss: 2.81004
Epoch: 3/20, step: 65360, training_loss: 2.30439
Epoch: 3/20, step: 65380, training_loss: 1.65920
Epoch: 3/20, step: 65400, training_loss: 2.89787
Epoch: 3/20, step: 65420, training_loss: 2.18689
Epoch: 3/20, step: 65440, training_loss: 2.88681
Epoch: 3/20, step: 65460, training_loss: 1.61292
Epoch: 3/20, step: 65480, training_loss: 2.23807
Epoch: 3/20, step: 65500, training_loss: 2.58259
Epoch: 3/20, step: 65520, training_loss: 2.40334
Epoch: 3/20, step: 65540, training_loss: 2.77327
Epoch: 3/20, step: 65560, training_loss: 2.50459
Epoch: 3/20, step: 65580, training_loss: 2.30277
Epoch: 3/20, step: 65600, training_loss: 1.66921
Epoch: 3/20, step: 65620, training_loss: 2.23146
Epoch: 3/20, step: 65640, training_loss: 2.70597
Epoch: 3/20, step: 65660, training_loss: 2.20287
Epoch: 3/20, step: 65680, training_loss: 2.86140
Epoch: 3/20, step: 65700, training_loss: 2.31290
Epoch: 3/20, step: 65720, training_loss: 1.70727
Epoch: 3/20, step: 65740, training_loss: 2.53237
Epoch: 3/20, step: 65760, training_loss: 2.50604
Epoch: 3/20, step: 65780, training_loss: 2.84919
Epoch: 3/20, step: 65800, training_loss: 1.78211
Epoch: 3/20, step: 65820, training_loss: 1.84634
Epoch: 3/20, step: 65840, training_loss: 1.46224
Epoch: 3/20, step: 65860, training_loss: 2.46405
Epoch: 3/20, step: 65880, training_loss: 2.38402
Epoch: 3/20, step: 65900, training_loss: 2.14154
Epoch: 3/20, step: 65920, training_loss: 1.70333
Epoch: 3/20, step: 65940, training_loss: 1.76518
Epoch: 3/20, step: 65960, training_loss: 2.82727
Epoch: 3/20, step: 65980, training_loss: 2.00285
Epoch: 3/20, step: 66000, training_loss: 1.59405
accuracy: 0.41, validation_loss: 2.409377098083496, num_samples: 100
Epoch: 3/20, step: 66020, training_loss: 2.02861
Epoch: 3/20, step: 66040, training_loss: 1.87146
Epoch: 3/20, step: 66060, training_loss: 2.31575
Epoch: 3/20, step: 66080, training_loss: 2.09604
Epoch: 3/20, step: 66100, training_loss: 1.65980
Epoch: 3/20, step: 66120, training_loss: 1.54990
Epoch: 3/20, step: 66140, training_loss: 3.14276
Epoch: 3/20, step: 66160, training_loss: 2.26666
Epoch: 3/20, step: 66180, training_loss: 2.81325
Epoch: 3/20, step: 66200, training_loss: 2.45605
Epoch: 3/20, step: 66220, training_loss: 2.74074
Epoch: 3/20, step: 66240, training_loss: 1.89988
Epoch: 3/20, step: 66260, training_loss: 1.04435
Epoch: 3/20, step: 66280, training_loss: 1.44489
Epoch: 3/20, step: 66300, training_loss: 2.78318
Epoch: 3/20, step: 66320, training_loss: 2.71788
Epoch: 3/20, step: 66340, training_loss: 1.70865
Epoch: 3/20, step: 66360, training_loss: 2.64107
Epoch: 3/20, step: 66380, training_loss: 1.98811
Epoch: 3/20, step: 66400, training_loss: 2.58230
Epoch: 3/20, step: 66420, training_loss: 2.34675
Epoch: 3/20, step: 66440, training_loss: 2.37426
Epoch: 3/20, step: 66460, training_loss: 1.78052
Epoch: 3/20, step: 66480, training_loss: 2.03416
Epoch: 3/20, step: 66500, training_loss: 1.70976
Epoch: 3/20, step: 66520, training_loss: 1.70376
Epoch: 3/20, step: 66540, training_loss: 2.95908
Epoch: 3/20, step: 66560, training_loss: 2.05209
Epoch: 3/20, step: 66580, training_loss: 2.02390
Epoch: 3/20, step: 66600, training_loss: 2.63787
Epoch: 3/20, step: 66620, training_loss: 2.34022
Epoch: 3/20, step: 66640, training_loss: 1.69194
Epoch: 3/20, step: 66660, training_loss: 2.31509
Epoch: 3/20, step: 66680, training_loss: 2.61081
Epoch: 3/20, step: 66700, training_loss: 2.02785
Epoch: 3/20, step: 66720, training_loss: 2.51816
Epoch: 3/20, step: 66740, training_loss: 2.33536
Epoch: 3/20, step: 66760, training_loss: 3.21498
Epoch: 3/20, step: 66780, training_loss: 1.72279
Epoch: 3/20, step: 66800, training_loss: 2.08125
Epoch: 3/20, step: 66820, training_loss: 3.35947
Epoch: 3/20, step: 66840, training_loss: 2.23610
Epoch: 3/20, step: 66860, training_loss: 1.65115
Epoch: 3/20, step: 66880, training_loss: 3.21703
Epoch: 3/20, step: 66900, training_loss: 2.19977
Epoch: 3/20, step: 66920, training_loss: 2.46618
Epoch: 3/20, step: 66940, training_loss: 1.98096
Epoch: 3/20, step: 66960, training_loss: 1.51537
Epoch: 3/20, step: 66980, training_loss: 2.09178
Epoch: 3/20, step: 67000, training_loss: 2.20061
accuracy: 0.47, validation_loss: 2.030784845352173, num_samples: 100
Epoch: 3/20, step: 67020, training_loss: 2.41620
Epoch: 3/20, step: 67040, training_loss: 2.05861
Epoch: 3/20, step: 67060, training_loss: 2.06686
Epoch: 3/20, step: 67080, training_loss: 3.05020
Epoch: 3/20, step: 67100, training_loss: 1.81557
Epoch: 3/20, step: 67120, training_loss: 2.16214
Epoch: 3/20, step: 67140, training_loss: 2.96838
Epoch: 3/20, step: 67160, training_loss: 1.95204
Epoch: 3/20, step: 67180, training_loss: 2.09550
Epoch: 3/20, step: 67200, training_loss: 2.45525
Epoch: 3/20, step: 67220, training_loss: 1.71067
Epoch: 3/20, step: 67240, training_loss: 1.75829
Epoch: 3/20, step: 67260, training_loss: 2.12043
Epoch: 3/20, step: 67280, training_loss: 1.41360
Epoch: 3/20, step: 67300, training_loss: 1.72001
Epoch: 3/20, step: 67320, training_loss: 2.97218
Epoch: 3/20, step: 67340, training_loss: 2.04615
Epoch: 3/20, step: 67360, training_loss: 1.98017
Epoch: 3/20, step: 67380, training_loss: 2.53903
Epoch: 3/20, step: 67400, training_loss: 1.94582
Epoch: 3/20, step: 67420, training_loss: 1.39154
Epoch: 3/20, step: 67440, training_loss: 2.90853
Epoch: 3/20, step: 67460, training_loss: 1.83795
Epoch: 3/20, step: 67480, training_loss: 1.66557
Epoch: 3/20, step: 67500, training_loss: 3.08188
Epoch: 3/20, step: 67520, training_loss: 2.93440
Epoch: 3/20, step: 67540, training_loss: 1.54100
Epoch: 3/20, step: 67560, training_loss: 1.59839
Epoch: 3/20, step: 67580, training_loss: 3.22807
Epoch: 3/20, step: 67600, training_loss: 1.95785
Epoch: 3/20, step: 67620, training_loss: 1.79985
Epoch: 3/20, step: 67640, training_loss: 2.21393
Epoch: 3/20, step: 67660, training_loss: 1.96316
Epoch: 3/20, step: 67680, training_loss: 2.60348
Epoch: 3/20, step: 67700, training_loss: 2.59894
Epoch: 3/20, step: 67720, training_loss: 2.38800
Epoch: 3/20, step: 67740, training_loss: 2.27408
Epoch: 3/20, step: 67760, training_loss: 1.74774
Epoch: 3/20, step: 67780, training_loss: 1.91075
Epoch: 3/20, step: 67800, training_loss: 2.90798
Epoch: 3/20, step: 67820, training_loss: 3.05912
Epoch: 3/20, step: 67840, training_loss: 1.99919
Epoch: 3/20, step: 67860, training_loss: 1.90281
Epoch: 3/20, step: 67880, training_loss: 1.84808
Epoch: 3/20, step: 67900, training_loss: 1.62583
Epoch: 3/20, step: 67920, training_loss: 2.21896
Epoch: 3/20, step: 67940, training_loss: 2.97024
Epoch: 3/20, step: 67960, training_loss: 2.43966
Epoch: 3/20, step: 67980, training_loss: 2.00108
Epoch: 3/20, step: 68000, training_loss: 1.90550
accuracy: 0.37, validation_loss: 2.3879594802856445, num_samples: 100
Epoch: 3/20, step: 68020, training_loss: 1.70495
Epoch: 3/20, step: 68040, training_loss: 1.90445
Epoch: 3/20, step: 68060, training_loss: 2.14275
Epoch: 3/20, step: 68080, training_loss: 2.73294
Epoch: 3/20, step: 68100, training_loss: 2.37357
Epoch: 3/20, step: 68120, training_loss: 1.65269
Epoch: 3/20, step: 68140, training_loss: 1.77163
Epoch: 3/20, step: 68160, training_loss: 1.42847
Epoch: 3/20, step: 68180, training_loss: 1.69689
Epoch: 3/20, step: 68200, training_loss: 2.40797
Epoch: 3/20, step: 68220, training_loss: 2.83367
Epoch: 3/20, step: 68240, training_loss: 1.98440
Epoch: 3/20, step: 68260, training_loss: 2.16181
Epoch: 3/20, step: 68280, training_loss: 2.37552
Epoch: 3/20, step: 68300, training_loss: 1.97950
Epoch: 3/20, step: 68320, training_loss: 2.32769
Epoch: 3/20, step: 68340, training_loss: 2.84561
Epoch: 3/20, step: 68360, training_loss: 2.15497
Epoch: 3/20, step: 68380, training_loss: 2.16123
Epoch: 3/20, step: 68400, training_loss: 1.58248
Epoch: 3/20, step: 68420, training_loss: 1.88650
Epoch: 3/20, step: 68440, training_loss: 2.30195
Epoch: 3/20, step: 68460, training_loss: 2.10224
Epoch: 3/20, step: 68480, training_loss: 2.89572
Epoch: 3/20, step: 68500, training_loss: 2.22529
Epoch: 3/20, step: 68520, training_loss: 2.86091
Epoch: 3/20, step: 68540, training_loss: 2.79232
Epoch: 3/20, step: 68560, training_loss: 1.50932
Epoch: 3/20, step: 68580, training_loss: 2.99740
Epoch: 3/20, step: 68600, training_loss: 1.83912
Epoch: 3/20, step: 68620, training_loss: 1.27515
Epoch: 3/20, step: 68640, training_loss: 1.66703
Epoch: 3/20, step: 68660, training_loss: 1.33731
Epoch: 3/20, step: 68680, training_loss: 1.29351
Epoch: 3/20, step: 68700, training_loss: 1.91055
Epoch: 3/20, step: 68720, training_loss: 1.89361
Epoch: 3/20, step: 68740, training_loss: 0.94266
Epoch: 3/20, step: 68760, training_loss: 1.92502
Epoch: 3/20, step: 68780, training_loss: 2.01090
Epoch: 3/20, step: 68800, training_loss: 1.44665
Epoch: 3/20, step: 68820, training_loss: 1.76773
Epoch: 3/20, step: 68840, training_loss: 1.89932
Epoch: 3/20, step: 68860, training_loss: 1.67732
Epoch: 3/20, step: 68880, training_loss: 2.58161
Epoch: 3/20, step: 68900, training_loss: 1.65402
Epoch: 3/20, step: 68920, training_loss: 1.99360
Epoch: 3/20, step: 68940, training_loss: 2.14458
Epoch: 3/20, step: 68960, training_loss: 1.98747
Epoch: 3/20, step: 68980, training_loss: 2.36230
Epoch: 3/20, step: 69000, training_loss: 1.83861
accuracy: 0.36, validation_loss: 2.164719820022583, num_samples: 100
Epoch: 3/20, step: 69020, training_loss: 1.71002
Epoch: 3/20, step: 69040, training_loss: 3.15465
Epoch: 3/20, step: 69060, training_loss: 1.77830
Epoch: 3/20, step: 69080, training_loss: 2.85014
Epoch: 3/20, step: 69100, training_loss: 1.40642
Epoch: 3/20, step: 69120, training_loss: 2.86208
Epoch: 3/20, step: 69140, training_loss: 2.97887
Epoch: 3/20, step: 69160, training_loss: 1.72650
Epoch: 3/20, step: 69180, training_loss: 2.19164
Epoch: 3/20, step: 69200, training_loss: 1.72521
Epoch: 3/20, step: 69220, training_loss: 2.56065
Epoch: 3/20, step: 69240, training_loss: 1.81333
Epoch: 3/20, step: 69260, training_loss: 2.23956
Epoch: 3/20, step: 69280, training_loss: 2.26065
Epoch: 3/20, step: 69300, training_loss: 2.07357
Epoch: 3/20, step: 69320, training_loss: 1.82782
Epoch: 3/20, step: 69340, training_loss: 1.78451
Epoch: 3/20, step: 69360, training_loss: 2.36473
Epoch: 3/20, step: 69380, training_loss: 2.15417
Epoch: 3/20, step: 69400, training_loss: 2.45995
Epoch: 3/20, step: 69420, training_loss: 2.16484
Epoch: 3/20, step: 69440, training_loss: 2.54505
Epoch: 3/20, step: 69460, training_loss: 1.30254
Epoch: 3/20, step: 69480, training_loss: 1.96132
Epoch: 3/20, step: 69500, training_loss: 2.86770
Epoch: 3/20, step: 69520, training_loss: 2.62640
Epoch: 3/20, step: 69540, training_loss: 1.67920
Epoch: 3/20, step: 69560, training_loss: 1.98600
Epoch: 3/20, step: 69580, training_loss: 1.98800
Epoch: 3/20, step: 69600, training_loss: 1.59661
Epoch: 3/20, step: 69620, training_loss: 2.28023
Epoch: 3/20, step: 69640, training_loss: 1.41916
Epoch: 3/20, step: 69660, training_loss: 2.41275
Epoch: 3/20, step: 69680, training_loss: 2.59323
Epoch: 3/20, step: 69700, training_loss: 1.26411
Epoch: 3/20, step: 69720, training_loss: 1.95012
Epoch: 3/20, step: 69740, training_loss: 1.36931
Epoch: 3/20, step: 69760, training_loss: 2.20278
Epoch: 3/20, step: 69780, training_loss: 2.18756
Epoch: 3/20, step: 69800, training_loss: 2.57564
Epoch: 3/20, step: 69820, training_loss: 2.32441
Epoch: 3/20, step: 69840, training_loss: 3.08425
Epoch: 3/20, step: 69860, training_loss: 1.53655
Epoch: 3/20, step: 69880, training_loss: 2.04332
Epoch: 3/20, step: 69900, training_loss: 1.87116
Epoch: 3/20, step: 69920, training_loss: 2.38580
Epoch: 3/20, step: 69940, training_loss: 1.45731
Epoch: 3/20, step: 69960, training_loss: 1.52643
Epoch: 3/20, step: 69980, training_loss: 1.86467
Epoch: 3/20, step: 70000, training_loss: 2.72174
accuracy: 0.49, validation_loss: 1.8852423429489136, num_samples: 100
Epoch: 3/20, step: 70020, training_loss: 2.64244
Epoch: 3/20, step: 70040, training_loss: 2.27188
Epoch: 3/20, step: 70060, training_loss: 2.46611
Epoch: 3/20, step: 70080, training_loss: 1.98818
Epoch: 3/20, step: 70100, training_loss: 2.66381
Epoch: 3/20, step: 70120, training_loss: 2.28311
Epoch: 3/20, step: 70140, training_loss: 1.62297
Epoch: 3/20, step: 70160, training_loss: 2.19855
Epoch: 3/20, step: 70180, training_loss: 2.54037
Epoch: 3/20, step: 70200, training_loss: 2.09974
Epoch: 3/20, step: 70220, training_loss: 2.19349
Epoch: 3/20, step: 70240, training_loss: 1.68352
Epoch: 3/20, step: 70260, training_loss: 2.20057
Epoch: 3/20, step: 70280, training_loss: 1.77072
Epoch: 3/20, step: 70300, training_loss: 1.77176
Epoch: 3/20, step: 70320, training_loss: 2.35813
Epoch: 3/20, step: 70340, training_loss: 1.98262
Epoch: 3/20, step: 70360, training_loss: 3.00343
Epoch: 3/20, step: 70380, training_loss: 2.73045
Epoch: 3/20, step: 70400, training_loss: 2.50259
Epoch: 3/20, step: 70420, training_loss: 2.74822
Epoch: 3/20, step: 70440, training_loss: 2.95983
Epoch: 3/20, step: 70460, training_loss: 1.44297
Epoch: 3/20, step: 70480, training_loss: 2.34247
Epoch: 3/20, step: 70500, training_loss: 1.02580
Epoch: 3/20, step: 70520, training_loss: 1.59138
Epoch: 3/20, step: 70540, training_loss: 2.40722
Epoch: 3/20, step: 70560, training_loss: 2.08193
Epoch: 3/20, step: 70580, training_loss: 2.68787
Epoch: 3/20, step: 70600, training_loss: 2.99842
Epoch: 3/20, step: 70620, training_loss: 1.80827
Epoch: 3/20, step: 70640, training_loss: 2.85643
Epoch: 3/20, step: 70660, training_loss: 2.54346
Epoch: 3/20, step: 70680, training_loss: 2.11932
Epoch: 3/20, step: 70700, training_loss: 1.80851
Epoch: 3/20, step: 70720, training_loss: 2.53624
Epoch: 3/20, step: 70740, training_loss: 1.98102
Epoch: 3/20, step: 70760, training_loss: 2.47988
Epoch: 3/20, step: 70780, training_loss: 1.68025
Epoch: 3/20, step: 70800, training_loss: 1.74247
Epoch: 3/20, step: 70820, training_loss: 1.80267
Epoch: 3/20, step: 70840, training_loss: 2.56845
Epoch: 3/20, step: 70860, training_loss: 2.30343
Epoch: 3/20, step: 70880, training_loss: 2.25498
Epoch: 3/20, step: 70900, training_loss: 1.37466
Epoch: 3/20, step: 70920, training_loss: 1.83774
Epoch: 3/20, step: 70940, training_loss: 2.34535
Epoch: 3/20, step: 70960, training_loss: 1.80574
Epoch: 3/20, step: 70980, training_loss: 1.71301
Epoch: 3/20, step: 71000, training_loss: 2.22968
accuracy: 0.47, validation_loss: 1.981294870376587, num_samples: 100
Epoch: 3/20, step: 71020, training_loss: 2.42354
Epoch: 3/20, step: 71040, training_loss: 1.61237
Epoch: 3/20, step: 71060, training_loss: 1.89839
Epoch: 3/20, step: 71080, training_loss: 1.82557
Epoch: 3/20, step: 71100, training_loss: 1.94194
Epoch: 3/20, step: 71120, training_loss: 2.15934
Epoch: 3/20, step: 71140, training_loss: 1.67444
Epoch: 3/20, step: 71160, training_loss: 2.30627
Epoch: 3/20, step: 71180, training_loss: 1.83033
Epoch: 3/20, step: 71200, training_loss: 2.27777
Epoch: 3/20, step: 71220, training_loss: 1.80263
Epoch: 3/20, step: 71240, training_loss: 2.03265
Epoch: 3/20, step: 71260, training_loss: 1.98595
Epoch: 3/20, step: 71280, training_loss: 2.40669
Epoch: 3/20, step: 71300, training_loss: 2.29901
Epoch: 3/20, step: 71320, training_loss: 2.65077
Epoch: 3/20, step: 71340, training_loss: 1.77476
Epoch: 3/20, step: 71360, training_loss: 2.01377
Epoch: 3/20, step: 71380, training_loss: 2.21729
Epoch: 3/20, step: 71400, training_loss: 2.11215
Epoch: 3/20, step: 71420, training_loss: 2.13091
Epoch: 3/20, step: 71440, training_loss: 2.25855
Epoch: 3/20, step: 71460, training_loss: 1.68309
Epoch: 3/20, step: 71480, training_loss: 1.90621
Epoch: 3/20, step: 71500, training_loss: 2.30303
Epoch: 3/20, step: 71520, training_loss: 2.20586
Epoch: 3/20, step: 71540, training_loss: 1.53355
Epoch: 3/20, step: 71560, training_loss: 2.46931
Epoch: 3/20, step: 71580, training_loss: 2.23228
Epoch: 3/20, step: 71600, training_loss: 1.87435
Epoch: 3/20, step: 71620, training_loss: 2.01177
Epoch: 3/20, step: 71640, training_loss: 1.62703
Epoch: 3/20, step: 71660, training_loss: 1.92670
Epoch: 3/20, step: 71680, training_loss: 2.60489
Epoch: 3/20, step: 71700, training_loss: 1.64614
Epoch: 3/20, step: 71720, training_loss: 2.55823
Epoch: 3/20, step: 71740, training_loss: 2.31052
Epoch: 3/20, step: 71760, training_loss: 2.34636
Epoch: 3/20, step: 71780, training_loss: 2.24657
Epoch: 3/20, step: 71800, training_loss: 2.27874
Epoch: 3/20, step: 71820, training_loss: 2.84690
Epoch: 3/20, step: 71840, training_loss: 1.94229
Epoch: 3/20, step: 71860, training_loss: 2.18748
Epoch: 3/20, step: 71880, training_loss: 2.22722
Epoch: 3/20, step: 71900, training_loss: 2.21844
Epoch: 3/20, step: 71920, training_loss: 2.99123
Epoch: 3/20, step: 71940, training_loss: 1.60853
Epoch: 3/20, step: 71960, training_loss: 2.90122
Epoch: 3/20, step: 71980, training_loss: 1.53051
Epoch: 3/20, step: 72000, training_loss: 2.41086
accuracy: 0.38, validation_loss: 2.36698579788208, num_samples: 100
Epoch: 3/20, step: 72020, training_loss: 2.34514
Epoch: 3/20, step: 72040, training_loss: 1.82039
Epoch: 3/20, step: 72060, training_loss: 1.24835
Epoch: 3/20, step: 72080, training_loss: 2.18616
Epoch: 3/20, step: 72100, training_loss: 2.60682
Epoch: 3/20, step: 72120, training_loss: 3.05438
Epoch: 3/20, step: 72140, training_loss: 1.51980
Epoch: 3/20, step: 72160, training_loss: 2.21018
Epoch: 3/20, step: 72180, training_loss: 2.68081
Epoch: 3/20, step: 72200, training_loss: 1.83638
Epoch: 3/20, step: 72220, training_loss: 1.49618
Epoch: 3/20, step: 72240, training_loss: 2.06328
Epoch: 3/20, step: 72260, training_loss: 1.92858
Epoch: 3/20, step: 72280, training_loss: 2.43507
Epoch: 3/20, step: 72300, training_loss: 1.86229
Epoch: 3/20, step: 72320, training_loss: 1.89668
Epoch: 3/20, step: 72340, training_loss: 2.24251
Epoch: 3/20, step: 72360, training_loss: 1.90852
Epoch: 3/20, step: 72380, training_loss: 2.56059
Epoch: 3/20, step: 72400, training_loss: 1.71670
Epoch: 3/20, step: 72420, training_loss: 1.93757
Epoch: 3/20, step: 72440, training_loss: 2.15659
Epoch: 3/20, step: 72460, training_loss: 1.87482
Epoch: 3/20, step: 72480, training_loss: 2.23761
Epoch: 3/20, step: 72500, training_loss: 1.57572
Epoch: 3/20, step: 72520, training_loss: 2.50389
Epoch: 3/20, step: 72540, training_loss: 2.07360
Epoch: 3/20, step: 72560, training_loss: 1.51712
Epoch: 3/20, step: 72580, training_loss: 2.15901
Epoch: 3/20, step: 72600, training_loss: 1.59525
Epoch: 3/20, step: 72620, training_loss: 2.35854
Epoch: 3/20, step: 72640, training_loss: 1.56432
Epoch: 3/20, step: 72660, training_loss: 2.28984
Epoch: 3/20, step: 72680, training_loss: 2.29015
Epoch: 3/20, step: 72700, training_loss: 3.35157
Epoch: 3/20, step: 72720, training_loss: 1.83099
Epoch: 3/20, step: 72740, training_loss: 2.19323
Epoch: 3/20, step: 72760, training_loss: 1.93962
Epoch: 3/20, step: 72780, training_loss: 1.90444
Epoch: 3/20, step: 72800, training_loss: 3.30751
Epoch: 3/20, step: 72820, training_loss: 1.85436
Epoch: 3/20, step: 72840, training_loss: 1.98925
Epoch: 3/20, step: 72860, training_loss: 1.90034
Epoch: 3/20, step: 72880, training_loss: 1.91410
Epoch: 3/20, step: 72900, training_loss: 2.28137
Epoch: 3/20, step: 72920, training_loss: 1.86522
Epoch: 3/20, step: 72940, training_loss: 2.46582
Epoch: 3/20, step: 72960, training_loss: 2.21418
Epoch: 3/20, step: 72980, training_loss: 2.00559
Epoch: 3/20, step: 73000, training_loss: 2.74549
accuracy: 0.39, validation_loss: 2.0676705837249756, num_samples: 100
Epoch: 3/20, step: 73020, training_loss: 2.36200
Epoch: 3/20, step: 73040, training_loss: 3.21989
Epoch: 3/20, step: 73060, training_loss: 2.70627
Epoch: 3/20, step: 73080, training_loss: 2.56164
Epoch: 3/20, step: 73100, training_loss: 1.56454
Epoch: 3/20, step: 73120, training_loss: 1.73288
Epoch: 3/20, step: 73140, training_loss: 2.67278
Epoch: 3/20, step: 73160, training_loss: 2.48159
Epoch: 3/20, step: 73180, training_loss: 1.94733
Epoch: 3/20, step: 73200, training_loss: 1.65563
Epoch: 3/20, step: 73220, training_loss: 2.23714
Epoch: 3/20, step: 73240, training_loss: 2.01370
Epoch: 3/20, step: 73260, training_loss: 2.34199
Epoch: 3/20, step: 73280, training_loss: 1.96193
Epoch: 3/20, step: 73300, training_loss: 1.12757
Epoch: 3/20, step: 73320, training_loss: 1.74196
Epoch: 3/20, step: 73340, training_loss: 3.07746
Epoch: 3/20, step: 73360, training_loss: 2.27193
Epoch: 3/20, step: 73380, training_loss: 2.39987
Epoch: 3/20, step: 73400, training_loss: 2.59059
Epoch: 3/20, step: 73420, training_loss: 3.16190
Epoch: 3/20, step: 73440, training_loss: 2.80291
Epoch: 3/20, step: 73460, training_loss: 1.53261
Epoch: 3/20, step: 73480, training_loss: 1.53160
Epoch: 3/20, step: 73500, training_loss: 2.89149
Epoch: 3/20, step: 73520, training_loss: 2.22414
Epoch: 3/20, step: 73540, training_loss: 1.75997
Epoch: 3/20, step: 73560, training_loss: 1.68193
Epoch: 3/20, step: 73580, training_loss: 1.70995
Epoch: 3/20, step: 73600, training_loss: 1.67964
Epoch: 3/20, step: 73620, training_loss: 2.70342
Epoch: 3/20, step: 73640, training_loss: 2.21616
Epoch: 3/20, step: 73660, training_loss: 2.03869
Epoch: 3/20, step: 73680, training_loss: 2.78144
Epoch: 3/20, step: 73700, training_loss: 1.89625
Epoch: 3/20, step: 73720, training_loss: 1.98215
Epoch: 3/20, step: 73740, training_loss: 1.90321
Epoch: 3/20, step: 73760, training_loss: 1.62146
Epoch: 3/20, step: 73780, training_loss: 1.63020
Epoch: 3/20, step: 73800, training_loss: 2.31945
Epoch: 3/20, step: 73820, training_loss: 2.37953
Epoch: 3/20, step: 73840, training_loss: 1.80761
Epoch: 3/20, step: 73860, training_loss: 2.07539
Epoch: 3/20, step: 73880, training_loss: 1.94237
Epoch: 3/20, step: 73900, training_loss: 1.58059
Epoch: 3/20, step: 73920, training_loss: 1.66162
Epoch: 3/20, step: 73940, training_loss: 1.93641
Epoch: 3/20, step: 73960, training_loss: 1.86002
Epoch: 3/20, step: 73980, training_loss: 3.15035
Epoch: 3/20, step: 74000, training_loss: 1.90513
accuracy: 0.44, validation_loss: 2.125685691833496, num_samples: 100
Epoch: 3/20, step: 74020, training_loss: 2.32036
Epoch: 3/20, step: 74040, training_loss: 1.83269
Epoch: 3/20, step: 74060, training_loss: 2.59507
Epoch: 3/20, step: 74080, training_loss: 2.39562
Epoch: 3/20, step: 74100, training_loss: 2.23749
Epoch: 3/20, step: 74120, training_loss: 2.66682
Epoch: 3/20, step: 74140, training_loss: 2.00694
Epoch: 3/20, step: 74160, training_loss: 2.06884
Epoch: 3/20, step: 74180, training_loss: 2.33943
Epoch: 3/20, step: 74200, training_loss: 1.58090
Epoch: 3/20, step: 74220, training_loss: 2.03499
Epoch: 3/20, step: 74240, training_loss: 2.41885
Epoch: 3/20, step: 74260, training_loss: 2.00315
Epoch: 3/20, step: 74280, training_loss: 1.57328
Epoch: 3/20, step: 74300, training_loss: 2.09350
Epoch: 3/20, step: 74320, training_loss: 2.36394
Epoch: 3/20, step: 74340, training_loss: 2.49077
Epoch: 3/20, step: 74360, training_loss: 2.85391
Epoch: 3/20, step: 74380, training_loss: 2.13728
Epoch: 3/20, step: 74400, training_loss: 2.01636
Epoch: 3/20, step: 74420, training_loss: 2.22931
Epoch: 3/20, step: 74440, training_loss: 2.21687
Epoch: 3/20, step: 74460, training_loss: 2.39085
Epoch: 3/20, step: 74480, training_loss: 2.09012
Epoch: 3/20, step: 74500, training_loss: 2.05066
Epoch: 3/20, step: 74520, training_loss: 1.48480
Epoch: 3/20, step: 74540, training_loss: 1.45714
Epoch: 3/20, step: 74560, training_loss: 1.89021
Epoch: 3/20, step: 74580, training_loss: 2.26336
Epoch: 3/20, step: 74600, training_loss: 1.64510
Epoch: 3/20, step: 74620, training_loss: 1.65986
Epoch: 3/20, step: 74640, training_loss: 1.89095
Epoch: 3/20, step: 74660, training_loss: 1.94987
Epoch: 3/20, step: 74680, training_loss: 2.09316
Epoch: 3/20, step: 74700, training_loss: 2.01403
Epoch: 3/20, step: 74720, training_loss: 1.99620
Epoch: 3/20, step: 74740, training_loss: 2.03275
Epoch: 3/20, step: 74760, training_loss: 2.28121
Epoch: 3/20, step: 74780, training_loss: 2.56107
Epoch: 3/20, step: 74800, training_loss: 1.27086
Epoch: 3/20, step: 74820, training_loss: 2.18986
Epoch: 3/20, step: 74840, training_loss: 2.02311
Epoch: 3/20, step: 74860, training_loss: 1.97946
Epoch: 3/20, step: 74880, training_loss: 1.82134
Epoch: 3/20, step: 74900, training_loss: 2.30144
Epoch: 3/20, step: 74920, training_loss: 2.16898
Epoch: 3/20, step: 74940, training_loss: 2.42355
Epoch: 3/20, step: 74960, training_loss: 2.26206
Epoch: 3/20, step: 74980, training_loss: 1.96462
Epoch: 3/20, step: 75000, training_loss: 1.57657
accuracy: 0.43, validation_loss: 1.9444618225097656, num_samples: 100
Epoch: 3/20, step: 75020, training_loss: 2.34719
Epoch: 3/20, step: 75040, training_loss: 1.15975
Epoch: 3/20, step: 75060, training_loss: 2.13399
Epoch: 3/20, step: 75080, training_loss: 2.96399
Epoch: 3/20, step: 75100, training_loss: 1.63317
Epoch: 3/20, step: 75120, training_loss: 2.44199
Epoch: 3/20, step: 75140, training_loss: 2.32885
Epoch: 3/20, step: 75160, training_loss: 1.80184
Epoch: 3/20, step: 75180, training_loss: 1.88615
Epoch: 3/20, step: 75200, training_loss: 2.19898
Epoch: 3/20, step: 75220, training_loss: 2.54323
Epoch: 3/20, step: 75240, training_loss: 1.93431
Epoch: 3/20, step: 75260, training_loss: 2.74230
Epoch: 3/20, step: 75280, training_loss: 1.88183
Epoch: 3/20, step: 75300, training_loss: 2.53559
Epoch: 3/20, step: 75320, training_loss: 2.13982
Epoch: 3/20, step: 75340, training_loss: 3.02519
Epoch: 3/20, step: 75360, training_loss: 1.78129
Epoch: 3/20, step: 75380, training_loss: 2.07252
Epoch: 3/20, step: 75400, training_loss: 1.91878
Epoch: 3/20, step: 75420, training_loss: 1.98305
Epoch: 3/20, step: 75440, training_loss: 1.94956
Epoch: 3/20, step: 75460, training_loss: 2.74937
Epoch: 3/20, step: 75480, training_loss: 2.57746
Epoch: 3/20, step: 75500, training_loss: 2.46895
Epoch: 3/20, step: 75520, training_loss: 2.51069
Epoch: 3/20, step: 75540, training_loss: 2.36774
Epoch: 3/20, step: 75560, training_loss: 1.88946
Epoch: 3/20, step: 75580, training_loss: 2.54315
Epoch: 3/20, step: 75600, training_loss: 2.15259
Epoch: 3/20, step: 75620, training_loss: 3.07640
Epoch: 3/20, step: 75640, training_loss: 2.38542
Epoch: 3/20, step: 75660, training_loss: 2.45204
Epoch: 3/20, step: 75680, training_loss: 1.37695
Epoch: 3/20, step: 75700, training_loss: 2.50188
Epoch: 3/20, step: 75720, training_loss: 2.26796
Epoch: 3/20, step: 75740, training_loss: 2.18848
Epoch: 3/20, step: 75760, training_loss: 2.13962
Epoch: 3/20, step: 75780, training_loss: 1.76177
Epoch: 3/20, step: 75800, training_loss: 1.91240
Epoch: 3/20, step: 75820, training_loss: 2.64548
Epoch: 3/20, step: 75840, training_loss: 2.29514
Epoch: 3/20, step: 75860, training_loss: 1.36903
Epoch: 3/20, step: 75880, training_loss: 1.77327
Epoch: 3/20, step: 75900, training_loss: 2.09182
Epoch: 3/20, step: 75920, training_loss: 2.16772
Epoch: 3/20, step: 75940, training_loss: 2.64114
Epoch: 3/20, step: 75960, training_loss: 2.02847
Epoch: 3/20, step: 75980, training_loss: 1.47914
Epoch: 3/20, step: 76000, training_loss: 2.19935
accuracy: 0.44, validation_loss: 2.2187814712524414, num_samples: 100
Epoch: 3/20, step: 76020, training_loss: 1.54592
Epoch: 3/20, step: 76040, training_loss: 2.33455
Epoch: 3/20, step: 76060, training_loss: 2.33147
Epoch: 3/20, step: 76080, training_loss: 2.24918
Epoch: 3/20, step: 76100, training_loss: 2.55119
Epoch: 3/20, step: 76120, training_loss: 2.11244
Epoch: 3/20, step: 76140, training_loss: 2.28878
Epoch: 3/20, step: 76160, training_loss: 2.92728
Epoch: 3/20, step: 76180, training_loss: 3.21944
Epoch: 3/20, step: 76200, training_loss: 2.01300
Epoch: 3/20, step: 76220, training_loss: 1.95666
Epoch: 3/20, step: 76240, training_loss: 2.84506
Epoch: 3/20, step: 76260, training_loss: 2.67137
Epoch: 3/20, step: 76280, training_loss: 1.45155
Epoch: 3/20, step: 76300, training_loss: 2.36392
Epoch: 3/20, step: 76320, training_loss: 1.64013
Epoch: 3/20, step: 76340, training_loss: 2.11346
Epoch: 3/20, step: 76360, training_loss: 1.91576
Epoch: 3/20, step: 76380, training_loss: 2.53224
Epoch: 3/20, step: 76400, training_loss: 1.57069
Epoch: 3/20, step: 76420, training_loss: 2.22207
Epoch: 3/20, step: 76440, training_loss: 1.72777
Epoch: 3/20, step: 76460, training_loss: 2.06454
Epoch: 3/20, step: 76480, training_loss: 1.84301
Epoch: 3/20, step: 76500, training_loss: 2.40572
Epoch: 3/20, step: 76520, training_loss: 2.16007
Epoch: 3/20, step: 76540, training_loss: 1.13328
Epoch: 3/20, step: 76560, training_loss: 2.57261
Epoch: 3/20, step: 76580, training_loss: 2.59644
Epoch: 3/20, step: 76600, training_loss: 1.89368
Epoch: 3/20, step: 76620, training_loss: 2.00038
Epoch: 3/20, step: 76640, training_loss: 1.86032
Epoch: 3/20, step: 76660, training_loss: 2.55997
Epoch: 3/20, step: 76680, training_loss: 1.90466
Epoch: 3/20, step: 76700, training_loss: 2.14137
Epoch: 3/20, step: 76720, training_loss: 1.77758
Epoch: 3/20, step: 76740, training_loss: 1.55163
Epoch: 3/20, step: 76760, training_loss: 2.18459
Epoch: 3/20, step: 76780, training_loss: 1.51298
Epoch: 3/20, step: 76800, training_loss: 3.05151
Epoch: 3/20, step: 76820, training_loss: 2.42161
Epoch: 3/20, step: 76840, training_loss: 1.78670
Epoch: 3/20, step: 76860, training_loss: 2.55999
Epoch: 3/20, step: 76880, training_loss: 1.71523
Epoch: 3/20, step: 76900, training_loss: 2.29229
Epoch: 3/20, step: 76920, training_loss: 2.29508
Epoch: 3/20, step: 76940, training_loss: 2.14093
Epoch: 3/20, step: 76960, training_loss: 1.45538
Epoch: 3/20, step: 76980, training_loss: 2.13052
Epoch: 3/20, step: 77000, training_loss: 1.86385
accuracy: 0.52, validation_loss: 1.822933316230774, num_samples: 100
Epoch: 3/20, step: 77020, training_loss: 1.49492
Epoch: 3/20, step: 77040, training_loss: 2.94077
Epoch: 3/20, step: 77060, training_loss: 2.21332
Epoch: 3/20, step: 77080, training_loss: 1.59064
Epoch: 3/20, step: 77100, training_loss: 1.74366
Epoch: 3/20, step: 77120, training_loss: 2.35553
Epoch: 3/20, step: 77140, training_loss: 2.68688
Epoch: 3/20, step: 77160, training_loss: 2.52791
Epoch: 3/20, step: 77180, training_loss: 2.61891
Epoch: 3/20, step: 77200, training_loss: 2.24472
Epoch: 3/20, step: 77220, training_loss: 2.54960
Epoch: 3/20, step: 77240, training_loss: 1.65405
Epoch: 3/20, step: 77260, training_loss: 2.49740
Epoch: 3/20, step: 77280, training_loss: 1.40248
Epoch: 3/20, step: 77300, training_loss: 1.28909
Epoch: 3/20, step: 77320, training_loss: 1.79679
Epoch: 3/20, step: 77340, training_loss: 2.25405
Epoch: 3/20, step: 77360, training_loss: 3.23250
Epoch: 3/20, step: 77380, training_loss: 2.25075
Epoch: 3/20, step: 77400, training_loss: 1.52883
Epoch: 3/20, step: 77420, training_loss: 2.36139
Epoch: 3/20, step: 77440, training_loss: 1.31402
Epoch: 3/20, step: 77460, training_loss: 1.58845
Epoch: 3/20, step: 77480, training_loss: 1.96124
Epoch: 3/20, step: 77500, training_loss: 2.13321
Epoch: 3/20, step: 77520, training_loss: 1.25962
Epoch: 3/20, step: 77540, training_loss: 2.46839
Epoch: 3/20, step: 77560, training_loss: 1.83767
Epoch: 3/20, step: 77580, training_loss: 2.64066
Epoch: 3/20, step: 77600, training_loss: 2.02087
Epoch: 3/20, step: 77620, training_loss: 1.92817
Epoch: 3/20, step: 77640, training_loss: 2.13159
Epoch: 3/20, step: 77660, training_loss: 2.95220
Epoch: 3/20, step: 77680, training_loss: 1.15556
Epoch: 3/20, step: 77700, training_loss: 1.85580
Epoch: 3/20, step: 77720, training_loss: 1.71175
Epoch: 3/20, step: 77740, training_loss: 2.78963
Epoch: 3/20, step: 77760, training_loss: 2.09370
Epoch: 3/20, step: 77780, training_loss: 2.54645
Epoch: 3/20, step: 77800, training_loss: 2.03022
Epoch: 3/20, step: 77820, training_loss: 1.50554
Epoch: 3/20, step: 77840, training_loss: 1.42708
Epoch: 3/20, step: 77860, training_loss: 2.29510
Epoch: 3/20, step: 77880, training_loss: 2.20311
Epoch: 3/20, step: 77900, training_loss: 1.96357
Epoch: 3/20, step: 77920, training_loss: 1.89930
Epoch: 3/20, step: 77940, training_loss: 2.46941
Epoch: 3/20, step: 77960, training_loss: 1.87284
Epoch: 3/20, step: 77980, training_loss: 2.40403
Epoch: 3/20, step: 78000, training_loss: 2.31211
accuracy: 0.47, validation_loss: 1.9981476068496704, num_samples: 100
Epoch: 3/20, step: 78020, training_loss: 2.55129
Epoch: 3/20, step: 78040, training_loss: 1.99885
Epoch: 3/20, step: 78060, training_loss: 1.69323
Epoch: 3/20, step: 78080, training_loss: 3.06019
Epoch: 3/20, step: 78100, training_loss: 2.40400
Epoch: 3/20, step: 78120, training_loss: 3.09780
Epoch: 3/20, step: 78140, training_loss: 3.04092
Epoch: 3/20, step: 78160, training_loss: 1.56526
Epoch: 3/20, step: 78180, training_loss: 2.26344
Epoch: 3/20, step: 78200, training_loss: 2.99264
Epoch: 3/20, step: 78220, training_loss: 1.71364
Epoch: 3/20, step: 78240, training_loss: 1.43131
Epoch: 3/20, step: 78260, training_loss: 1.60568
Epoch: 3/20, step: 78280, training_loss: 2.49861
Epoch: 3/20, step: 78300, training_loss: 3.18479
Epoch: 3/20, step: 78320, training_loss: 2.04162
Epoch: 3/20, step: 78340, training_loss: 2.93939
Epoch: 3/20, step: 78360, training_loss: 2.15933
Epoch: 3/20, step: 78380, training_loss: 2.56951
Epoch: 3/20, step: 78400, training_loss: 2.17400
Epoch: 3/20, step: 78420, training_loss: 2.03685
Epoch: 3/20, step: 78440, training_loss: 1.98107
Epoch: 3/20, step: 78460, training_loss: 1.98022
Epoch: 3/20, step: 78480, training_loss: 2.05222
Epoch: 3/20, step: 78500, training_loss: 1.85170
Epoch: 3/20, step: 78520, training_loss: 2.08147
Epoch: 3/20, step: 78540, training_loss: 1.98390
Epoch: 3/20, step: 78560, training_loss: 1.90370
Epoch: 3/20, step: 78580, training_loss: 2.36149
Epoch: 3/20, step: 78600, training_loss: 3.10630
Epoch: 3/20, step: 78620, training_loss: 2.42695
Epoch: 3/20, step: 78640, training_loss: 0.87814
Epoch: 3/20, step: 78660, training_loss: 1.97528
Epoch: 3/20, step: 78680, training_loss: 2.59950
Epoch: 3/20, step: 78700, training_loss: 2.47608
Epoch: 3/20, step: 78720, training_loss: 2.13614
Epoch: 3/20, step: 78740, training_loss: 2.43317
Epoch: 3/20, step: 78760, training_loss: 1.90196
Epoch: 3/20, step: 78780, training_loss: 2.43704
Epoch: 3/20, step: 78800, training_loss: 2.39786
Epoch: 3/20, step: 78820, training_loss: 1.60997
Epoch: 3/20, step: 78840, training_loss: 2.10744
Epoch: 3/20, step: 78860, training_loss: 2.06410
Epoch: 3/20, step: 78880, training_loss: 2.39592
Epoch: 3/20, step: 78900, training_loss: 2.78985
Epoch: 3/20, step: 78920, training_loss: 1.81839
Epoch: 3/20, step: 78940, training_loss: 1.58995
Epoch: 3/20, step: 78960, training_loss: 1.81544
Epoch: 3/20, step: 78980, training_loss: 2.35590
Epoch: 3/20, step: 79000, training_loss: 2.28363
accuracy: 0.37, validation_loss: 2.327831506729126, num_samples: 100
Epoch: 3/20, step: 79020, training_loss: 2.01703
Epoch: 3/20, step: 79040, training_loss: 2.37838
Epoch: 3/20, step: 79060, training_loss: 2.57126
Epoch: 3/20, step: 79080, training_loss: 1.42780
Epoch: 3/20, step: 79100, training_loss: 2.90509
Epoch: 3/20, step: 79120, training_loss: 1.64188
Epoch: 3/20, step: 79140, training_loss: 1.67586
Epoch: 3/20, step: 79160, training_loss: 3.41521
Epoch: 3/20, step: 79180, training_loss: 2.62447
Epoch: 3/20, step: 79200, training_loss: 2.31720
Epoch: 3/20, step: 79220, training_loss: 2.20533
Epoch: 3/20, step: 79240, training_loss: 2.52487
Epoch: 3/20, step: 79260, training_loss: 1.91997
Epoch: 3/20, step: 79280, training_loss: 1.44404
Epoch: 3/20, step: 79300, training_loss: 2.46830
Epoch: 3/20, step: 79320, training_loss: 2.68358
Epoch: 3/20, step: 79340, training_loss: 1.83808
Epoch: 3/20, step: 79360, training_loss: 2.26886
Epoch: 3/20, step: 79380, training_loss: 2.17083
Epoch: 3/20, step: 79400, training_loss: 2.41436
Epoch: 3/20, step: 79420, training_loss: 1.40944
Epoch: 3/20, step: 79440, training_loss: 2.64589
Epoch: 3/20, step: 79460, training_loss: 1.64889
Epoch: 3/20, step: 79480, training_loss: 2.03633
Epoch: 3/20, step: 79500, training_loss: 1.88206
Epoch: 3/20, step: 79520, training_loss: 2.13760
Epoch: 3/20, step: 79540, training_loss: 2.52069
Epoch: 3/20, step: 79560, training_loss: 2.24853
Epoch: 3/20, step: 79580, training_loss: 2.90549
Epoch: 3/20, step: 79600, training_loss: 1.88752
Epoch: 3/20, step: 79620, training_loss: 2.06731
Epoch: 3/20, step: 79640, training_loss: 1.64252
Epoch: 3/20, step: 79660, training_loss: 1.95275
Epoch: 3/20, step: 79680, training_loss: 2.22257
Epoch: 3/20, step: 79700, training_loss: 1.61811
Epoch: 3/20, step: 79720, training_loss: 2.74874
Epoch: 3/20, step: 79740, training_loss: 2.05233
Epoch: 3/20, step: 79760, training_loss: 2.61299
Epoch: 3/20, step: 79780, training_loss: 2.02048
Epoch: 3/20, step: 79800, training_loss: 2.65892
Epoch: 3/20, step: 79820, training_loss: 2.54353
Epoch: 3/20, step: 79840, training_loss: 2.46318
Epoch: 3/20, step: 79860, training_loss: 2.21205
Epoch: 3/20, step: 79880, training_loss: 1.35707
Epoch: 3/20, step: 79900, training_loss: 1.88362
Epoch: 3/20, step: 79920, training_loss: 1.84956
Epoch: 3/20, step: 79940, training_loss: 2.08299
Epoch: 3/20, step: 79960, training_loss: 2.14838
Epoch: 3/20, step: 79980, training_loss: 2.85745
Epoch: 3/20, step: 80000, training_loss: 1.68229
accuracy: 0.46, validation_loss: 1.956915259361267, num_samples: 100
Epoch: 3/20, step: 80020, training_loss: 2.02870
Epoch: 3/20, step: 80040, training_loss: 2.34302
Epoch: 3/20, step: 80060, training_loss: 1.36229
Epoch: 3/20, step: 80080, training_loss: 2.71201
Epoch: 3/20, step: 80100, training_loss: 2.23845
Epoch: 3/20, step: 80120, training_loss: 2.08009
Epoch: 3/20, step: 80140, training_loss: 2.21350
Epoch: 3/20, step: 80160, training_loss: 1.61016
Epoch: 3/20, step: 80180, training_loss: 2.77125
Epoch: 3/20, step: 80200, training_loss: 2.17770
Epoch: 3/20, step: 80220, training_loss: 1.55605
Epoch: 3/20, step: 80240, training_loss: 1.48507
Epoch: 3/20, step: 80260, training_loss: 1.70905
Epoch: 3/20, step: 80280, training_loss: 2.52465
Epoch: 3/20, step: 80300, training_loss: 1.73704
Epoch: 3/20, step: 80320, training_loss: 2.40495
Epoch: 3/20, step: 80340, training_loss: 2.09781
Epoch: 3/20, step: 80360, training_loss: 2.36144
Epoch: 3/20, step: 80380, training_loss: 2.28688
Epoch: 3/20, step: 80400, training_loss: 2.28469
Epoch: 3/20, step: 80420, training_loss: 2.97617
Epoch: 3/20, step: 80440, training_loss: 1.72605
Epoch: 3/20, step: 80460, training_loss: 2.24934
Epoch: 3/20, step: 80480, training_loss: 2.17829
Epoch: 3/20, step: 80500, training_loss: 2.31144
Epoch: 3/20, step: 80520, training_loss: 2.55698
Epoch: 3/20, step: 80540, training_loss: 2.18098
Epoch: 3/20, step: 80560, training_loss: 2.01167
Epoch: 3/20, step: 80580, training_loss: 2.41402
Epoch: 3/20, step: 80600, training_loss: 1.62536
Epoch: 3/20, step: 80620, training_loss: 1.86383
Epoch: 3/20, step: 80640, training_loss: 2.22188
Epoch: 3/20, step: 80660, training_loss: 2.73978
Epoch: 3/20, step: 80680, training_loss: 2.20170
Epoch: 3/20, step: 80700, training_loss: 1.84293
Epoch: 3/20, step: 80720, training_loss: 2.15494
Epoch: 3/20, step: 80740, training_loss: 2.19920
Epoch: 3/20, step: 80760, training_loss: 2.16496
Epoch: 3/20, step: 80780, training_loss: 2.00369
Epoch: 3/20, step: 80800, training_loss: 2.06073
Epoch: 3/20, step: 80820, training_loss: 2.14915
Epoch: 3/20, step: 80840, training_loss: 2.03518
Epoch: 3/20, step: 80860, training_loss: 2.20369
Epoch: 3/20, step: 80880, training_loss: 2.86582
Epoch: 3/20, step: 80900, training_loss: 2.96147
Epoch: 3/20, step: 80920, training_loss: 2.27568
Epoch: 3/20, step: 80940, training_loss: 3.01222
Epoch: 3/20, step: 80960, training_loss: 2.29608
Epoch: 3/20, step: 80980, training_loss: 1.74597
Epoch: 3/20, step: 81000, training_loss: 1.94002
accuracy: 0.42, validation_loss: 2.150817632675171, num_samples: 100
Epoch: 3/20, step: 81020, training_loss: 1.90231
Epoch: 3/20, step: 81040, training_loss: 2.56586
Epoch: 3/20, step: 81060, training_loss: 1.43951
Epoch: 3/20, step: 81080, training_loss: 2.46710
Epoch: 3/20, step: 81100, training_loss: 2.20641
Epoch: 3/20, step: 81120, training_loss: 1.40212
Epoch: 3/20, step: 81140, training_loss: 2.09293
Epoch: 3/20, step: 81160, training_loss: 2.03816
Epoch: 3/20, step: 81180, training_loss: 2.95144
Epoch: 3/20, step: 81200, training_loss: 1.87883
Epoch: 3/20, step: 81220, training_loss: 1.78635
Epoch: 3/20, step: 81240, training_loss: 1.79957
Epoch: 3/20, step: 81260, training_loss: 2.67704
Epoch: 3/20, step: 81280, training_loss: 2.41699
Epoch: 3/20, step: 81300, training_loss: 1.83801
Epoch: 3/20, step: 81320, training_loss: 2.30354
Epoch: 3/20, step: 81340, training_loss: 1.48578
Epoch: 3/20, step: 81360, training_loss: 1.89892
Epoch: 3/20, step: 81380, training_loss: 1.93956
Epoch: 3/20, step: 81400, training_loss: 0.85086
Epoch: 3/20, step: 81420, training_loss: 2.79368
Epoch: 3/20, step: 81440, training_loss: 1.93112
Epoch: 3/20, step: 81460, training_loss: 3.18222
Epoch: 3/20, step: 81480, training_loss: 1.73107
Epoch: 3/20, step: 81500, training_loss: 1.64248
Epoch: 3/20, step: 81520, training_loss: 1.47796
Epoch: 3/20, step: 81540, training_loss: 2.07819
Epoch: 3/20, step: 81560, training_loss: 2.27254
Epoch: 3/20, step: 81580, training_loss: 1.62639
Epoch: 3/20, step: 81600, training_loss: 1.85880
Epoch: 3/20, step: 81620, training_loss: 1.46947
Epoch: 3/20, step: 81640, training_loss: 1.98466
Epoch: 3/20, step: 81660, training_loss: 2.58856
Epoch: 3/20, step: 81680, training_loss: 2.06228
Epoch: 3/20, step: 81700, training_loss: 2.14572
Epoch: 3/20, step: 81720, training_loss: 1.83037
Epoch: 3/20, step: 81740, training_loss: 1.25773
Epoch: 3/20, step: 81760, training_loss: 1.70644
Epoch: 3/20, step: 81780, training_loss: 1.36727
Epoch: 3/20, step: 81800, training_loss: 2.43633
Epoch: 3/20, step: 81820, training_loss: 2.11143
Epoch: 3/20, step: 81840, training_loss: 2.02533
Epoch: 3/20, step: 81860, training_loss: 1.38232
Epoch: 3/20, step: 81880, training_loss: 2.22035
Epoch: 3/20, step: 81900, training_loss: 2.09743
Epoch: 3/20, step: 81920, training_loss: 2.07503
Epoch: 3/20, step: 81940, training_loss: 2.31583
Epoch: 3/20, step: 81960, training_loss: 1.92703
Epoch: 3/20, step: 81980, training_loss: 2.00506
Epoch: 3/20, step: 82000, training_loss: 1.82245
accuracy: 0.42, validation_loss: 2.112092971801758, num_samples: 100
Epoch: 3/20, step: 82020, training_loss: 2.35209
Epoch: 3/20, step: 82040, training_loss: 2.61922
Epoch: 3/20, step: 82060, training_loss: 2.03451
Epoch: 3/20, step: 82080, training_loss: 1.49830
Epoch: 3/20, step: 82100, training_loss: 1.51479
Epoch: 3/20, step: 82120, training_loss: 1.56032
Epoch: 3/20, step: 82140, training_loss: 2.28552
Epoch: 3/20, step: 82160, training_loss: 1.78068
Epoch: 3/20, step: 82180, training_loss: 1.65736
Epoch: 3/20, step: 82200, training_loss: 2.26542
Epoch: 3/20, step: 82220, training_loss: 2.96427
Epoch: 3/20, step: 82240, training_loss: 2.29420
Epoch: 3/20, step: 82260, training_loss: 3.22094
Epoch: 3/20, step: 82280, training_loss: 2.24467
Epoch: 3/20, step: 82300, training_loss: 2.82195
Epoch: 3/20, step: 82320, training_loss: 2.69810
Epoch: 3/20, step: 82340, training_loss: 2.06945
Epoch: 3/20, step: 82360, training_loss: 2.21581
Epoch: 3/20, step: 82380, training_loss: 1.25951
Epoch: 3/20, step: 82400, training_loss: 2.30498
Epoch: 3/20, step: 82420, training_loss: 1.84646
Epoch: 3/20, step: 82440, training_loss: 2.57829
Epoch: 3/20, step: 82460, training_loss: 1.99956
Epoch: 3/20, step: 82480, training_loss: 2.62397
Epoch: 3/20, step: 82500, training_loss: 2.58845
Epoch: 3/20, step: 82520, training_loss: 2.42325
Epoch: 3/20, step: 82540, training_loss: 2.43050
Epoch: 3/20, step: 82560, training_loss: 2.64796
Epoch: 3/20, step: 82580, training_loss: 1.89394
Epoch: 3/20, step: 82600, training_loss: 2.44491
Epoch: 3/20, step: 82620, training_loss: 1.83063
Epoch: 3/20, step: 82640, training_loss: 2.59661
Epoch: 3/20, step: 82660, training_loss: 1.61668
Epoch: 3/20, step: 82680, training_loss: 2.05701
Epoch: 3/20, step: 82700, training_loss: 2.41579
Epoch: 3/20, step: 82720, training_loss: 1.13621
Epoch: 3/20, step: 82740, training_loss: 2.23128
Epoch: 3/20, step: 82760, training_loss: 2.51031
Epoch: 3/20, step: 82780, training_loss: 1.83827
Epoch: 3/20, step: 82800, training_loss: 2.66849
Epoch: 3/20, step: 82820, training_loss: 2.18290
Epoch: 3/20, step: 82840, training_loss: 1.42110
Epoch: 3/20, step: 82860, training_loss: 2.04130
Epoch: 3/20, step: 82880, training_loss: 2.12219
Epoch: 3/20, step: 82900, training_loss: 2.14463
Epoch: 3/20, step: 82920, training_loss: 1.61990
Epoch: 3/20, step: 82940, training_loss: 2.18787
Epoch: 3/20, step: 82960, training_loss: 1.59755
Epoch: 3/20, step: 82980, training_loss: 2.40710
Epoch: 3/20, step: 83000, training_loss: 2.57827
accuracy: 0.45, validation_loss: 1.9423097372055054, num_samples: 100
Epoch: 3/20, step: 83020, training_loss: 2.06780
Epoch: 3/20, step: 83040, training_loss: 1.83584
Epoch: 3/20, step: 83060, training_loss: 2.13038
Epoch: 3/20, step: 83080, training_loss: 2.77973
Epoch: 3/20, step: 83100, training_loss: 1.97285
Epoch: 3/20, step: 83120, training_loss: 2.91655
Epoch: 3/20, step: 83140, training_loss: 2.24384
Epoch: 3/20, step: 83160, training_loss: 1.91818
Epoch: 3/20, step: 83180, training_loss: 2.00069
Epoch: 3/20, step: 83200, training_loss: 1.49285
Epoch: 3/20, step: 83220, training_loss: 2.01094
Epoch: 3/20, step: 83240, training_loss: 1.83249
Epoch: 3/20, step: 83260, training_loss: 1.91011
Epoch: 3/20, step: 83280, training_loss: 2.47738
Epoch: 3/20, step: 83300, training_loss: 1.95305
Epoch: 3/20, step: 83320, training_loss: 2.16967
Epoch: 3/20, step: 83340, training_loss: 2.30560
Epoch: 3/20, step: 83360, training_loss: 1.45361
Epoch: 3/20, step: 83380, training_loss: 2.36122
Epoch: 3/20, step: 83400, training_loss: 2.35260
Epoch: 3/20, step: 83420, training_loss: 3.06923
Epoch: 3/20, step: 83440, training_loss: 3.03095
Epoch: 3/20, step: 83460, training_loss: 2.06647
Epoch: 3/20, step: 83480, training_loss: 1.17261
Epoch: 3/20, step: 83500, training_loss: 2.48698
Epoch: 3/20, step: 83520, training_loss: 3.14445
Epoch: 3/20, step: 83540, training_loss: 1.90351
Epoch: 3/20, step: 83560, training_loss: 1.97981
Epoch: 3/20, step: 83580, training_loss: 2.57890
Epoch: 3/20, step: 83600, training_loss: 2.33056
Epoch: 3/20, step: 83620, training_loss: 1.86829
Epoch: 3/20, step: 83640, training_loss: 1.99223
Epoch: 3/20, step: 83660, training_loss: 2.85855
Epoch: 3/20, step: 83680, training_loss: 1.96925
Epoch: 3/20, step: 83700, training_loss: 1.26618
Epoch: 3/20, step: 83720, training_loss: 2.05832
Epoch: 3/20, step: 83740, training_loss: 1.20595
Epoch: 3/20, step: 83760, training_loss: 2.01563
Epoch: 3/20, step: 83780, training_loss: 2.03113
Epoch: 3/20, step: 83800, training_loss: 2.22375
Epoch: 3/20, step: 83820, training_loss: 1.91862
Epoch: 3/20, step: 83840, training_loss: 1.75947
Epoch: 3/20, step: 83860, training_loss: 1.93089
Epoch: 3/20, step: 83880, training_loss: 1.97044
Epoch: 3/20, step: 83900, training_loss: 1.35468
Epoch: 3/20, step: 83920, training_loss: 2.81966
Epoch: 3/20, step: 83940, training_loss: 2.97366
Epoch: 3/20, step: 83960, training_loss: 1.91808
Epoch: 3/20, step: 83980, training_loss: 2.15592
Epoch: 3/20, step: 84000, training_loss: 2.17831
accuracy: 0.45, validation_loss: 2.0346975326538086, num_samples: 100
Epoch: 3/20, step: 84020, training_loss: 2.76836
Epoch: 3/20, step: 84040, training_loss: 2.65574
Epoch: 3/20, step: 84060, training_loss: 1.78089
Epoch: 3/20, step: 84080, training_loss: 2.59394
Epoch: 3/20, step: 84100, training_loss: 2.63985
Epoch: 3/20, step: 84120, training_loss: 2.50959
Epoch: 3/20, step: 84140, training_loss: 2.34328
Epoch: 3/20, step: 84160, training_loss: 2.42225
Epoch: 3/20, step: 84180, training_loss: 2.40437
Epoch: 3/20, step: 84200, training_loss: 2.17168
Epoch: 3/20, step: 84220, training_loss: 1.59906
Epoch: 3/20, step: 84240, training_loss: 1.91210
Epoch: 3/20, step: 84260, training_loss: 3.18083
Epoch: 3/20, step: 84280, training_loss: 1.91754
Epoch: 3/20, step: 84300, training_loss: 2.04842
Epoch: 3/20, step: 84320, training_loss: 1.60090
Epoch: 3/20, step: 84340, training_loss: 2.38749
Epoch: 3/20, step: 84360, training_loss: 2.01257
Epoch: 3/20, step: 84380, training_loss: 1.89137
Epoch: 3/20, step: 84400, training_loss: 2.74640
Epoch: 3/20, step: 84420, training_loss: 1.59106
Epoch: 3/20, step: 84440, training_loss: 1.11169
Epoch: 3/20, step: 84460, training_loss: 1.48341
Epoch: 3/20, step: 84480, training_loss: 3.08035
Epoch: 3/20, step: 84500, training_loss: 2.10810
Epoch: 3/20, step: 84520, training_loss: 2.02540
Epoch: 3/20, step: 84540, training_loss: 2.05933
Epoch: 3/20, step: 84560, training_loss: 2.46775
Epoch: 3/20, step: 84580, training_loss: 1.89540
Epoch: 3/20, step: 84600, training_loss: 2.68049
Epoch: 3/20, step: 84620, training_loss: 1.55879
Epoch: 3/20, step: 84640, training_loss: 3.70019
Epoch: 3/20, step: 84660, training_loss: 2.12745
Epoch: 3/20, step: 84680, training_loss: 1.80526
Epoch: 3/20, step: 84700, training_loss: 1.86316
Epoch: 3/20, step: 84720, training_loss: 2.00132
Epoch: 3/20, step: 84740, training_loss: 2.58860
Epoch: 3/20, step: 84760, training_loss: 2.38671
Epoch: 3/20, step: 84780, training_loss: 2.03855
Epoch: 3/20, step: 84800, training_loss: 1.90689
Epoch: 3/20, step: 84820, training_loss: 1.97164
Epoch: 3/20, step: 84840, training_loss: 2.57482
Epoch: 3/20, step: 84860, training_loss: 2.85933
Epoch: 3/20, step: 84880, training_loss: 3.15938
Epoch: 3/20, step: 84900, training_loss: 1.90899
Epoch: 3/20, step: 84920, training_loss: 2.03895
Epoch: 3/20, step: 84940, training_loss: 1.58068
Epoch: 3/20, step: 84960, training_loss: 1.79261
Epoch: 3/20, step: 84980, training_loss: 1.73925
Epoch: 3/20, step: 85000, training_loss: 2.50975
accuracy: 0.39, validation_loss: 2.2124428749084473, num_samples: 100
Epoch: 3/20, step: 85020, training_loss: 2.85960
Epoch: 3/20, step: 85040, training_loss: 2.06272
Epoch: 3/20, step: 85060, training_loss: 2.45994
Epoch: 3/20, step: 85080, training_loss: 2.69837
Epoch: 3/20, step: 85100, training_loss: 1.77731
Epoch: 3/20, step: 85120, training_loss: 2.56696
Epoch: 3/20, step: 85140, training_loss: 1.77154
Epoch: 3/20, step: 85160, training_loss: 2.91387
Epoch: 3/20, step: 85180, training_loss: 2.09917
Epoch: 3/20, step: 85200, training_loss: 2.14590
Epoch: 3/20, step: 85220, training_loss: 2.04365
Epoch: 3/20, step: 85240, training_loss: 2.60075
Epoch: 3/20, step: 85260, training_loss: 2.71399
Epoch: 3/20, step: 85280, training_loss: 2.23798
Epoch: 3/20, step: 85300, training_loss: 1.72369
Epoch: 3/20, step: 85320, training_loss: 2.36940
Epoch: 3/20, step: 85340, training_loss: 1.80473
Epoch: 3/20, step: 85360, training_loss: 2.85960
Epoch: 3/20, step: 85380, training_loss: 2.37331
Epoch: 3/20, step: 85400, training_loss: 2.79105
Epoch: 3/20, step: 85420, training_loss: 2.33572
Epoch: 3/20, step: 85440, training_loss: 2.60080
Epoch: 3/20, step: 85460, training_loss: 2.88151
Epoch: 3/20, step: 85480, training_loss: 1.90800
Epoch: 3/20, step: 85500, training_loss: 2.42987
Epoch: 3/20, step: 85520, training_loss: 3.35432
Epoch: 3/20, step: 85540, training_loss: 3.20704
Epoch: 3/20, step: 85560, training_loss: 1.35211
Epoch: 3/20, step: 85580, training_loss: 2.35446
Epoch: 3/20, step: 85600, training_loss: 1.83678
Epoch: 3/20, step: 85620, training_loss: 2.76674
Epoch: 3/20, step: 85640, training_loss: 3.23899
Epoch: 3/20, step: 85660, training_loss: 2.47866
Epoch: 3/20, step: 85680, training_loss: 2.53652
Epoch: 3/20, step: 85700, training_loss: 1.61745
Epoch: 3/20, step: 85720, training_loss: 1.75323
Epoch: 3/20, step: 85740, training_loss: 2.43853
Epoch: 3/20, step: 85760, training_loss: 2.24994
Epoch: 3/20, step: 85780, training_loss: 2.32150
Epoch: 3/20, step: 85800, training_loss: 1.45023
Epoch: 3/20, step: 85820, training_loss: 2.46503
Epoch: 3/20, step: 85840, training_loss: 1.53996
Epoch: 3/20, step: 85860, training_loss: 1.57517
Epoch: 3/20, step: 85880, training_loss: 1.54747
Epoch: 3/20, step: 85900, training_loss: 2.56918
Epoch: 3/20, step: 85920, training_loss: 2.60183
Epoch: 3/20, step: 85940, training_loss: 1.92204
Epoch: 3/20, step: 85960, training_loss: 2.09211
Epoch: 3/20, step: 85980, training_loss: 1.96741
Epoch: 3/20, step: 86000, training_loss: 2.29570
accuracy: 0.45, validation_loss: 2.0559592247009277, num_samples: 100
Epoch: 3/20, step: 86020, training_loss: 2.07692
Epoch: 3/20, step: 86040, training_loss: 2.18830
Epoch: 3/20, step: 86060, training_loss: 2.02918
Epoch: 3/20, step: 86080, training_loss: 2.01034
Epoch: 3/20, step: 86100, training_loss: 1.91027
Epoch: 3/20, step: 86120, training_loss: 2.23757
Epoch: 3/20, step: 86140, training_loss: 1.47632
Epoch: 3/20, step: 86160, training_loss: 2.34086
Epoch: 3/20, step: 86180, training_loss: 1.89355
Epoch: 3/20, step: 86200, training_loss: 1.91078
Epoch: 3/20, step: 86220, training_loss: 1.75662
Epoch: 3/20, step: 86240, training_loss: 3.17827
Epoch: 3/20, step: 86260, training_loss: 2.15371
Epoch: 3/20, step: 86280, training_loss: 2.18295
Epoch: 3/20, step: 86300, training_loss: 1.56088
Epoch: 3/20, step: 86320, training_loss: 2.22488
Epoch: 3/20, step: 86340, training_loss: 2.69802
Epoch: 3/20, step: 86360, training_loss: 3.03544
Epoch: 3/20, step: 86380, training_loss: 2.04260
Epoch: 3/20, step: 86400, training_loss: 2.13505
Epoch: 3/20, step: 86420, training_loss: 2.52505
Epoch: 3/20, step: 86440, training_loss: 1.81762
Epoch: 3/20, step: 86460, training_loss: 2.14954
Epoch: 3/20, step: 86480, training_loss: 1.51304
Epoch: 3/20, step: 86500, training_loss: 2.27973
Epoch: 3/20, step: 86520, training_loss: 2.36921
Epoch: 3/20, step: 86540, training_loss: 1.46216
Epoch: 3/20, step: 86560, training_loss: 1.84250
Epoch: 3/20, step: 86580, training_loss: 2.85203
Epoch: 3/20, step: 86600, training_loss: 1.90861
Epoch: 3/20, step: 86620, training_loss: 2.71698
Epoch: 3/20, step: 86640, training_loss: 2.15508
Epoch: 3/20, step: 86660, training_loss: 3.03882
Epoch: 3/20, step: 86680, training_loss: 2.19165
Epoch: 3/20, step: 86700, training_loss: 2.52553
Epoch: 3/20, step: 86720, training_loss: 2.03978
Epoch: 3/20, step: 86740, training_loss: 2.37780
Epoch: 3/20, step: 86760, training_loss: 2.25740
Epoch: 3/20, step: 86780, training_loss: 2.69628
Epoch: 3/20, step: 86800, training_loss: 2.61538
Epoch: 3/20, step: 86820, training_loss: 3.07859
Epoch: 3/20, step: 86840, training_loss: 2.37595
Epoch: 3/20, step: 86860, training_loss: 1.87572
Epoch: 3/20, step: 86880, training_loss: 2.07959
Epoch: 3/20, step: 86900, training_loss: 2.60351
Epoch: 3/20, step: 86920, training_loss: 1.65682
Epoch: 3/20, step: 86940, training_loss: 3.06254
Epoch: 3/20, step: 86960, training_loss: 2.25785
Epoch: 3/20, step: 86980, training_loss: 2.55632
Epoch: 3/20, step: 87000, training_loss: 2.03557
accuracy: 0.44, validation_loss: 2.1232433319091797, num_samples: 100
Epoch: 3/20, step: 87020, training_loss: 1.83923
Epoch: 3/20, step: 87040, training_loss: 1.95709
Epoch: 3/20, step: 87060, training_loss: 2.02106
Epoch: 3/20, step: 87080, training_loss: 2.49862
Epoch: 3/20, step: 87100, training_loss: 3.54873
Epoch: 3/20, step: 87120, training_loss: 3.03902
Epoch: 3/20, step: 87140, training_loss: 1.84301
Epoch: 3/20, step: 87160, training_loss: 1.25412
Epoch: 3/20, step: 87180, training_loss: 2.17255
Epoch: 3/20, step: 87200, training_loss: 1.74097
Epoch: 3/20, step: 87220, training_loss: 1.60339
Epoch: 3/20, step: 87240, training_loss: 1.75535
Epoch: 3/20, step: 87260, training_loss: 2.18064
Epoch: 3/20, step: 87280, training_loss: 2.29654
Epoch: 3/20, step: 87300, training_loss: 2.28310
Epoch: 3/20, step: 87320, training_loss: 2.00669
Epoch: 3/20, step: 87340, training_loss: 1.96377
Epoch: 3/20, step: 87360, training_loss: 1.94769
Epoch: 3/20, step: 87380, training_loss: 1.89204
Epoch: 3/20, step: 87400, training_loss: 1.76122
Epoch: 3/20, step: 87420, training_loss: 1.88051
Epoch: 3/20, step: 87440, training_loss: 2.24214
Epoch: 3/20, step: 87460, training_loss: 1.35874
Epoch: 3/20, step: 87480, training_loss: 1.98029
Epoch: 3/20, step: 87500, training_loss: 2.85444
Epoch: 3/20, step: 87520, training_loss: 2.04666
Epoch: 3/20, step: 87540, training_loss: 2.01813
Epoch: 3/20, step: 87560, training_loss: 2.50948
Epoch: 3/20, step: 87580, training_loss: 2.31408
Epoch: 3/20, step: 87600, training_loss: 1.92923
Epoch: 3/20, step: 87620, training_loss: 1.92162
Epoch: 3/20, step: 87640, training_loss: 2.24324
Epoch: 3/20, step: 87660, training_loss: 2.24416
Epoch: 3/20, step: 87680, training_loss: 1.67757
Epoch: 3/20, step: 87700, training_loss: 2.51723
Epoch: 3/20, step: 87720, training_loss: 2.31012
Epoch: 3/20, step: 87740, training_loss: 2.46429
Epoch: 3/20, step: 87760, training_loss: 2.67925
Epoch: 3/20, step: 87780, training_loss: 2.17666
Epoch: 3/20, step: 87800, training_loss: 2.30071
Epoch: 3/20, step: 87820, training_loss: 1.33388
Epoch: 3/20, step: 87840, training_loss: 2.58029
Epoch: 3/20, step: 87860, training_loss: 2.09025
Epoch: 3/20, step: 87880, training_loss: 2.32952
Epoch: 3/20, step: 87900, training_loss: 1.74540
Epoch: 3/20, step: 87920, training_loss: 2.55366
Epoch: 3/20, step: 87940, training_loss: 2.00844
Epoch: 3/20, step: 87960, training_loss: 2.61620
Epoch: 3/20, step: 87980, training_loss: 2.43182
Epoch: 3/20, step: 88000, training_loss: 1.50999
accuracy: 0.42, validation_loss: 2.196913957595825, num_samples: 100
Epoch: 3/20, step: 88020, training_loss: 1.55921
Epoch: 3/20, step: 88040, training_loss: 2.32667
Epoch: 3/20, step: 88060, training_loss: 3.30854
Epoch: 3/20, step: 88080, training_loss: 2.35042
Epoch: 3/20, step: 88100, training_loss: 1.67053
Epoch: 3/20, step: 88120, training_loss: 2.89996
Epoch: 3/20, step: 88140, training_loss: 2.01103
Epoch: 3/20, step: 88160, training_loss: 1.57069
Epoch: 3/20, step: 88180, training_loss: 2.38398
Epoch: 3/20, step: 88200, training_loss: 2.54890
Epoch: 3/20, step: 88220, training_loss: 2.04509
Epoch: 3/20, step: 88240, training_loss: 2.42748
Epoch: 3/20, step: 88260, training_loss: 2.30330
Epoch: 3/20, step: 88280, training_loss: 1.96058
Epoch: 3/20, step: 88300, training_loss: 1.91587
Epoch: 3/20, step: 88320, training_loss: 1.63114
Epoch: 3/20, step: 88340, training_loss: 2.17007
Epoch: 3/20, step: 88360, training_loss: 2.04793
Epoch: 3/20, step: 88380, training_loss: 2.37406
Epoch: 3/20, step: 88400, training_loss: 2.49814
Epoch: 3/20, step: 88420, training_loss: 1.96881
Epoch: 3/20, step: 88440, training_loss: 1.41413
Epoch: 3/20, step: 88460, training_loss: 1.94372
Epoch: 3/20, step: 88480, training_loss: 1.73745
Epoch: 3/20, step: 88500, training_loss: 2.46632
Epoch: 3/20, step: 88520, training_loss: 2.12073
Epoch: 3/20, step: 88540, training_loss: 2.05427
Epoch: 3/20, step: 88560, training_loss: 2.23796
Epoch: 3/20, step: 88580, training_loss: 1.75406
Epoch: 3/20, step: 88600, training_loss: 2.59480
Epoch: 3/20, step: 88620, training_loss: 1.77010
Epoch: 3/20, step: 88640, training_loss: 2.32571
Epoch: 3/20, step: 88660, training_loss: 2.24339
Epoch: 3/20, step: 88680, training_loss: 1.88720
Epoch: 3/20, step: 88700, training_loss: 2.31324
Epoch: 3/20, step: 88720, training_loss: 2.56565
Epoch: 3/20, step: 88740, training_loss: 2.35258
Epoch: 3/20, step: 88760, training_loss: 1.63567
Epoch: 3/20, step: 88780, training_loss: 2.08504
Epoch: 3/20, step: 88800, training_loss: 2.28886
Epoch: 3/20, step: 88820, training_loss: 2.69298
Epoch: 3/20, step: 88840, training_loss: 2.35038
Epoch: 3/20, step: 88860, training_loss: 1.50958
Epoch: 3/20, step: 88880, training_loss: 1.97806
Epoch: 3/20, step: 88900, training_loss: 1.99053
Epoch: 3/20, step: 88920, training_loss: 2.30019
Epoch: 3/20, step: 88940, training_loss: 1.52310
Epoch: 3/20, step: 88960, training_loss: 2.11536
Epoch: 3/20, step: 88980, training_loss: 3.21738
Epoch: 3/20, step: 89000, training_loss: 2.56657
accuracy: 0.43, validation_loss: 2.0028090476989746, num_samples: 100
Epoch: 3/20, step: 89020, training_loss: 2.40579
Epoch: 3/20, step: 89040, training_loss: 2.34198
Epoch: 3/20, step: 89060, training_loss: 2.71826
Epoch: 3/20, step: 89080, training_loss: 2.11808
Epoch: 3/20, step: 89100, training_loss: 2.09774
Epoch: 3/20, step: 89120, training_loss: 3.00308
Epoch: 3/20, step: 89140, training_loss: 1.46909
Epoch: 3/20, step: 89160, training_loss: 2.51076
Epoch: 3/20, step: 89180, training_loss: 2.28792
Epoch: 3/20, step: 89200, training_loss: 1.86468
Epoch: 3/20, step: 89220, training_loss: 1.68101
Epoch: 3/20, step: 89240, training_loss: 2.27399
Epoch: 3/20, step: 89260, training_loss: 1.66626
Epoch: 3/20, step: 89280, training_loss: 1.77559
Epoch: 3/20, step: 89300, training_loss: 2.61913
Epoch: 3/20, step: 89320, training_loss: 2.55726
Epoch: 3/20, step: 89340, training_loss: 2.21735
Epoch: 3/20, step: 89360, training_loss: 1.74646
Epoch: 3/20, step: 89380, training_loss: 2.23716
Epoch: 3/20, step: 89400, training_loss: 1.61345
Epoch: 3/20, step: 89420, training_loss: 2.96143
Epoch: 3/20, step: 89440, training_loss: 2.01840
Epoch: 3/20, step: 89460, training_loss: 2.42898
Epoch: 3/20, step: 89480, training_loss: 2.75040
Epoch: 3/20, step: 89500, training_loss: 2.34241
Epoch: 3/20, step: 89520, training_loss: 2.60560
Epoch: 3/20, step: 89540, training_loss: 1.48135
Epoch: 3/20, step: 89560, training_loss: 2.56696
Epoch: 3/20, step: 89580, training_loss: 2.68873
Epoch: 3/20, step: 89600, training_loss: 2.16629
Epoch: 3/20, step: 89620, training_loss: 2.67723
Epoch: 3/20, step: 89640, training_loss: 2.13550
Epoch: 3/20, step: 89660, training_loss: 3.02505
Epoch: 3/20, step: 89680, training_loss: 3.07889
Epoch: 3/20, step: 89700, training_loss: 2.33302
Epoch: 3/20, step: 89720, training_loss: 1.41993
Epoch: 3/20, step: 89740, training_loss: 2.40500
Epoch: 3/20, step: 89760, training_loss: 2.33917
Epoch: 3/20, step: 89780, training_loss: 1.88778
Epoch: 3/20, step: 89800, training_loss: 2.85095
Epoch: 3/20, step: 89820, training_loss: 2.18082
Epoch: 3/20, step: 89840, training_loss: 2.85045
Epoch: 3/20, step: 89860, training_loss: 1.38807
Epoch: 3/20, step: 89880, training_loss: 1.56313
Epoch: 3/20, step: 89900, training_loss: 2.79941
Epoch: 3/20, step: 89920, training_loss: 2.59646
Epoch: 3/20, step: 89940, training_loss: 2.28161
Epoch: 3/20, step: 89960, training_loss: 2.50587
Epoch: 3/20, step: 89980, training_loss: 2.84101
Epoch: 3/20, step: 90000, training_loss: 1.56891
accuracy: 0.42, validation_loss: 2.3012900352478027, num_samples: 100
Epoch: 3/20, step: 90020, training_loss: 2.99975
Epoch: 3/20, step: 90040, training_loss: 2.41968
Epoch: 3/20, step: 90060, training_loss: 1.66255
Epoch: 3/20, step: 90080, training_loss: 1.13057
Epoch: 3/20, step: 90100, training_loss: 2.02265
Epoch: 3/20, step: 90120, training_loss: 2.78417
Epoch: 3/20, step: 90140, training_loss: 2.17930
Epoch: 3/20, step: 90160, training_loss: 2.19440
Epoch: 3/20, step: 90180, training_loss: 1.93176
Epoch: 3/20, step: 90200, training_loss: 1.69472
Epoch: 3/20, step: 90220, training_loss: 2.03867
Epoch: 3/20, step: 90240, training_loss: 1.79423
Epoch: 3/20, step: 90260, training_loss: 2.59570
Epoch: 3/20, step: 90280, training_loss: 2.34696
Epoch: 3/20, step: 90300, training_loss: 2.72421
Epoch: 3/20, step: 90320, training_loss: 2.62769
Epoch: 3/20, step: 90340, training_loss: 1.62962
Epoch: 3/20, step: 90360, training_loss: 2.56814
Epoch: 3/20, step: 90380, training_loss: 2.53533
Epoch: 3/20, step: 90400, training_loss: 1.54226
Epoch: 3/20, step: 90420, training_loss: 2.12153
Epoch: 3/20, step: 90440, training_loss: 1.64644
Epoch: 3/20, step: 90460, training_loss: 2.84686
Epoch: 3/20, step: 90480, training_loss: 1.86528
Epoch: 3/20, step: 90500, training_loss: 2.44096
Epoch: 3/20, step: 90520, training_loss: 1.74366
Epoch: 3/20, step: 90540, training_loss: 1.74563
Epoch: 3/20, step: 90560, training_loss: 1.06885
Epoch: 3/20, step: 90580, training_loss: 3.13768
Epoch: 3/20, step: 90600, training_loss: 2.65279
Epoch: 3/20, step: 90620, training_loss: 1.41616
Epoch: 3/20, step: 90640, training_loss: 2.23151
Epoch: 3/20, step: 90660, training_loss: 2.27951
Epoch: 3/20, step: 90680, training_loss: 1.58073
Epoch: 3/20, step: 90700, training_loss: 2.30239
Epoch: 3/20, step: 90720, training_loss: 2.30405
Epoch: 3/20, step: 90740, training_loss: 2.76526
Epoch: 3/20, step: 90760, training_loss: 1.99183
Epoch: 3/20, step: 90780, training_loss: 2.23284
Epoch: 3/20, step: 90800, training_loss: 1.73052
Epoch: 3/20, step: 90820, training_loss: 2.22637
Epoch: 3/20, step: 90840, training_loss: 2.72524
Epoch: 3/20, step: 90860, training_loss: 2.50289
Epoch: 3/20, step: 90880, training_loss: 1.98272
Epoch: 3/20, step: 90900, training_loss: 2.63780
Epoch: 3/20, step: 90920, training_loss: 1.82173
Epoch: 3/20, step: 90940, training_loss: 1.99304
Epoch: 3/20, step: 90960, training_loss: 2.08976
Epoch: 3/20, step: 90980, training_loss: 2.21670
Epoch: 3/20, step: 91000, training_loss: 1.58664
accuracy: 0.49, validation_loss: 2.056774377822876, num_samples: 100
Epoch: 3/20, step: 91020, training_loss: 2.47459
Epoch: 3/20, step: 91040, training_loss: 2.06530
Epoch: 3/20, step: 91060, training_loss: 3.03382
Epoch: 3/20, step: 91080, training_loss: 1.74776
Epoch: 3/20, step: 91100, training_loss: 2.53468
Epoch: 3/20, step: 91120, training_loss: 2.12899
Epoch: 3/20, step: 91140, training_loss: 2.02491
Epoch: 3/20, step: 91160, training_loss: 1.89967
Epoch: 3/20, step: 91180, training_loss: 2.10942
Epoch: 3/20, step: 91200, training_loss: 1.94468
Epoch: 3/20, step: 91220, training_loss: 1.59486
Epoch: 3/20, step: 91240, training_loss: 2.00501
Epoch: 3/20, step: 91260, training_loss: 2.36777
Epoch: 3/20, step: 91280, training_loss: 2.39007
Epoch: 3/20, step: 91300, training_loss: 1.68012
Epoch: 3/20, step: 91320, training_loss: 1.90421
Epoch: 3/20, step: 91340, training_loss: 2.54351
Epoch: 3/20, step: 91360, training_loss: 1.90300
Epoch: 3/20, step: 91380, training_loss: 2.80611
Epoch: 3/20, step: 91400, training_loss: 1.89942
Epoch: 3/20, step: 91420, training_loss: 1.32987
Epoch: 3/20, step: 91440, training_loss: 2.07045
Epoch: 3/20, step: 91460, training_loss: 2.70791
Epoch: 3/20, step: 91480, training_loss: 2.26936
Epoch: 3/20, step: 91500, training_loss: 2.41634
Epoch: 3/20, step: 91520, training_loss: 2.59705
Epoch: 3/20, step: 91540, training_loss: 2.62663
Epoch: 3/20, step: 91560, training_loss: 2.72812
Epoch: 3/20, step: 91580, training_loss: 2.14038
Epoch: 3/20, step: 91600, training_loss: 2.57266
Epoch: 3/20, step: 91620, training_loss: 1.56985
Epoch: 3/20, step: 91640, training_loss: 1.95882
Epoch: 3/20, step: 91660, training_loss: 2.48551
Epoch: 3/20, step: 91680, training_loss: 2.64445
Epoch: 3/20, step: 91700, training_loss: 2.06365
Epoch: 3/20, step: 91720, training_loss: 2.91018
Epoch: 3/20, step: 91740, training_loss: 2.17607
Epoch: 3/20, step: 91760, training_loss: 1.65435
Epoch: 3/20, step: 91780, training_loss: 1.97807
Epoch: 3/20, step: 91800, training_loss: 1.65339
Epoch: 3/20, step: 91820, training_loss: 1.38556
Epoch: 3/20, step: 91840, training_loss: 2.75163
Epoch: 3/20, step: 91860, training_loss: 2.19882
Epoch: 3/20, step: 91880, training_loss: 1.82591
Epoch: 3/20, step: 91900, training_loss: 2.50562
Epoch: 3/20, step: 91920, training_loss: 3.04270
Epoch: 3/20, step: 91940, training_loss: 2.01107
Epoch: 3/20, step: 91960, training_loss: 2.69480
Epoch: 3/20, step: 91980, training_loss: 2.64947
Epoch: 3/20, step: 92000, training_loss: 2.33303
accuracy: 0.35, validation_loss: 2.4677419662475586, num_samples: 100
Epoch: 3/20, step: 92020, training_loss: 1.99750
Epoch: 3/20, step: 92040, training_loss: 2.09798
Epoch: 3/20, step: 92060, training_loss: 2.01587
Epoch: 3/20, step: 92080, training_loss: 1.60648
Epoch: 3/20, step: 92100, training_loss: 1.60445
Epoch: 3/20, step: 92120, training_loss: 1.91888
Epoch: 3/20, step: 92140, training_loss: 2.10737
Epoch: 3/20, step: 92160, training_loss: 2.50918
Epoch: 3/20, step: 92180, training_loss: 1.63214
Epoch: 3/20, step: 92200, training_loss: 1.84266
Epoch: 3/20, step: 92220, training_loss: 2.07782
Epoch: 3/20, step: 92240, training_loss: 2.51856
Epoch: 3/20, step: 92260, training_loss: 2.33992
Epoch: 3/20, step: 92280, training_loss: 1.84465
Epoch: 3/20, step: 92300, training_loss: 1.90121
Epoch: 3/20, step: 92320, training_loss: 3.42233
Epoch: 3/20, step: 92340, training_loss: 1.70516
Epoch: 3/20, step: 92360, training_loss: 1.55828
Epoch: 3/20, step: 92380, training_loss: 1.73898
Epoch: 3/20, step: 92400, training_loss: 2.51417
Epoch: 3/20, step: 92420, training_loss: 2.37828
Epoch: 3/20, step: 92440, training_loss: 2.59548
Epoch: 3/20, step: 92460, training_loss: 2.22698
Epoch: 3/20, step: 92480, training_loss: 2.75118
Epoch: 3/20, step: 92500, training_loss: 1.58375
Epoch: 3/20, step: 92520, training_loss: 3.01972
Epoch: 3/20, step: 92540, training_loss: 2.73874
Epoch: 3/20, step: 92560, training_loss: 2.04580
Epoch: 3/20, step: 92580, training_loss: 2.32562
Epoch: 3/20, step: 92600, training_loss: 2.48094
Epoch: 3/20, step: 92620, training_loss: 1.78479
Epoch: 3/20, step: 92640, training_loss: 1.65501
Epoch: 3/20, step: 92660, training_loss: 2.18295
Epoch: 3/20, step: 92680, training_loss: 1.98217
Epoch: 3/20, step: 92700, training_loss: 3.20364
Epoch: 3/20, step: 92720, training_loss: 2.35187
Epoch: 3/20, step: 92740, training_loss: 2.44897
Epoch: 3/20, step: 92760, training_loss: 2.13171
Epoch: 3/20, step: 92780, training_loss: 1.76902
Epoch: 3/20, step: 92800, training_loss: 2.73872
Epoch: 3/20, step: 92820, training_loss: 1.97477
Epoch: 3/20, step: 92840, training_loss: 2.05245
Epoch: 3/20, step: 92860, training_loss: 2.65733
Epoch: 3/20, step: 92880, training_loss: 2.66570
Epoch: 3/20, step: 92900, training_loss: 3.24811
Epoch: 3/20, step: 92920, training_loss: 1.62684
Epoch: 3/20, step: 92940, training_loss: 2.65475
Epoch: 3/20, step: 92960, training_loss: 2.02006
Epoch: 3/20, step: 92980, training_loss: 2.32940
Epoch: 3/20, step: 93000, training_loss: 2.09962
accuracy: 0.48, validation_loss: 1.9159214496612549, num_samples: 100
Epoch: 3/20, step: 93020, training_loss: 1.76026
Epoch: 3/20, step: 93040, training_loss: 1.67963
Epoch: 3/20, step: 93060, training_loss: 2.08425
Epoch: 3/20, step: 93080, training_loss: 2.17293
Epoch: 3/20, step: 93100, training_loss: 1.24300
Epoch: 3/20, step: 93120, training_loss: 2.53735
Epoch: 3/20, step: 93140, training_loss: 1.60076
Epoch: 3/20, step: 93160, training_loss: 2.14854
Epoch: 3/20, step: 93180, training_loss: 2.19510
Epoch: 3/20, step: 93200, training_loss: 2.32694
Epoch: 3/20, step: 93220, training_loss: 2.60160
Epoch: 3/20, step: 93240, training_loss: 1.81010
Epoch: 3/20, step: 93260, training_loss: 2.25116
Epoch: 3/20, step: 93280, training_loss: 1.27901
Epoch: 3/20, step: 93300, training_loss: 2.17405
Epoch: 3/20, step: 93320, training_loss: 1.71498
Epoch: 3/20, step: 93340, training_loss: 2.25906
Epoch: 3/20, step: 93360, training_loss: 2.47600
Epoch: 3/20, step: 93380, training_loss: 2.31262
Epoch: 3/20, step: 93400, training_loss: 2.37803
Epoch: 3/20, step: 93420, training_loss: 2.60818
Epoch: 3/20, step: 93440, training_loss: 2.10465
Epoch: 3/20, step: 93460, training_loss: 1.79313
Epoch: 3/20, step: 93480, training_loss: 1.87749
Epoch: 3/20, step: 93500, training_loss: 2.25647
Epoch: 3/20, step: 93520, training_loss: 1.34585
Epoch: 3/20, step: 93540, training_loss: 1.98390
Epoch: 3/20, step: 93560, training_loss: 1.90254
Epoch: 3/20, step: 93580, training_loss: 0.79083
Epoch: 3/20, step: 93600, training_loss: 2.26055
Epoch: 3/20, step: 93620, training_loss: 1.62350
Epoch: 3/20, step: 93640, training_loss: 1.91789
Epoch: 3/20, step: 93660, training_loss: 2.58193
Epoch: 3/20, step: 93680, training_loss: 1.38971
Epoch: 3/20, step: 93700, training_loss: 2.45237
Epoch: 3/20, step: 93720, training_loss: 1.70219
Epoch: 3/20, step: 93740, training_loss: 2.75064
Epoch: 3/20, step: 93760, training_loss: 1.94306
Epoch: 3/20, step: 93780, training_loss: 2.72166
Epoch: 3/20, step: 93800, training_loss: 1.40469
Epoch: 3/20, step: 93820, training_loss: 1.94422
Epoch: 3/20, step: 93840, training_loss: 2.33073
Epoch: 3/20, step: 93860, training_loss: 2.37337
Epoch: 3/20, step: 93880, training_loss: 2.44507
Epoch: 3/20, step: 93900, training_loss: 2.13163
Epoch: 3/20, step: 93920, training_loss: 2.11104
Epoch: 3/20, step: 93940, training_loss: 2.36095
Epoch: 3/20, step: 93960, training_loss: 1.91720
Epoch: 3/20, step: 93980, training_loss: 1.60861
Epoch: 3/20, step: 94000, training_loss: 2.36742
accuracy: 0.35, validation_loss: 2.234945297241211, num_samples: 100
Epoch: 3/20, step: 94020, training_loss: 2.16954
Epoch: 3/20, step: 94040, training_loss: 2.43575
Epoch: 3/20, step: 94060, training_loss: 2.07930
Epoch: 3/20, step: 94080, training_loss: 2.00055
Epoch: 3/20, step: 94100, training_loss: 1.63219
Epoch: 3/20, step: 94120, training_loss: 1.61006
Epoch: 3/20, step: 94140, training_loss: 3.08428
Epoch: 3/20, step: 94160, training_loss: 1.65193
Epoch: 3/20, step: 94180, training_loss: 2.36298
Epoch: 3/20, step: 94200, training_loss: 2.16555
Epoch: 3/20, step: 94220, training_loss: 1.35644
Epoch: 3/20, step: 94240, training_loss: 1.82270
Epoch: 3/20, step: 94260, training_loss: 2.42572
Epoch: 3/20, step: 94280, training_loss: 3.03464
Epoch: 3/20, step: 94300, training_loss: 2.17178
Epoch: 3/20, step: 94320, training_loss: 2.56669
Epoch: 3/20, step: 94340, training_loss: 2.26492
Epoch: 3/20, step: 94360, training_loss: 1.89406
Epoch: 3/20, step: 94380, training_loss: 2.03786
Epoch: 3/20, step: 94400, training_loss: 2.05007
Epoch: 3/20, step: 94420, training_loss: 1.81095
Epoch: 3/20, step: 94440, training_loss: 1.86085
Epoch: 3/20, step: 94460, training_loss: 2.31614
Epoch: 3/20, step: 94480, training_loss: 1.44238
Epoch: 3/20, step: 94500, training_loss: 2.03089
Epoch: 3/20, step: 94520, training_loss: 2.06576
Epoch: 3/20, step: 94540, training_loss: 1.95902
Epoch: 3/20, step: 94560, training_loss: 1.74001
Epoch: 3/20, step: 94580, training_loss: 1.67405
Epoch: 3/20, step: 94600, training_loss: 1.18961
Epoch: 3/20, step: 94620, training_loss: 2.67895
Epoch: 3/20, step: 94640, training_loss: 1.99440
Epoch: 3/20, step: 94660, training_loss: 2.47661
Epoch: 3/20, step: 94680, training_loss: 3.07411
Epoch: 3/20, step: 94700, training_loss: 1.34438
Epoch: 3/20, step: 94720, training_loss: 2.34475
Epoch: 3/20, step: 94740, training_loss: 2.36367
Epoch: 3/20, step: 94760, training_loss: 2.74195
Epoch: 3/20, step: 94780, training_loss: 1.71207
Epoch: 3/20, step: 94800, training_loss: 2.18915
Epoch: 3/20, step: 94820, training_loss: 2.11058
Epoch: 3/20, step: 94840, training_loss: 1.92684
Epoch: 3/20, step: 94860, training_loss: 1.52354
Epoch: 3/20, step: 94880, training_loss: 2.26891
Epoch: 3/20, step: 94900, training_loss: 2.29020
Epoch: 3/20, step: 94920, training_loss: 2.35187
Epoch: 3/20, step: 94940, training_loss: 1.43185
Epoch: 3/20, step: 94960, training_loss: 2.32451
Epoch: 3/20, step: 94980, training_loss: 1.67045
Epoch: 3/20, step: 95000, training_loss: 2.64516
accuracy: 0.41, validation_loss: 2.08931827545166, num_samples: 100
Epoch: 3/20, step: 95020, training_loss: 2.91399
Epoch: 3/20, step: 95040, training_loss: 1.48681
Epoch: 3/20, step: 95060, training_loss: 1.86079
Epoch: 3/20, step: 95080, training_loss: 2.31564
Epoch: 3/20, step: 95100, training_loss: 2.33264
Epoch: 3/20, step: 95120, training_loss: 1.51123
Epoch: 3/20, step: 95140, training_loss: 2.55013
Epoch: 3/20, step: 95160, training_loss: 1.58545
Epoch: 3/20, step: 95180, training_loss: 2.48852
Epoch: 3/20, step: 95200, training_loss: 2.38921
Epoch: 3/20, step: 95220, training_loss: 2.41564
Epoch: 3/20, step: 95240, training_loss: 2.17059
Epoch: 3/20, step: 95260, training_loss: 2.25490
Epoch: 3/20, step: 95280, training_loss: 2.97609
Epoch: 3/20, step: 95300, training_loss: 2.28519
Epoch: 3/20, step: 95320, training_loss: 1.23436
Epoch: 3/20, step: 95340, training_loss: 2.11798
Epoch: 3/20, step: 95360, training_loss: 2.02750
Epoch: 3/20, step: 95380, training_loss: 2.20668
Epoch: 3/20, step: 95400, training_loss: 2.37915
Epoch: 3/20, step: 95420, training_loss: 1.36248
Epoch: 3/20, step: 95440, training_loss: 1.79621
Epoch: 3/20, step: 95460, training_loss: 2.56001
Epoch: 3/20, step: 95480, training_loss: 1.68349
Epoch: 3/20, step: 95500, training_loss: 2.08597
Epoch: 3/20, step: 95520, training_loss: 2.12626
Epoch: 3/20, step: 95540, training_loss: 2.35212
Epoch: 3/20, step: 95560, training_loss: 2.25311
Epoch: 3/20, step: 95580, training_loss: 1.49970
Epoch: 3/20, step: 95600, training_loss: 2.12374
Epoch: 3/20, step: 95620, training_loss: 2.37536
Epoch: 3/20, step: 95640, training_loss: 1.68241
Epoch: 3/20, step: 95660, training_loss: 1.78765
Epoch: 3/20, step: 95680, training_loss: 2.64973
Epoch: 3/20, step: 95700, training_loss: 1.60771
Epoch: 3/20, step: 95720, training_loss: 2.38113
Epoch: 3/20, step: 95740, training_loss: 2.44096
Epoch: 3/20, step: 95760, training_loss: 2.14603
Epoch: 3/20, step: 95780, training_loss: 2.56872
Epoch: 3/20, step: 95800, training_loss: 1.29083
Epoch: 3/20, step: 95820, training_loss: 2.73934
Epoch: 3/20, step: 95840, training_loss: 2.06149
Epoch: 3/20, step: 95860, training_loss: 2.01362
Epoch: 3/20, step: 95880, training_loss: 2.42194
Epoch: 3/20, step: 95900, training_loss: 2.95590
Epoch: 3/20, step: 95920, training_loss: 1.90642
Epoch: 3/20, step: 95940, training_loss: 1.72017
Epoch: 3/20, step: 95960, training_loss: 1.91494
Epoch: 3/20, step: 95980, training_loss: 2.28929
Epoch: 3/20, step: 96000, training_loss: 2.24489
accuracy: 0.41, validation_loss: 2.2124133110046387, num_samples: 100
Epoch: 3/20, step: 96020, training_loss: 2.67502
Epoch: 3/20, step: 96040, training_loss: 2.20235
Epoch: 3/20, step: 96060, training_loss: 2.46335
Epoch: 3/20, step: 96080, training_loss: 2.33680
Epoch: 3/20, step: 96100, training_loss: 1.77978
Epoch: 3/20, step: 96120, training_loss: 2.62245
Epoch: 3/20, step: 96140, training_loss: 1.16796
Epoch: 3/20, step: 96160, training_loss: 1.90805
Epoch: 3/20, step: 96180, training_loss: 2.39984
Epoch: 3/20, step: 96200, training_loss: 1.83691
Epoch: 3/20, step: 96220, training_loss: 2.71198
Epoch: 3/20, step: 96240, training_loss: 1.55773
Epoch: 3/20, step: 96260, training_loss: 3.34098
Epoch: 3/20, step: 96280, training_loss: 2.32437
Epoch: 3/20, step: 96300, training_loss: 2.85355
Epoch: 3/20, step: 96320, training_loss: 2.18739
Epoch: 3/20, step: 96340, training_loss: 2.46806
Epoch: 3/20, step: 96360, training_loss: 2.39521
Epoch: 3/20, step: 96380, training_loss: 2.53450
Epoch: 3/20, step: 96400, training_loss: 1.98821
Epoch: 3/20, step: 96420, training_loss: 2.25072
Epoch: 3/20, step: 96440, training_loss: 2.06732
Epoch: 3/20, step: 96460, training_loss: 2.37867
Epoch: 3/20, step: 96480, training_loss: 1.48231
Epoch: 3/20, step: 96500, training_loss: 1.96219
Epoch: 3/20, step: 96520, training_loss: 1.96792
Epoch: 3/20, step: 96540, training_loss: 2.97985
Epoch: 3/20, step: 96560, training_loss: 2.03773
Epoch: 3/20, step: 96580, training_loss: 1.96127
Epoch: 3/20, step: 96600, training_loss: 2.78234
Epoch: 3/20, step: 96620, training_loss: 2.46145
Epoch: 3/20, step: 96640, training_loss: 2.57452
Epoch: 3/20, step: 96660, training_loss: 1.79019
Epoch: 3/20, step: 96680, training_loss: 2.46092
Epoch: 3/20, step: 96700, training_loss: 2.42712
Epoch: 3/20, step: 96720, training_loss: 2.34988
Epoch: 3/20, step: 96740, training_loss: 2.08752
Epoch: 3/20, step: 96760, training_loss: 2.62465
Epoch: 3/20, step: 96780, training_loss: 1.60333
Epoch: 3/20, step: 96800, training_loss: 2.91088
Epoch: 3/20, step: 96820, training_loss: 1.82571
Epoch: 3/20, step: 96840, training_loss: 3.44627
Epoch: 3/20, step: 96860, training_loss: 3.10872
Epoch: 3/20, step: 96880, training_loss: 1.99539
Epoch: 3/20, step: 96900, training_loss: 2.16156
Epoch: 3/20, step: 96920, training_loss: 2.86837
Epoch: 3/20, step: 96940, training_loss: 1.56136
Epoch: 3/20, step: 96960, training_loss: 1.33699
Epoch: 3/20, step: 96980, training_loss: 1.86215
Epoch: 3/20, step: 97000, training_loss: 1.91465
accuracy: 0.39, validation_loss: 2.0977377891540527, num_samples: 100
Epoch: 3/20, step: 97020, training_loss: 1.64532
Epoch: 3/20, step: 97040, training_loss: 1.82128
Epoch: 3/20, step: 97060, training_loss: 1.92162
Epoch: 3/20, step: 97080, training_loss: 1.74991
Epoch: 3/20, step: 97100, training_loss: 1.44683
Epoch: 3/20, step: 97120, training_loss: 3.09555
Epoch: 3/20, step: 97140, training_loss: 2.14205
Epoch: 3/20, step: 97160, training_loss: 2.42075
Epoch: 3/20, step: 97180, training_loss: 2.17362
Epoch: 3/20, step: 97200, training_loss: 3.05979
Epoch: 3/20, step: 97220, training_loss: 1.97139
Epoch: 3/20, step: 97240, training_loss: 2.27767
Epoch: 3/20, step: 97260, training_loss: 2.57816
Epoch: 3/20, step: 97280, training_loss: 1.54935
Epoch: 3/20, step: 97300, training_loss: 2.50291
Epoch: 3/20, step: 97320, training_loss: 2.20581
Epoch: 3/20, step: 97340, training_loss: 2.32926
Epoch: 3/20, step: 97360, training_loss: 0.92329
Epoch: 3/20, step: 97380, training_loss: 2.32570
Epoch: 3/20, step: 97400, training_loss: 1.85099
Epoch: 3/20, step: 97420, training_loss: 2.50897
Epoch: 3/20, step: 97440, training_loss: 1.65891
Epoch: 3/20, step: 97460, training_loss: 2.00401
Epoch: 3/20, step: 97480, training_loss: 1.72338
Epoch: 3/20, step: 97500, training_loss: 2.69529
Epoch: 3/20, step: 97520, training_loss: 2.06669
Epoch: 3/20, step: 97540, training_loss: 2.75722
Epoch: 3/20, step: 97560, training_loss: 2.44969
Epoch: 3/20, step: 97580, training_loss: 2.09347
Epoch: 3/20, step: 97600, training_loss: 1.86165
Epoch: 3/20, step: 97620, training_loss: 2.05232
Epoch: 3/20, step: 97640, training_loss: 1.76228
Epoch: 3/20, step: 97660, training_loss: 2.71101
Epoch: 3/20, step: 97680, training_loss: 1.80651
Epoch: 3/20, step: 97700, training_loss: 2.48716
Epoch: 3/20, step: 97720, training_loss: 2.58588
Epoch: 3/20, step: 97740, training_loss: 2.82372
Epoch: 3/20, step: 97760, training_loss: 1.74755
Epoch: 3/20, step: 97780, training_loss: 2.11669
Epoch: 3/20, step: 97800, training_loss: 1.53159
Epoch: 3/20, step: 97820, training_loss: 2.37132
Epoch: 3/20, step: 97840, training_loss: 1.65072
Epoch: 3/20, step: 97860, training_loss: 2.84164
Epoch: 3/20, step: 97880, training_loss: 2.31931
Epoch: 3/20, step: 97900, training_loss: 2.20394
Epoch: 3/20, step: 97920, training_loss: 1.91676
Epoch: 3/20, step: 97940, training_loss: 2.58889
Epoch: 3/20, step: 97960, training_loss: 1.80692
Epoch: 3/20, step: 97980, training_loss: 1.54841
Epoch: 3/20, step: 98000, training_loss: 1.17612
accuracy: 0.39, validation_loss: 1.99989652633667, num_samples: 100
Epoch: 3/20, step: 98020, training_loss: 3.01069
Epoch: 3/20, step: 98040, training_loss: 2.27822
Epoch: 3/20, step: 98060, training_loss: 2.05202
Epoch: 3/20, step: 98080, training_loss: 1.79003
Epoch: 3/20, step: 98100, training_loss: 2.13940
Epoch: 3/20, step: 98120, training_loss: 2.74331
Epoch: 3/20, step: 98140, training_loss: 2.07661
Epoch: 3/20, step: 98160, training_loss: 1.36937
Epoch: 3/20, step: 98180, training_loss: 1.75912
Epoch: 3/20, step: 98200, training_loss: 1.83370
Epoch: 3/20, step: 98220, training_loss: 2.53388
Epoch: 3/20, step: 98240, training_loss: 2.31968
Epoch: 3/20, step: 98260, training_loss: 2.17739
Epoch: 3/20, step: 98280, training_loss: 1.82707
Epoch: 3/20, step: 98300, training_loss: 2.60556
Epoch: 3/20, step: 98320, training_loss: 2.39798
Epoch: 3/20, step: 98340, training_loss: 1.61405
Epoch: 3/20, step: 98360, training_loss: 2.56638
Epoch: 3/20, step: 98380, training_loss: 2.57478
Epoch: 3/20, step: 98400, training_loss: 1.89570
Epoch: 3/20, step: 98420, training_loss: 1.74773
Epoch: 3/20, step: 98440, training_loss: 1.90046
Epoch: 3/20, step: 98460, training_loss: 2.09721
Epoch: 3/20, step: 98480, training_loss: 1.86058
Epoch: 3/20, step: 98500, training_loss: 1.73796
Epoch: 3/20, step: 98520, training_loss: 2.49564
Epoch: 3/20, step: 98540, training_loss: 1.76019
Epoch: 3/20, step: 98560, training_loss: 1.98506
Epoch: 3/20, step: 98580, training_loss: 1.72522
Epoch: 3/20, step: 98600, training_loss: 2.23742
Epoch: 3/20, step: 98620, training_loss: 1.50698
Epoch: 3/20, step: 98640, training_loss: 1.53017
Epoch: 3/20, step: 98660, training_loss: 2.31348
Epoch: 3/20, step: 98680, training_loss: 1.94179
Epoch: 3/20, step: 98700, training_loss: 2.36386
Epoch: 3/20, step: 98720, training_loss: 2.34785
Epoch: 3/20, step: 98740, training_loss: 2.40700
Epoch: 3/20, step: 98760, training_loss: 2.50796
Epoch: 3/20, step: 98780, training_loss: 1.68709
Epoch: 3/20, step: 98800, training_loss: 2.26887
Epoch: 3/20, step: 98820, training_loss: 2.99546
Epoch: 3/20, step: 98840, training_loss: 2.25221
Epoch: 3/20, step: 98860, training_loss: 2.19627
Epoch: 3/20, step: 98880, training_loss: 2.33792
Epoch: 3/20, step: 98900, training_loss: 1.71926
Epoch: 3/20, step: 98920, training_loss: 1.26821
Epoch: 3/20, step: 98940, training_loss: 0.93404
Epoch: 3/20, step: 98960, training_loss: 2.06769
Epoch: 3/20, step: 98980, training_loss: 1.92283
Epoch: 3/20, step: 99000, training_loss: 2.43137
accuracy: 0.49, validation_loss: 1.8292864561080933, num_samples: 100
Epoch: 3/20, step: 99020, training_loss: 2.10064
Epoch: 3/20, step: 99040, training_loss: 1.95246
Epoch: 3/20, step: 99060, training_loss: 1.91627
Epoch: 3/20, step: 99080, training_loss: 3.00905
Epoch: 3/20, step: 99100, training_loss: 1.68334
Epoch: 3/20, step: 99120, training_loss: 2.21914
Epoch: 3/20, step: 99140, training_loss: 2.59424
Epoch: 3/20, step: 99160, training_loss: 1.89465
Epoch: 3/20, step: 99180, training_loss: 2.61380
Epoch: 3/20, step: 99200, training_loss: 2.79815
Epoch: 3/20, step: 99220, training_loss: 1.85703
Epoch: 3/20, step: 99240, training_loss: 2.47700
Epoch: 3/20, step: 99260, training_loss: 2.60920
Epoch: 3/20, step: 99280, training_loss: 2.34221
Epoch: 3/20, step: 99300, training_loss: 2.11679
Epoch: 3/20, step: 99320, training_loss: 3.20799
Epoch: 3/20, step: 99340, training_loss: 2.31649
Epoch: 3/20, step: 99360, training_loss: 2.00067
Epoch: 3/20, step: 99380, training_loss: 1.84687
Epoch: 3/20, step: 99400, training_loss: 2.58056
Epoch: 3/20, step: 99420, training_loss: 2.15903
Epoch: 3/20, step: 99440, training_loss: 2.81696
Epoch: 3/20, step: 99460, training_loss: 2.40857
Epoch: 3/20, step: 99480, training_loss: 3.07949
Epoch: 3/20, step: 99500, training_loss: 2.86343
Epoch: 3/20, step: 99520, training_loss: 2.41876
Epoch: 3/20, step: 99540, training_loss: 1.79333
Epoch: 3/20, step: 99560, training_loss: 1.91765
Epoch: 3/20, step: 99580, training_loss: 1.59863
Epoch: 3/20, step: 99600, training_loss: 1.56593
Epoch: 3/20, step: 99620, training_loss: 2.29567
Epoch: 3/20, step: 99640, training_loss: 2.43660
Epoch: 3/20, step: 99660, training_loss: 2.05055
Epoch: 3/20, step: 99680, training_loss: 1.78259
Epoch: 3/20, step: 99700, training_loss: 2.23574
Epoch: 3/20, step: 99720, training_loss: 2.46526
Epoch: 3/20, step: 99740, training_loss: 2.43719
Epoch: 3/20, step: 99760, training_loss: 2.12946
Epoch: 3/20, step: 99780, training_loss: 2.39484
Epoch: 3/20, step: 99800, training_loss: 2.52873
Epoch: 3/20, step: 99820, training_loss: 2.36336
Epoch: 3/20, step: 99840, training_loss: 2.36884
Epoch: 3/20, step: 99860, training_loss: 2.64523
Epoch: 3/20, step: 99880, training_loss: 2.22332
Epoch: 3/20, step: 99900, training_loss: 1.87779
Epoch: 3/20, step: 99920, training_loss: 2.60134
Epoch: 3/20, step: 99940, training_loss: 3.12037
Epoch: 3/20, step: 99960, training_loss: 2.49976
Epoch: 3/20, step: 99980, training_loss: 2.20714
Epoch: 3/20, step: 100000, training_loss: 2.08388
accuracy: 0.36, validation_loss: 2.2470617294311523, num_samples: 100
Epoch: 3/20, step: 100020, training_loss: 2.22498
Epoch: 3/20, step: 100040, training_loss: 1.95838
Epoch: 3/20, step: 100060, training_loss: 2.64598
Epoch: 3/20, step: 100080, training_loss: 1.50092
Epoch: 3/20, step: 100100, training_loss: 2.38360
Epoch: 3/20, step: 100120, training_loss: 2.22560
Epoch: 3/20, step: 100140, training_loss: 1.65662
Epoch: 3/20, step: 100160, training_loss: 1.75831
Epoch: 3/20, step: 100180, training_loss: 1.49282
Epoch: 3/20, step: 100200, training_loss: 1.70636
Epoch: 3/20, step: 100220, training_loss: 1.54789
Epoch: 3/20, step: 100240, training_loss: 1.29381
Epoch: 3/20, step: 100260, training_loss: 1.98832
Epoch: 3/20, step: 100280, training_loss: 2.02564
Epoch: 3/20, step: 100300, training_loss: 1.54195
Epoch: 3/20, step: 100320, training_loss: 1.82055
Epoch: 3/20, step: 100340, training_loss: 1.78896
Epoch: 3/20, step: 100360, training_loss: 1.75731
Epoch: 3/20, step: 100380, training_loss: 2.57446
Epoch: 3/20, step: 100400, training_loss: 1.88472
Epoch: 3/20, step: 100420, training_loss: 2.59134
Epoch: 3/20, step: 100440, training_loss: 1.79119
Epoch: 3/20, step: 100460, training_loss: 1.37127
Epoch: 3/20, step: 100480, training_loss: 1.66364
Epoch: 3/20, step: 100500, training_loss: 2.46231
Epoch: 3/20, step: 100520, training_loss: 2.43878
Epoch: 3/20, step: 100540, training_loss: 1.36151
Epoch: 3/20, step: 100560, training_loss: 2.00404
Epoch: 3/20, step: 100580, training_loss: 2.05974
Epoch: 3/20, step: 100600, training_loss: 3.02670
Epoch: 3/20, step: 100620, training_loss: 2.38603
Epoch: 3/20, step: 100640, training_loss: 2.32605
Epoch: 3/20, step: 100660, training_loss: 1.37996
Epoch: 3/20, step: 100680, training_loss: 2.96222
Epoch: 3/20, step: 100700, training_loss: 2.35552
Epoch: 3/20, step: 100720, training_loss: 2.27415
Epoch: 3/20, step: 100740, training_loss: 2.05808
Epoch: 3/20, step: 100760, training_loss: 2.52959
Epoch: 3/20, step: 100780, training_loss: 2.29579
Epoch: 3/20, step: 100800, training_loss: 1.80201
Epoch: 3/20, step: 100820, training_loss: 2.53591
Epoch: 3/20, step: 100840, training_loss: 1.98254
Epoch: 3/20, step: 100860, training_loss: 2.69871
Epoch: 3/20, step: 100880, training_loss: 2.05276
Epoch: 3/20, step: 100900, training_loss: 2.32971
Epoch: 3/20, step: 100920, training_loss: 2.36693
Epoch: 3/20, step: 100940, training_loss: 2.47079
Epoch: 3/20, step: 100960, training_loss: 1.37187
Epoch: 3/20, step: 100980, training_loss: 2.64852
Epoch: 3/20, step: 101000, training_loss: 2.02776
accuracy: 0.38, validation_loss: 2.059185743331909, num_samples: 100
Epoch: 3/20, step: 101020, training_loss: 2.35376
Epoch: 3/20, step: 101040, training_loss: 2.18868
Epoch: 3/20, step: 101060, training_loss: 2.56084
Epoch: 3/20, step: 101080, training_loss: 2.31071
Epoch: 3/20, step: 101100, training_loss: 1.95029
Epoch: 3/20, step: 101120, training_loss: 1.72406
Epoch: 3/20, step: 101140, training_loss: 1.51889
Epoch: 3/20, step: 101160, training_loss: 2.16948
Epoch: 3/20, step: 101180, training_loss: 2.36546
Epoch: 3/20, step: 101200, training_loss: 1.78520
Epoch: 3/20, step: 101220, training_loss: 2.08864
Epoch: 3/20, step: 101240, training_loss: 2.09709
Epoch: 3/20, step: 101260, training_loss: 1.59665
Epoch: 3/20, step: 101280, training_loss: 1.88232
Epoch: 3/20, step: 101300, training_loss: 1.26899
Epoch: 3/20, step: 101320, training_loss: 1.74360
Epoch: 3/20, step: 101340, training_loss: 2.25385
Epoch: 3/20, step: 101360, training_loss: 2.85216
Epoch: 3/20, step: 101380, training_loss: 2.18446
Epoch: 3/20, step: 101400, training_loss: 2.03846
Epoch: 3/20, step: 101420, training_loss: 1.76313
Epoch: 3/20, step: 101440, training_loss: 2.32829
Epoch: 3/20, step: 101460, training_loss: 2.66434
Epoch: 3/20, step: 101480, training_loss: 2.22174
Epoch: 3/20, step: 101500, training_loss: 1.62987
Epoch: 3/20, step: 101520, training_loss: 2.13795
Epoch: 3/20, step: 101540, training_loss: 1.52195
Epoch: 3/20, step: 101560, training_loss: 1.84278
Epoch: 3/20, step: 101580, training_loss: 3.00159
Epoch: 3/20, step: 101600, training_loss: 2.47989
Epoch: 3/20, step: 101620, training_loss: 2.18998
Epoch: 3/20, step: 101640, training_loss: 1.98836
Epoch: 3/20, step: 101660, training_loss: 2.29985
Epoch: 3/20, step: 101680, training_loss: 2.16643
Epoch: 3/20, step: 101700, training_loss: 2.38024
Epoch: 3/20, step: 101720, training_loss: 2.41271
Epoch: 3/20, step: 101740, training_loss: 2.11813
Epoch: 3/20, step: 101760, training_loss: 1.39056
Epoch: 3/20, step: 101780, training_loss: 2.58910
Epoch: 3/20, step: 101800, training_loss: 1.83089
Epoch: 3/20, step: 101820, training_loss: 1.26271
Epoch: 3/20, step: 101840, training_loss: 2.48918
Epoch: 3/20, step: 101860, training_loss: 2.81902
Epoch: 3/20, step: 101880, training_loss: 2.79012
Epoch: 3/20, step: 101900, training_loss: 0.89718
Epoch: 3/20, step: 101920, training_loss: 2.60092
Epoch: 3/20, step: 101940, training_loss: 1.86910
Epoch: 3/20, step: 101960, training_loss: 3.10906
Epoch: 3/20, step: 101980, training_loss: 1.86881
Epoch: 3/20, step: 102000, training_loss: 2.39679
accuracy: 0.45, validation_loss: 2.028596878051758, num_samples: 100
Epoch: 3/20, step: 102020, training_loss: 1.66517
Epoch: 3/20, step: 102040, training_loss: 1.66779
Epoch: 3/20, step: 102060, training_loss: 1.75000
Epoch: 3/20, step: 102080, training_loss: 2.06100
Epoch: 3/20, step: 102100, training_loss: 2.18482
Epoch: 3/20, step: 102120, training_loss: 1.93378
Epoch: 3/20, step: 102140, training_loss: 2.00956
Epoch: 3/20, step: 102160, training_loss: 2.20406
Epoch: 3/20, step: 102180, training_loss: 2.32857
Epoch: 3/20, step: 102200, training_loss: 1.84208
Epoch: 3/20, step: 102220, training_loss: 2.24096
Epoch: 3/20, step: 102240, training_loss: 2.06166
Epoch: 3/20, step: 102260, training_loss: 2.20613
Epoch: 3/20, step: 102280, training_loss: 1.68229
Epoch: 3/20, step: 102300, training_loss: 2.77884
Epoch: 3/20, step: 102320, training_loss: 2.47371
Epoch: 3/20, step: 102340, training_loss: 1.97115
Epoch: 3/20, step: 102360, training_loss: 2.80383
Epoch: 3/20, step: 102380, training_loss: 3.06566
Epoch: 3/20, step: 102400, training_loss: 2.52591
Epoch: 3/20, step: 102420, training_loss: 2.62621
Epoch: 3/20, step: 102440, training_loss: 1.97835
Epoch: 3/20, step: 102460, training_loss: 2.52843
Epoch: 3/20, step: 102480, training_loss: 1.46928
Epoch: 3/20, step: 102500, training_loss: 1.94732
Epoch: 3/20, step: 102520, training_loss: 2.74605
Epoch: 3/20, step: 102540, training_loss: 2.21553
Epoch: 3/20, step: 102560, training_loss: 2.27977
Epoch: 3/20, step: 102580, training_loss: 1.32497
Epoch: 3/20, step: 102600, training_loss: 1.37293
Epoch: 3/20, step: 102620, training_loss: 2.83368
Epoch: 3/20, step: 102640, training_loss: 2.37964
Epoch: 4/20, step: 20, training_loss: 1.47048
Epoch: 4/20, step: 40, training_loss: 1.81946
Epoch: 4/20, step: 60, training_loss: 1.88527
Epoch: 4/20, step: 80, training_loss: 1.77485
Epoch: 4/20, step: 100, training_loss: 2.33747
Epoch: 4/20, step: 120, training_loss: 1.51766
Epoch: 4/20, step: 140, training_loss: 1.89358
Epoch: 4/20, step: 160, training_loss: 1.92251
Epoch: 4/20, step: 180, training_loss: 1.21980
Epoch: 4/20, step: 200, training_loss: 2.01443
Epoch: 4/20, step: 220, training_loss: 1.37738
Epoch: 4/20, step: 240, training_loss: 2.50705
Epoch: 4/20, step: 260, training_loss: 1.48029
Epoch: 4/20, step: 280, training_loss: 2.21794
Epoch: 4/20, step: 300, training_loss: 2.15037
Epoch: 4/20, step: 320, training_loss: 1.72057
Epoch: 4/20, step: 340, training_loss: 0.98944
Epoch: 4/20, step: 360, training_loss: 2.68941
Epoch: 4/20, step: 380, training_loss: 2.96683
Epoch: 4/20, step: 400, training_loss: 3.00746
Epoch: 4/20, step: 420, training_loss: 2.96628
Epoch: 4/20, step: 440, training_loss: 1.76383
Epoch: 4/20, step: 460, training_loss: 2.59425
Epoch: 4/20, step: 480, training_loss: 2.10345
Epoch: 4/20, step: 500, training_loss: 2.57960
Epoch: 4/20, step: 520, training_loss: 1.98808
Epoch: 4/20, step: 540, training_loss: 1.59630
Epoch: 4/20, step: 560, training_loss: 1.81851
Epoch: 4/20, step: 580, training_loss: 2.54469
Epoch: 4/20, step: 600, training_loss: 2.41032
Epoch: 4/20, step: 620, training_loss: 2.01194
Epoch: 4/20, step: 640, training_loss: 1.36803
Epoch: 4/20, step: 660, training_loss: 2.19780
Epoch: 4/20, step: 680, training_loss: 1.86718
Epoch: 4/20, step: 700, training_loss: 2.95146
Epoch: 4/20, step: 720, training_loss: 1.99827
Epoch: 4/20, step: 740, training_loss: 1.37787
Epoch: 4/20, step: 760, training_loss: 2.05268
Epoch: 4/20, step: 780, training_loss: 2.24645
Epoch: 4/20, step: 800, training_loss: 2.66555
Epoch: 4/20, step: 820, training_loss: 1.67745
Epoch: 4/20, step: 840, training_loss: 2.81241
Epoch: 4/20, step: 860, training_loss: 2.55970
Epoch: 4/20, step: 880, training_loss: 2.21151
Epoch: 4/20, step: 900, training_loss: 2.10505
Epoch: 4/20, step: 920, training_loss: 2.34860
Epoch: 4/20, step: 940, training_loss: 2.85964
Epoch: 4/20, step: 960, training_loss: 2.63071
Epoch: 4/20, step: 980, training_loss: 1.90610
Epoch: 4/20, step: 1000, training_loss: 2.71076
accuracy: 0.34, validation_loss: 2.18280029296875, num_samples: 100
Epoch: 4/20, step: 1020, training_loss: 2.18447
Epoch: 4/20, step: 1040, training_loss: 1.25995
Epoch: 4/20, step: 1060, training_loss: 2.34638
Epoch: 4/20, step: 1080, training_loss: 1.48529
Epoch: 4/20, step: 1100, training_loss: 2.42695
Epoch: 4/20, step: 1120, training_loss: 2.48482
Epoch: 4/20, step: 1140, training_loss: 2.55635
Epoch: 4/20, step: 1160, training_loss: 1.74221
Epoch: 4/20, step: 1180, training_loss: 1.16467
Epoch: 4/20, step: 1200, training_loss: 3.24526
Epoch: 4/20, step: 1220, training_loss: 2.15431
Epoch: 4/20, step: 1240, training_loss: 2.13330
Epoch: 4/20, step: 1260, training_loss: 1.59003
Epoch: 4/20, step: 1280, training_loss: 1.69848
Epoch: 4/20, step: 1300, training_loss: 2.32171
Epoch: 4/20, step: 1320, training_loss: 2.57436
Epoch: 4/20, step: 1340, training_loss: 1.37738
Epoch: 4/20, step: 1360, training_loss: 1.54701
Epoch: 4/20, step: 1380, training_loss: 2.03480
Epoch: 4/20, step: 1400, training_loss: 2.02689
Epoch: 4/20, step: 1420, training_loss: 1.61145
Epoch: 4/20, step: 1440, training_loss: 2.84350
Epoch: 4/20, step: 1460, training_loss: 1.99452
Epoch: 4/20, step: 1480, training_loss: 2.23628
Epoch: 4/20, step: 1500, training_loss: 2.18871
Epoch: 4/20, step: 1520, training_loss: 2.30334
Epoch: 4/20, step: 1540, training_loss: 1.58619
Epoch: 4/20, step: 1560, training_loss: 1.58553
Epoch: 4/20, step: 1580, training_loss: 2.30718
Epoch: 4/20, step: 1600, training_loss: 1.72350
Epoch: 4/20, step: 1620, training_loss: 2.33226
Epoch: 4/20, step: 1640, training_loss: 1.80815
Epoch: 4/20, step: 1660, training_loss: 3.17643
Epoch: 4/20, step: 1680, training_loss: 2.61624
Epoch: 4/20, step: 1700, training_loss: 2.68965
Epoch: 4/20, step: 1720, training_loss: 3.24325
Epoch: 4/20, step: 1740, training_loss: 2.54805
Epoch: 4/20, step: 1760, training_loss: 2.65272
Epoch: 4/20, step: 1780, training_loss: 1.50536
Epoch: 4/20, step: 1800, training_loss: 2.16220
Epoch: 4/20, step: 1820, training_loss: 2.19164
Epoch: 4/20, step: 1840, training_loss: 2.38774
Epoch: 4/20, step: 1860, training_loss: 1.82051
Epoch: 4/20, step: 1880, training_loss: 3.31754
Epoch: 4/20, step: 1900, training_loss: 2.19095
Epoch: 4/20, step: 1920, training_loss: 3.01633
Epoch: 4/20, step: 1940, training_loss: 2.34829
Epoch: 4/20, step: 1960, training_loss: 2.41848
Epoch: 4/20, step: 1980, training_loss: 1.46893
Epoch: 4/20, step: 2000, training_loss: 1.76636
accuracy: 0.43, validation_loss: 2.3284049034118652, num_samples: 100
Epoch: 4/20, step: 2020, training_loss: 2.02655
Epoch: 4/20, step: 2040, training_loss: 1.65906
Epoch: 4/20, step: 2060, training_loss: 2.47972
Epoch: 4/20, step: 2080, training_loss: 1.85830
Epoch: 4/20, step: 2100, training_loss: 2.32654
Epoch: 4/20, step: 2120, training_loss: 2.34799
Epoch: 4/20, step: 2140, training_loss: 2.03578
Epoch: 4/20, step: 2160, training_loss: 2.01127
Epoch: 4/20, step: 2180, training_loss: 1.93658
Epoch: 4/20, step: 2200, training_loss: 1.43995
Epoch: 4/20, step: 2220, training_loss: 1.82615
Epoch: 4/20, step: 2240, training_loss: 2.70978
Epoch: 4/20, step: 2260, training_loss: 2.37071
Epoch: 4/20, step: 2280, training_loss: 3.03636
Epoch: 4/20, step: 2300, training_loss: 2.16825
Epoch: 4/20, step: 2320, training_loss: 1.99789
Epoch: 4/20, step: 2340, training_loss: 2.19536
Epoch: 4/20, step: 2360, training_loss: 1.30735
Epoch: 4/20, step: 2380, training_loss: 2.61654
Epoch: 4/20, step: 2400, training_loss: 2.15660
Epoch: 4/20, step: 2420, training_loss: 2.83588
Epoch: 4/20, step: 2440, training_loss: 1.71595
Epoch: 4/20, step: 2460, training_loss: 1.72810
Epoch: 4/20, step: 2480, training_loss: 2.14751
Epoch: 4/20, step: 2500, training_loss: 2.35075
Epoch: 4/20, step: 2520, training_loss: 2.85796
Epoch: 4/20, step: 2540, training_loss: 2.41200
Epoch: 4/20, step: 2560, training_loss: 2.51353
Epoch: 4/20, step: 2580, training_loss: 1.61981
Epoch: 4/20, step: 2600, training_loss: 2.39654
Epoch: 4/20, step: 2620, training_loss: 2.36616
Epoch: 4/20, step: 2640, training_loss: 1.69872
Epoch: 4/20, step: 2660, training_loss: 2.55178
Epoch: 4/20, step: 2680, training_loss: 1.69820
Epoch: 4/20, step: 2700, training_loss: 2.58717
Epoch: 4/20, step: 2720, training_loss: 2.16200
Epoch: 4/20, step: 2740, training_loss: 2.02119
Epoch: 4/20, step: 2760, training_loss: 1.64643
Epoch: 4/20, step: 2780, training_loss: 2.46279
Epoch: 4/20, step: 2800, training_loss: 1.91610
Epoch: 4/20, step: 2820, training_loss: 2.50713
Epoch: 4/20, step: 2840, training_loss: 2.45835
Epoch: 4/20, step: 2860, training_loss: 2.20368
Epoch: 4/20, step: 2880, training_loss: 2.52394
Epoch: 4/20, step: 2900, training_loss: 2.43209
Epoch: 4/20, step: 2920, training_loss: 2.19310
Epoch: 4/20, step: 2940, training_loss: 2.35286
Epoch: 4/20, step: 2960, training_loss: 2.09133
Epoch: 4/20, step: 2980, training_loss: 1.64170
Epoch: 4/20, step: 3000, training_loss: 1.77615
accuracy: 0.35, validation_loss: 2.230950355529785, num_samples: 100
Epoch: 4/20, step: 3020, training_loss: 2.37131
Epoch: 4/20, step: 3040, training_loss: 2.06325
Epoch: 4/20, step: 3060, training_loss: 2.56033
Epoch: 4/20, step: 3080, training_loss: 1.49475
Epoch: 4/20, step: 3100, training_loss: 1.67362
Epoch: 4/20, step: 3120, training_loss: 1.63440
Epoch: 4/20, step: 3140, training_loss: 2.01820
Epoch: 4/20, step: 3160, training_loss: 1.79780
Epoch: 4/20, step: 3180, training_loss: 1.99702
Epoch: 4/20, step: 3200, training_loss: 2.27266
Epoch: 4/20, step: 3220, training_loss: 2.38866
Epoch: 4/20, step: 3240, training_loss: 1.65296
Epoch: 4/20, step: 3260, training_loss: 3.22630
Epoch: 4/20, step: 3280, training_loss: 2.30468
Epoch: 4/20, step: 3300, training_loss: 1.71140
Epoch: 4/20, step: 3320, training_loss: 2.10704
Epoch: 4/20, step: 3340, training_loss: 1.86927
Epoch: 4/20, step: 3360, training_loss: 2.07582
Epoch: 4/20, step: 3380, training_loss: 2.51208
Epoch: 4/20, step: 3400, training_loss: 2.19925
Epoch: 4/20, step: 3420, training_loss: 1.78243
Epoch: 4/20, step: 3440, training_loss: 2.18275
Epoch: 4/20, step: 3460, training_loss: 1.66138
Epoch: 4/20, step: 3480, training_loss: 1.63788
Epoch: 4/20, step: 3500, training_loss: 3.15206
Epoch: 4/20, step: 3520, training_loss: 1.66785
Epoch: 4/20, step: 3540, training_loss: 1.46824
Epoch: 4/20, step: 3560, training_loss: 1.72113
Epoch: 4/20, step: 3580, training_loss: 1.96977
Epoch: 4/20, step: 3600, training_loss: 2.73987
Epoch: 4/20, step: 3620, training_loss: 1.95765
Epoch: 4/20, step: 3640, training_loss: 1.51682
Epoch: 4/20, step: 3660, training_loss: 2.81493
Epoch: 4/20, step: 3680, training_loss: 1.82510
Epoch: 4/20, step: 3700, training_loss: 2.09342
Epoch: 4/20, step: 3720, training_loss: 2.83034
Epoch: 4/20, step: 3740, training_loss: 1.85115
Epoch: 4/20, step: 3760, training_loss: 1.90317
Epoch: 4/20, step: 3780, training_loss: 2.16691
Epoch: 4/20, step: 3800, training_loss: 1.74039
Epoch: 4/20, step: 3820, training_loss: 1.68994
Epoch: 4/20, step: 3840, training_loss: 1.93251
Epoch: 4/20, step: 3860, training_loss: 2.34986
Epoch: 4/20, step: 3880, training_loss: 2.12316
Epoch: 4/20, step: 3900, training_loss: 1.56626
Epoch: 4/20, step: 3920, training_loss: 1.65948
Epoch: 4/20, step: 3940, training_loss: 2.80738
Epoch: 4/20, step: 3960, training_loss: 2.55151
Epoch: 4/20, step: 3980, training_loss: 1.39977
Epoch: 4/20, step: 4000, training_loss: 2.40866
accuracy: 0.42, validation_loss: 2.1780896186828613, num_samples: 100
Epoch: 4/20, step: 4020, training_loss: 2.11193
Epoch: 4/20, step: 4040, training_loss: 1.69606
Epoch: 4/20, step: 4060, training_loss: 1.96223
Epoch: 4/20, step: 4080, training_loss: 2.77518
Epoch: 4/20, step: 4100, training_loss: 1.98074
Epoch: 4/20, step: 4120, training_loss: 1.46617
Epoch: 4/20, step: 4140, training_loss: 2.58573
Epoch: 4/20, step: 4160, training_loss: 0.94239
Epoch: 4/20, step: 4180, training_loss: 1.65250
Epoch: 4/20, step: 4200, training_loss: 2.81753
Epoch: 4/20, step: 4220, training_loss: 1.80110
Epoch: 4/20, step: 4240, training_loss: 1.99746
Epoch: 4/20, step: 4260, training_loss: 2.57792
Epoch: 4/20, step: 4280, training_loss: 1.71025
Epoch: 4/20, step: 4300, training_loss: 2.31589
Epoch: 4/20, step: 4320, training_loss: 2.03547
Epoch: 4/20, step: 4340, training_loss: 1.40695
Epoch: 4/20, step: 4360, training_loss: 2.01738
Epoch: 4/20, step: 4380, training_loss: 2.53750
Epoch: 4/20, step: 4400, training_loss: 1.19472
Epoch: 4/20, step: 4420, training_loss: 2.27924
Epoch: 4/20, step: 4440, training_loss: 2.80821
Epoch: 4/20, step: 4460, training_loss: 1.91666
Epoch: 4/20, step: 4480, training_loss: 2.07977
Epoch: 4/20, step: 4500, training_loss: 2.20523
Epoch: 4/20, step: 4520, training_loss: 3.06866
Epoch: 4/20, step: 4540, training_loss: 2.64392
Epoch: 4/20, step: 4560, training_loss: 2.13656
Epoch: 4/20, step: 4580, training_loss: 1.95498
Epoch: 4/20, step: 4600, training_loss: 2.19387
Epoch: 4/20, step: 4620, training_loss: 2.24750
Epoch: 4/20, step: 4640, training_loss: 2.42337
Epoch: 4/20, step: 4660, training_loss: 2.93113
Epoch: 4/20, step: 4680, training_loss: 2.70965
Epoch: 4/20, step: 4700, training_loss: 1.60006
Epoch: 4/20, step: 4720, training_loss: 1.86227
Epoch: 4/20, step: 4740, training_loss: 2.31966
Epoch: 4/20, step: 4760, training_loss: 2.50245
Epoch: 4/20, step: 4780, training_loss: 2.16311
Epoch: 4/20, step: 4800, training_loss: 1.93348
Epoch: 4/20, step: 4820, training_loss: 1.98838
Epoch: 4/20, step: 4840, training_loss: 2.37680
Epoch: 4/20, step: 4860, training_loss: 2.39912
Epoch: 4/20, step: 4880, training_loss: 1.76516
Epoch: 4/20, step: 4900, training_loss: 2.33351
Epoch: 4/20, step: 4920, training_loss: 2.46026
Epoch: 4/20, step: 4940, training_loss: 2.69392
Epoch: 4/20, step: 4960, training_loss: 1.70698
Epoch: 4/20, step: 4980, training_loss: 0.83179
Epoch: 4/20, step: 5000, training_loss: 2.01815
accuracy: 0.36, validation_loss: 2.339594841003418, num_samples: 100
Epoch: 4/20, step: 5020, training_loss: 1.61047
Epoch: 4/20, step: 5040, training_loss: 2.22596
Epoch: 4/20, step: 5060, training_loss: 2.13534
Epoch: 4/20, step: 5080, training_loss: 2.49144
Epoch: 4/20, step: 5100, training_loss: 2.20903
Epoch: 4/20, step: 5120, training_loss: 2.51841
Epoch: 4/20, step: 5140, training_loss: 2.46479
Epoch: 4/20, step: 5160, training_loss: 2.47633
Epoch: 4/20, step: 5180, training_loss: 1.61001
Epoch: 4/20, step: 5200, training_loss: 1.78535
Epoch: 4/20, step: 5220, training_loss: 2.40120
Epoch: 4/20, step: 5240, training_loss: 2.26068
Epoch: 4/20, step: 5260, training_loss: 2.58112
Epoch: 4/20, step: 5280, training_loss: 2.53866
Epoch: 4/20, step: 5300, training_loss: 2.75291
Epoch: 4/20, step: 5320, training_loss: 2.21554
Epoch: 4/20, step: 5340, training_loss: 2.46032
Epoch: 4/20, step: 5360, training_loss: 2.22422
Epoch: 4/20, step: 5380, training_loss: 1.71993
Epoch: 4/20, step: 5400, training_loss: 2.78637
Epoch: 4/20, step: 5420, training_loss: 1.76028
Epoch: 4/20, step: 5440, training_loss: 2.38764
Epoch: 4/20, step: 5460, training_loss: 2.62846
Epoch: 4/20, step: 5480, training_loss: 1.41266
Epoch: 4/20, step: 5500, training_loss: 1.85763
Epoch: 4/20, step: 5520, training_loss: 2.85352
Epoch: 4/20, step: 5540, training_loss: 3.05571
Epoch: 4/20, step: 5560, training_loss: 2.70148
Epoch: 4/20, step: 5580, training_loss: 1.83821
Epoch: 4/20, step: 5600, training_loss: 1.65699
Epoch: 4/20, step: 5620, training_loss: 2.42456
Epoch: 4/20, step: 5640, training_loss: 0.85726
Epoch: 4/20, step: 5660, training_loss: 2.12119
Epoch: 4/20, step: 5680, training_loss: 2.16162
Epoch: 4/20, step: 5700, training_loss: 1.89632
Epoch: 4/20, step: 5720, training_loss: 2.90747
Epoch: 4/20, step: 5740, training_loss: 1.97462
Epoch: 4/20, step: 5760, training_loss: 1.88143
Epoch: 4/20, step: 5780, training_loss: 2.58494
Epoch: 4/20, step: 5800, training_loss: 2.76288
Epoch: 4/20, step: 5820, training_loss: 1.79889
Epoch: 4/20, step: 5840, training_loss: 2.86089
Epoch: 4/20, step: 5860, training_loss: 2.48013
Epoch: 4/20, step: 5880, training_loss: 1.30053
Epoch: 4/20, step: 5900, training_loss: 2.21404
Epoch: 4/20, step: 5920, training_loss: 1.67242
Epoch: 4/20, step: 5940, training_loss: 1.88400
Epoch: 4/20, step: 5960, training_loss: 2.30969
Epoch: 4/20, step: 5980, training_loss: 2.25634
Epoch: 4/20, step: 6000, training_loss: 2.07878
accuracy: 0.43, validation_loss: 2.1974000930786133, num_samples: 100
Epoch: 4/20, step: 6020, training_loss: 1.43248
Epoch: 4/20, step: 6040, training_loss: 2.09956
Epoch: 4/20, step: 6060, training_loss: 1.71559
Epoch: 4/20, step: 6080, training_loss: 1.93037
Epoch: 4/20, step: 6100, training_loss: 2.23102
Epoch: 4/20, step: 6120, training_loss: 1.92465
Epoch: 4/20, step: 6140, training_loss: 1.52237
Epoch: 4/20, step: 6160, training_loss: 2.53239
Epoch: 4/20, step: 6180, training_loss: 2.63675
Epoch: 4/20, step: 6200, training_loss: 1.99420
Epoch: 4/20, step: 6220, training_loss: 2.13408
Epoch: 4/20, step: 6240, training_loss: 2.13908
Epoch: 4/20, step: 6260, training_loss: 2.27542
Epoch: 4/20, step: 6280, training_loss: 1.76052
Epoch: 4/20, step: 6300, training_loss: 1.75841
Epoch: 4/20, step: 6320, training_loss: 2.76927
Epoch: 4/20, step: 6340, training_loss: 1.44814
Epoch: 4/20, step: 6360, training_loss: 2.57311
Epoch: 4/20, step: 6380, training_loss: 2.50780
Epoch: 4/20, step: 6400, training_loss: 1.94366
Epoch: 4/20, step: 6420, training_loss: 1.88822
Epoch: 4/20, step: 6440, training_loss: 1.61564
Epoch: 4/20, step: 6460, training_loss: 2.84054
Epoch: 4/20, step: 6480, training_loss: 1.28171
Epoch: 4/20, step: 6500, training_loss: 1.47136
Epoch: 4/20, step: 6520, training_loss: 1.54832
Epoch: 4/20, step: 6540, training_loss: 2.48392
Epoch: 4/20, step: 6560, training_loss: 2.16668
Epoch: 4/20, step: 6580, training_loss: 1.66279
Epoch: 4/20, step: 6600, training_loss: 2.50868
Epoch: 4/20, step: 6620, training_loss: 2.50166
Epoch: 4/20, step: 6640, training_loss: 2.54193
Epoch: 4/20, step: 6660, training_loss: 1.75143
Epoch: 4/20, step: 6680, training_loss: 1.91022
Epoch: 4/20, step: 6700, training_loss: 1.30540
Epoch: 4/20, step: 6720, training_loss: 1.66893
Epoch: 4/20, step: 6740, training_loss: 2.08298
Epoch: 4/20, step: 6760, training_loss: 2.10742
Epoch: 4/20, step: 6780, training_loss: 1.69330
Epoch: 4/20, step: 6800, training_loss: 1.70450
Epoch: 4/20, step: 6820, training_loss: 2.44551
Epoch: 4/20, step: 6840, training_loss: 2.02379
Epoch: 4/20, step: 6860, training_loss: 1.59042
Epoch: 4/20, step: 6880, training_loss: 2.03603
Epoch: 4/20, step: 6900, training_loss: 2.15656
Epoch: 4/20, step: 6920, training_loss: 1.97691
Epoch: 4/20, step: 6940, training_loss: 2.73986
Epoch: 4/20, step: 6960, training_loss: 1.66547
Epoch: 4/20, step: 6980, training_loss: 1.88460
Epoch: 4/20, step: 7000, training_loss: 2.14026
accuracy: 0.37, validation_loss: 2.191413164138794, num_samples: 100
Epoch: 4/20, step: 7020, training_loss: 2.12387
Epoch: 4/20, step: 7040, training_loss: 2.65701
Epoch: 4/20, step: 7060, training_loss: 2.06892
Epoch: 4/20, step: 7080, training_loss: 2.53393
Epoch: 4/20, step: 7100, training_loss: 1.91896
Epoch: 4/20, step: 7120, training_loss: 2.51324
Epoch: 4/20, step: 7140, training_loss: 2.02292
Epoch: 4/20, step: 7160, training_loss: 1.83817
Epoch: 4/20, step: 7180, training_loss: 2.54307
Epoch: 4/20, step: 7200, training_loss: 1.91475
Epoch: 4/20, step: 7220, training_loss: 2.24520
Epoch: 4/20, step: 7240, training_loss: 2.20146
Epoch: 4/20, step: 7260, training_loss: 2.21321
Epoch: 4/20, step: 7280, training_loss: 2.65347
Epoch: 4/20, step: 7300, training_loss: 2.49679
Epoch: 4/20, step: 7320, training_loss: 2.02271
Epoch: 4/20, step: 7340, training_loss: 2.68169
Epoch: 4/20, step: 7360, training_loss: 2.21609
Epoch: 4/20, step: 7380, training_loss: 2.03040
Epoch: 4/20, step: 7400, training_loss: 1.68836
Epoch: 4/20, step: 7420, training_loss: 1.92907
Epoch: 4/20, step: 7440, training_loss: 2.35914
Epoch: 4/20, step: 7460, training_loss: 2.39499
Epoch: 4/20, step: 7480, training_loss: 2.00515
Epoch: 4/20, step: 7500, training_loss: 2.06353
Epoch: 4/20, step: 7520, training_loss: 2.16063
Epoch: 4/20, step: 7540, training_loss: 1.99722
Epoch: 4/20, step: 7560, training_loss: 1.78049
Epoch: 4/20, step: 7580, training_loss: 2.45946
Epoch: 4/20, step: 7600, training_loss: 2.67296
Epoch: 4/20, step: 7620, training_loss: 2.41629
Epoch: 4/20, step: 7640, training_loss: 2.11071
Epoch: 4/20, step: 7660, training_loss: 1.66905
Epoch: 4/20, step: 7680, training_loss: 1.97361
Epoch: 4/20, step: 7700, training_loss: 2.20773
Epoch: 4/20, step: 7720, training_loss: 2.70008
Epoch: 4/20, step: 7740, training_loss: 2.39029
Epoch: 4/20, step: 7760, training_loss: 1.93404
Epoch: 4/20, step: 7780, training_loss: 2.01103
Epoch: 4/20, step: 7800, training_loss: 2.14092
Epoch: 4/20, step: 7820, training_loss: 1.89040
Epoch: 4/20, step: 7840, training_loss: 1.93367
Epoch: 4/20, step: 7860, training_loss: 2.34672
Epoch: 4/20, step: 7880, training_loss: 1.87662
Epoch: 4/20, step: 7900, training_loss: 1.95415
Epoch: 4/20, step: 7920, training_loss: 2.95883
Epoch: 4/20, step: 7940, training_loss: 2.07392
Epoch: 4/20, step: 7960, training_loss: 2.32169
Epoch: 4/20, step: 7980, training_loss: 2.56854
Epoch: 4/20, step: 8000, training_loss: 2.81955
accuracy: 0.41, validation_loss: 2.239680767059326, num_samples: 100
Epoch: 4/20, step: 8020, training_loss: 1.76535
Epoch: 4/20, step: 8040, training_loss: 2.55464
Epoch: 4/20, step: 8060, training_loss: 2.07776
Epoch: 4/20, step: 8080, training_loss: 2.75308
Epoch: 4/20, step: 8100, training_loss: 3.05358
Epoch: 4/20, step: 8120, training_loss: 2.24149
Epoch: 4/20, step: 8140, training_loss: 2.41992
Epoch: 4/20, step: 8160, training_loss: 2.00605
Epoch: 4/20, step: 8180, training_loss: 3.11524
Epoch: 4/20, step: 8200, training_loss: 2.14966
Epoch: 4/20, step: 8220, training_loss: 2.45112
Epoch: 4/20, step: 8240, training_loss: 2.56747
Epoch: 4/20, step: 8260, training_loss: 2.69029
Epoch: 4/20, step: 8280, training_loss: 1.77212
Epoch: 4/20, step: 8300, training_loss: 2.22412
Epoch: 4/20, step: 8320, training_loss: 2.41341
Epoch: 4/20, step: 8340, training_loss: 2.35669
Epoch: 4/20, step: 8360, training_loss: 2.81015
Epoch: 4/20, step: 8380, training_loss: 1.74970
Epoch: 4/20, step: 8400, training_loss: 2.00832
Epoch: 4/20, step: 8420, training_loss: 2.40807
Epoch: 4/20, step: 8440, training_loss: 1.82163
Epoch: 4/20, step: 8460, training_loss: 2.36379
Epoch: 4/20, step: 8480, training_loss: 1.52076
Epoch: 4/20, step: 8500, training_loss: 1.76925
Epoch: 4/20, step: 8520, training_loss: 2.24812
Epoch: 4/20, step: 8540, training_loss: 2.37556
Epoch: 4/20, step: 8560, training_loss: 2.01848
Epoch: 4/20, step: 8580, training_loss: 2.28179
Epoch: 4/20, step: 8600, training_loss: 1.24578
Epoch: 4/20, step: 8620, training_loss: 1.90392
Epoch: 4/20, step: 8640, training_loss: 2.16558
Epoch: 4/20, step: 8660, training_loss: 1.86597
Epoch: 4/20, step: 8680, training_loss: 1.41957
Epoch: 4/20, step: 8700, training_loss: 1.80609
Epoch: 4/20, step: 8720, training_loss: 2.56076
Epoch: 4/20, step: 8740, training_loss: 2.68471
Epoch: 4/20, step: 8760, training_loss: 2.07142
Epoch: 4/20, step: 8780, training_loss: 1.75263
Epoch: 4/20, step: 8800, training_loss: 2.34940
Epoch: 4/20, step: 8820, training_loss: 2.68833
Epoch: 4/20, step: 8840, training_loss: 2.44008
Epoch: 4/20, step: 8860, training_loss: 1.74277
Epoch: 4/20, step: 8880, training_loss: 1.40669
Epoch: 4/20, step: 8900, training_loss: 2.73506
Epoch: 4/20, step: 8920, training_loss: 1.84269
Epoch: 4/20, step: 8940, training_loss: 2.24613
Epoch: 4/20, step: 8960, training_loss: 2.13445
Epoch: 4/20, step: 8980, training_loss: 1.81079
Epoch: 4/20, step: 9000, training_loss: 2.02119
accuracy: 0.49, validation_loss: 1.9383505582809448, num_samples: 100
Epoch: 4/20, step: 9020, training_loss: 2.18170
Epoch: 4/20, step: 9040, training_loss: 2.06660
Epoch: 4/20, step: 9060, training_loss: 2.58148
Epoch: 4/20, step: 9080, training_loss: 1.96486
Epoch: 4/20, step: 9100, training_loss: 2.27714
Epoch: 4/20, step: 9120, training_loss: 1.80784
Epoch: 4/20, step: 9140, training_loss: 2.59181
Epoch: 4/20, step: 9160, training_loss: 1.74015
Epoch: 4/20, step: 9180, training_loss: 2.17342
Epoch: 4/20, step: 9200, training_loss: 2.35016
Epoch: 4/20, step: 9220, training_loss: 2.22539
Epoch: 4/20, step: 9240, training_loss: 2.11549
Epoch: 4/20, step: 9260, training_loss: 2.07844
Epoch: 4/20, step: 9280, training_loss: 1.75365
Epoch: 4/20, step: 9300, training_loss: 2.16040
Epoch: 4/20, step: 9320, training_loss: 1.68408
Epoch: 4/20, step: 9340, training_loss: 2.75541
Epoch: 4/20, step: 9360, training_loss: 2.16905
Epoch: 4/20, step: 9380, training_loss: 2.68292
Epoch: 4/20, step: 9400, training_loss: 1.91385
Epoch: 4/20, step: 9420, training_loss: 1.81023
Epoch: 4/20, step: 9440, training_loss: 1.88155
Epoch: 4/20, step: 9460, training_loss: 2.16036
Epoch: 4/20, step: 9480, training_loss: 2.15288
Epoch: 4/20, step: 9500, training_loss: 1.82105
Epoch: 4/20, step: 9520, training_loss: 2.51149
Epoch: 4/20, step: 9540, training_loss: 2.45817
Epoch: 4/20, step: 9560, training_loss: 1.77392
Epoch: 4/20, step: 9580, training_loss: 2.43031
Epoch: 4/20, step: 9600, training_loss: 2.34589
Epoch: 4/20, step: 9620, training_loss: 2.27742
Epoch: 4/20, step: 9640, training_loss: 2.27534
Epoch: 4/20, step: 9660, training_loss: 1.43557
Epoch: 4/20, step: 9680, training_loss: 2.11952
Epoch: 4/20, step: 9700, training_loss: 2.73716
Epoch: 4/20, step: 9720, training_loss: 2.11592
Epoch: 4/20, step: 9740, training_loss: 2.52809
Epoch: 4/20, step: 9760, training_loss: 2.63250
Epoch: 4/20, step: 9780, training_loss: 2.75161
Epoch: 4/20, step: 9800, training_loss: 1.61563
Epoch: 4/20, step: 9820, training_loss: 2.99432
Epoch: 4/20, step: 9840, training_loss: 2.49754
Epoch: 4/20, step: 9860, training_loss: 2.33229
Epoch: 4/20, step: 9880, training_loss: 2.29935
Epoch: 4/20, step: 9900, training_loss: 2.41460
Epoch: 4/20, step: 9920, training_loss: 2.17150
Epoch: 4/20, step: 9940, training_loss: 1.55630
Epoch: 4/20, step: 9960, training_loss: 1.11721
Epoch: 4/20, step: 9980, training_loss: 2.26550
Epoch: 4/20, step: 10000, training_loss: 2.21865
accuracy: 0.37, validation_loss: 2.352105140686035, num_samples: 100
Epoch: 4/20, step: 10020, training_loss: 0.67443
Epoch: 4/20, step: 10040, training_loss: 2.25335
Epoch: 4/20, step: 10060, training_loss: 1.39640
Epoch: 4/20, step: 10080, training_loss: 2.91791
Epoch: 4/20, step: 10100, training_loss: 1.55925
Epoch: 4/20, step: 10120, training_loss: 2.45695
Epoch: 4/20, step: 10140, training_loss: 2.08911
Epoch: 4/20, step: 10160, training_loss: 2.03028
Epoch: 4/20, step: 10180, training_loss: 2.60122
Epoch: 4/20, step: 10200, training_loss: 2.19282
Epoch: 4/20, step: 10220, training_loss: 2.34688
Epoch: 4/20, step: 10240, training_loss: 1.82408
Epoch: 4/20, step: 10260, training_loss: 2.03192
Epoch: 4/20, step: 10280, training_loss: 1.14592
Epoch: 4/20, step: 10300, training_loss: 1.99227
Epoch: 4/20, step: 10320, training_loss: 2.26175
Epoch: 4/20, step: 10340, training_loss: 1.68009
Epoch: 4/20, step: 10360, training_loss: 1.72348
Epoch: 4/20, step: 10380, training_loss: 1.62816
Epoch: 4/20, step: 10400, training_loss: 1.71731
Epoch: 4/20, step: 10420, training_loss: 2.16717
Epoch: 4/20, step: 10440, training_loss: 1.97291
Epoch: 4/20, step: 10460, training_loss: 2.71611
Epoch: 4/20, step: 10480, training_loss: 1.95362
Epoch: 4/20, step: 10500, training_loss: 2.39419
Epoch: 4/20, step: 10520, training_loss: 2.46234
Epoch: 4/20, step: 10540, training_loss: 2.49730
Epoch: 4/20, step: 10560, training_loss: 2.55434
Epoch: 4/20, step: 10580, training_loss: 2.19507
Epoch: 4/20, step: 10600, training_loss: 2.19962
Epoch: 4/20, step: 10620, training_loss: 2.31997
Epoch: 4/20, step: 10640, training_loss: 2.12576
Epoch: 4/20, step: 10660, training_loss: 1.87693
Epoch: 4/20, step: 10680, training_loss: 2.07508
Epoch: 4/20, step: 10700, training_loss: 2.03485
Epoch: 4/20, step: 10720, training_loss: 2.13064
Epoch: 4/20, step: 10740, training_loss: 2.27421
Epoch: 4/20, step: 10760, training_loss: 2.12996
Epoch: 4/20, step: 10780, training_loss: 1.89087
Epoch: 4/20, step: 10800, training_loss: 2.38301
Epoch: 4/20, step: 10820, training_loss: 2.49027
Epoch: 4/20, step: 10840, training_loss: 2.17424
Epoch: 4/20, step: 10860, training_loss: 2.03617
Epoch: 4/20, step: 10880, training_loss: 2.50752
Epoch: 4/20, step: 10900, training_loss: 1.88061
Epoch: 4/20, step: 10920, training_loss: 2.44760
Epoch: 4/20, step: 10940, training_loss: 1.99289
Epoch: 4/20, step: 10960, training_loss: 2.16049
Epoch: 4/20, step: 10980, training_loss: 2.64744
Epoch: 4/20, step: 11000, training_loss: 2.50457
accuracy: 0.44, validation_loss: 2.076103925704956, num_samples: 100
Epoch: 4/20, step: 11020, training_loss: 2.53312
Epoch: 4/20, step: 11040, training_loss: 2.32978
Epoch: 4/20, step: 11060, training_loss: 1.49978
Epoch: 4/20, step: 11080, training_loss: 2.42502
Epoch: 4/20, step: 11100, training_loss: 1.56407
Epoch: 4/20, step: 11120, training_loss: 2.39647
Epoch: 4/20, step: 11140, training_loss: 1.89321
Epoch: 4/20, step: 11160, training_loss: 1.80268
Epoch: 4/20, step: 11180, training_loss: 2.67605
Epoch: 4/20, step: 11200, training_loss: 2.48063
Epoch: 4/20, step: 11220, training_loss: 2.45693
Epoch: 4/20, step: 11240, training_loss: 2.75676
Epoch: 4/20, step: 11260, training_loss: 1.58252
Epoch: 4/20, step: 11280, training_loss: 2.22854
Epoch: 4/20, step: 11300, training_loss: 2.42899
Epoch: 4/20, step: 11320, training_loss: 2.16261
Epoch: 4/20, step: 11340, training_loss: 2.85800
Epoch: 4/20, step: 11360, training_loss: 1.87814
Epoch: 4/20, step: 11380, training_loss: 1.92672
Epoch: 4/20, step: 11400, training_loss: 1.25248
Epoch: 4/20, step: 11420, training_loss: 1.31555
Epoch: 4/20, step: 11440, training_loss: 2.23363
Epoch: 4/20, step: 11460, training_loss: 2.09518
Epoch: 4/20, step: 11480, training_loss: 1.73170
Epoch: 4/20, step: 11500, training_loss: 2.35310
Epoch: 4/20, step: 11520, training_loss: 2.52674
Epoch: 4/20, step: 11540, training_loss: 1.36905
Epoch: 4/20, step: 11560, training_loss: 1.46281
Epoch: 4/20, step: 11580, training_loss: 2.02753
Epoch: 4/20, step: 11600, training_loss: 2.84739
Epoch: 4/20, step: 11620, training_loss: 2.03622
Epoch: 4/20, step: 11640, training_loss: 2.38052
Epoch: 4/20, step: 11660, training_loss: 1.84541
Epoch: 4/20, step: 11680, training_loss: 1.92933
Epoch: 4/20, step: 11700, training_loss: 2.24192
Epoch: 4/20, step: 11720, training_loss: 2.33343
Epoch: 4/20, step: 11740, training_loss: 2.35711
Epoch: 4/20, step: 11760, training_loss: 2.32582
Epoch: 4/20, step: 11780, training_loss: 1.26417
Epoch: 4/20, step: 11800, training_loss: 2.09266
Epoch: 4/20, step: 11820, training_loss: 2.56922
Epoch: 4/20, step: 11840, training_loss: 1.73038
Epoch: 4/20, step: 11860, training_loss: 2.06932
Epoch: 4/20, step: 11880, training_loss: 2.79562
Epoch: 4/20, step: 11900, training_loss: 1.86144
Epoch: 4/20, step: 11920, training_loss: 1.57251
Epoch: 4/20, step: 11940, training_loss: 2.11695
Epoch: 4/20, step: 11960, training_loss: 2.16789
Epoch: 4/20, step: 11980, training_loss: 2.18104
Epoch: 4/20, step: 12000, training_loss: 1.23839
accuracy: 0.43, validation_loss: 2.0866124629974365, num_samples: 100
Epoch: 4/20, step: 12020, training_loss: 2.20709
Epoch: 4/20, step: 12040, training_loss: 1.96019
Epoch: 4/20, step: 12060, training_loss: 1.64575
Epoch: 4/20, step: 12080, training_loss: 2.69627
Epoch: 4/20, step: 12100, training_loss: 2.43860
Epoch: 4/20, step: 12120, training_loss: 1.40985
Epoch: 4/20, step: 12140, training_loss: 1.32784
Epoch: 4/20, step: 12160, training_loss: 1.60063
Epoch: 4/20, step: 12180, training_loss: 2.04128
Epoch: 4/20, step: 12200, training_loss: 2.08027
Epoch: 4/20, step: 12220, training_loss: 2.02141
Epoch: 4/20, step: 12240, training_loss: 2.36018
Epoch: 4/20, step: 12260, training_loss: 2.81707
Epoch: 4/20, step: 12280, training_loss: 2.20108
Epoch: 4/20, step: 12300, training_loss: 2.01892
Epoch: 4/20, step: 12320, training_loss: 2.32380
Epoch: 4/20, step: 12340, training_loss: 2.80362
Epoch: 4/20, step: 12360, training_loss: 2.15905
Epoch: 4/20, step: 12380, training_loss: 2.41233
Epoch: 4/20, step: 12400, training_loss: 2.82872
Epoch: 4/20, step: 12420, training_loss: 2.35678
Epoch: 4/20, step: 12440, training_loss: 2.95372
Epoch: 4/20, step: 12460, training_loss: 2.88439
Epoch: 4/20, step: 12480, training_loss: 2.18407
Epoch: 4/20, step: 12500, training_loss: 3.04748
Epoch: 4/20, step: 12520, training_loss: 1.89191
Epoch: 4/20, step: 12540, training_loss: 2.23167
Epoch: 4/20, step: 12560, training_loss: 1.27652
Epoch: 4/20, step: 12580, training_loss: 1.95044
Epoch: 4/20, step: 12600, training_loss: 1.80331
Epoch: 4/20, step: 12620, training_loss: 2.20097
Epoch: 4/20, step: 12640, training_loss: 2.47302
Epoch: 4/20, step: 12660, training_loss: 1.91870
Epoch: 4/20, step: 12680, training_loss: 2.01920
Epoch: 4/20, step: 12700, training_loss: 1.79459
Epoch: 4/20, step: 12720, training_loss: 2.05882
Epoch: 4/20, step: 12740, training_loss: 2.33873
Epoch: 4/20, step: 12760, training_loss: 1.90060
Epoch: 4/20, step: 12780, training_loss: 2.19731
Epoch: 4/20, step: 12800, training_loss: 1.97114
Epoch: 4/20, step: 12820, training_loss: 2.27808
Epoch: 4/20, step: 12840, training_loss: 1.89167
Epoch: 4/20, step: 12860, training_loss: 2.26819
Epoch: 4/20, step: 12880, training_loss: 2.09900
Epoch: 4/20, step: 12900, training_loss: 2.03873
Epoch: 4/20, step: 12920, training_loss: 2.40254
Epoch: 4/20, step: 12940, training_loss: 2.92172
Epoch: 4/20, step: 12960, training_loss: 2.06998
Epoch: 4/20, step: 12980, training_loss: 2.32055
Epoch: 4/20, step: 13000, training_loss: 1.75472
accuracy: 0.34, validation_loss: 2.3118059635162354, num_samples: 100
Epoch: 4/20, step: 13020, training_loss: 2.26762
Epoch: 4/20, step: 13040, training_loss: 1.84355
Epoch: 4/20, step: 13060, training_loss: 3.19619
Epoch: 4/20, step: 13080, training_loss: 1.90684
Epoch: 4/20, step: 13100, training_loss: 1.88720
Epoch: 4/20, step: 13120, training_loss: 2.24914
Epoch: 4/20, step: 13140, training_loss: 2.08057
Epoch: 4/20, step: 13160, training_loss: 2.05209
Epoch: 4/20, step: 13180, training_loss: 2.35677
Epoch: 4/20, step: 13200, training_loss: 1.69534
Epoch: 4/20, step: 13220, training_loss: 1.96020
Epoch: 4/20, step: 13240, training_loss: 2.09887
Epoch: 4/20, step: 13260, training_loss: 1.95210
Epoch: 4/20, step: 13280, training_loss: 3.13772
Epoch: 4/20, step: 13300, training_loss: 2.88196
Epoch: 4/20, step: 13320, training_loss: 3.01459
Epoch: 4/20, step: 13340, training_loss: 1.41274
Epoch: 4/20, step: 13360, training_loss: 2.75929
Epoch: 4/20, step: 13380, training_loss: 2.13937
Epoch: 4/20, step: 13400, training_loss: 2.03224
Epoch: 4/20, step: 13420, training_loss: 1.35879
Epoch: 4/20, step: 13440, training_loss: 2.22568
Epoch: 4/20, step: 13460, training_loss: 2.15447
Epoch: 4/20, step: 13480, training_loss: 1.66469
Epoch: 4/20, step: 13500, training_loss: 2.03106
Epoch: 4/20, step: 13520, training_loss: 1.53059
Epoch: 4/20, step: 13540, training_loss: 2.32271
Epoch: 4/20, step: 13560, training_loss: 2.19957
Epoch: 4/20, step: 13580, training_loss: 1.74376
Epoch: 4/20, step: 13600, training_loss: 1.79554
Epoch: 4/20, step: 13620, training_loss: 2.60812
Epoch: 4/20, step: 13640, training_loss: 2.54310
Epoch: 4/20, step: 13660, training_loss: 1.76965
Epoch: 4/20, step: 13680, training_loss: 2.60661
Epoch: 4/20, step: 13700, training_loss: 2.47654
Epoch: 4/20, step: 13720, training_loss: 1.72987
Epoch: 4/20, step: 13740, training_loss: 2.01514
Epoch: 4/20, step: 13760, training_loss: 2.10763
Epoch: 4/20, step: 13780, training_loss: 2.21050
Epoch: 4/20, step: 13800, training_loss: 1.89772
Epoch: 4/20, step: 13820, training_loss: 2.57681
Epoch: 4/20, step: 13840, training_loss: 1.26001
Epoch: 4/20, step: 13860, training_loss: 1.92334
Epoch: 4/20, step: 13880, training_loss: 2.21149
Epoch: 4/20, step: 13900, training_loss: 2.49231
Epoch: 4/20, step: 13920, training_loss: 2.42703
Epoch: 4/20, step: 13940, training_loss: 2.50216
Epoch: 4/20, step: 13960, training_loss: 1.49718
Epoch: 4/20, step: 13980, training_loss: 3.16936
Epoch: 4/20, step: 14000, training_loss: 1.69744
accuracy: 0.35, validation_loss: 2.464383840560913, num_samples: 100
Epoch: 4/20, step: 14020, training_loss: 2.80590
Epoch: 4/20, step: 14040, training_loss: 2.70302
Epoch: 4/20, step: 14060, training_loss: 2.28011
Epoch: 4/20, step: 14080, training_loss: 2.12103
Epoch: 4/20, step: 14100, training_loss: 2.39103
Epoch: 4/20, step: 14120, training_loss: 1.77617
Epoch: 4/20, step: 14140, training_loss: 2.24968
Epoch: 4/20, step: 14160, training_loss: 1.98794
Epoch: 4/20, step: 14180, training_loss: 2.46183
Epoch: 4/20, step: 14200, training_loss: 2.07031
Epoch: 4/20, step: 14220, training_loss: 1.53600
Epoch: 4/20, step: 14240, training_loss: 2.79805
Epoch: 4/20, step: 14260, training_loss: 2.42007
Epoch: 4/20, step: 14280, training_loss: 2.44177
Epoch: 4/20, step: 14300, training_loss: 1.42889
Epoch: 4/20, step: 14320, training_loss: 2.13067
Epoch: 4/20, step: 14340, training_loss: 2.35892
Epoch: 4/20, step: 14360, training_loss: 1.76724
Epoch: 4/20, step: 14380, training_loss: 1.81069
Epoch: 4/20, step: 14400, training_loss: 2.60853
Epoch: 4/20, step: 14420, training_loss: 2.65082
Epoch: 4/20, step: 14440, training_loss: 2.64688
Epoch: 4/20, step: 14460, training_loss: 1.97407
Epoch: 4/20, step: 14480, training_loss: 2.92759
Epoch: 4/20, step: 14500, training_loss: 1.95666
Epoch: 4/20, step: 14520, training_loss: 1.28664
Epoch: 4/20, step: 14540, training_loss: 2.00898
Epoch: 4/20, step: 14560, training_loss: 3.00061
Epoch: 4/20, step: 14580, training_loss: 1.14693
Epoch: 4/20, step: 14600, training_loss: 1.85377
Epoch: 4/20, step: 14620, training_loss: 2.65778
Epoch: 4/20, step: 14640, training_loss: 1.91579
Epoch: 4/20, step: 14660, training_loss: 1.90936
Epoch: 4/20, step: 14680, training_loss: 2.22979
Epoch: 4/20, step: 14700, training_loss: 1.95886
Epoch: 4/20, step: 14720, training_loss: 2.22419
Epoch: 4/20, step: 14740, training_loss: 2.23862
Epoch: 4/20, step: 14760, training_loss: 2.44771
Epoch: 4/20, step: 14780, training_loss: 1.80462
Epoch: 4/20, step: 14800, training_loss: 2.60834
Epoch: 4/20, step: 14820, training_loss: 2.28044
Epoch: 4/20, step: 14840, training_loss: 2.18825
Epoch: 4/20, step: 14860, training_loss: 1.40324
Epoch: 4/20, step: 14880, training_loss: 1.54260
Epoch: 4/20, step: 14900, training_loss: 2.00738
Epoch: 4/20, step: 14920, training_loss: 1.98633
Epoch: 4/20, step: 14940, training_loss: 2.19298
Epoch: 4/20, step: 14960, training_loss: 2.33549
Epoch: 4/20, step: 14980, training_loss: 1.73117
Epoch: 4/20, step: 15000, training_loss: 1.02228
accuracy: 0.39, validation_loss: 1.9194960594177246, num_samples: 100
Epoch: 4/20, step: 15020, training_loss: 1.70867
Epoch: 4/20, step: 15040, training_loss: 1.83269
Epoch: 4/20, step: 15060, training_loss: 2.00621
Epoch: 4/20, step: 15080, training_loss: 1.65501
Epoch: 4/20, step: 15100, training_loss: 2.70517
Epoch: 4/20, step: 15120, training_loss: 2.36344
Epoch: 4/20, step: 15140, training_loss: 1.77635
Epoch: 4/20, step: 15160, training_loss: 2.71333
Epoch: 4/20, step: 15180, training_loss: 2.20787
Epoch: 4/20, step: 15200, training_loss: 2.21213
Epoch: 4/20, step: 15220, training_loss: 1.86393
Epoch: 4/20, step: 15240, training_loss: 1.52940
Epoch: 4/20, step: 15260, training_loss: 2.22007
Epoch: 4/20, step: 15280, training_loss: 1.97254
Epoch: 4/20, step: 15300, training_loss: 2.04054
Epoch: 4/20, step: 15320, training_loss: 1.81839
Epoch: 4/20, step: 15340, training_loss: 1.39790
Epoch: 4/20, step: 15360, training_loss: 2.17893
Epoch: 4/20, step: 15380, training_loss: 2.65001
Epoch: 4/20, step: 15400, training_loss: 2.25138
Epoch: 4/20, step: 15420, training_loss: 2.96421
Epoch: 4/20, step: 15440, training_loss: 2.31820
Epoch: 4/20, step: 15460, training_loss: 2.39592
Epoch: 4/20, step: 15480, training_loss: 1.37089
Epoch: 4/20, step: 15500, training_loss: 1.35460
Epoch: 4/20, step: 15520, training_loss: 2.68265
Epoch: 4/20, step: 15540, training_loss: 2.04191
Epoch: 4/20, step: 15560, training_loss: 1.36491
Epoch: 4/20, step: 15580, training_loss: 2.32008
Epoch: 4/20, step: 15600, training_loss: 2.65547
Epoch: 4/20, step: 15620, training_loss: 1.95491
Epoch: 4/20, step: 15640, training_loss: 2.27272
Epoch: 4/20, step: 15660, training_loss: 2.68809
Epoch: 4/20, step: 15680, training_loss: 1.92571
Epoch: 4/20, step: 15700, training_loss: 2.34780
Epoch: 4/20, step: 15720, training_loss: 2.16020
Epoch: 4/20, step: 15740, training_loss: 2.55383
Epoch: 4/20, step: 15760, training_loss: 2.27602
Epoch: 4/20, step: 15780, training_loss: 1.30668
Epoch: 4/20, step: 15800, training_loss: 2.30891
Epoch: 4/20, step: 15820, training_loss: 1.67929
Epoch: 4/20, step: 15840, training_loss: 1.84471
Epoch: 4/20, step: 15860, training_loss: 2.10974
Epoch: 4/20, step: 15880, training_loss: 1.41934
Epoch: 4/20, step: 15900, training_loss: 1.92495
Epoch: 4/20, step: 15920, training_loss: 2.36272
Epoch: 4/20, step: 15940, training_loss: 2.55742
Epoch: 4/20, step: 15960, training_loss: 1.76315
Epoch: 4/20, step: 15980, training_loss: 2.18972
Epoch: 4/20, step: 16000, training_loss: 1.65285
accuracy: 0.32, validation_loss: 2.470061779022217, num_samples: 100
Epoch: 4/20, step: 16020, training_loss: 2.17420
Epoch: 4/20, step: 16040, training_loss: 1.64179
Epoch: 4/20, step: 16060, training_loss: 1.14169
Epoch: 4/20, step: 16080, training_loss: 1.98923
Epoch: 4/20, step: 16100, training_loss: 2.04529
Epoch: 4/20, step: 16120, training_loss: 1.96909
Epoch: 4/20, step: 16140, training_loss: 1.65219
Epoch: 4/20, step: 16160, training_loss: 2.16290
Epoch: 4/20, step: 16180, training_loss: 1.69245
Epoch: 4/20, step: 16200, training_loss: 1.35786
Epoch: 4/20, step: 16220, training_loss: 2.52335
Epoch: 4/20, step: 16240, training_loss: 2.59438
Epoch: 4/20, step: 16260, training_loss: 1.97779
Epoch: 4/20, step: 16280, training_loss: 2.89007
Epoch: 4/20, step: 16300, training_loss: 1.73683
Epoch: 4/20, step: 16320, training_loss: 1.44114
Epoch: 4/20, step: 16340, training_loss: 1.63758
Epoch: 4/20, step: 16360, training_loss: 2.05535
Epoch: 4/20, step: 16380, training_loss: 2.19704
Epoch: 4/20, step: 16400, training_loss: 1.97302
Epoch: 4/20, step: 16420, training_loss: 2.46255
Epoch: 4/20, step: 16440, training_loss: 2.08836
Epoch: 4/20, step: 16460, training_loss: 1.86401
Epoch: 4/20, step: 16480, training_loss: 2.95003
Epoch: 4/20, step: 16500, training_loss: 2.38734
Epoch: 4/20, step: 16520, training_loss: 1.86256
Epoch: 4/20, step: 16540, training_loss: 1.79670
Epoch: 4/20, step: 16560, training_loss: 2.07465
Epoch: 4/20, step: 16580, training_loss: 2.45108
Epoch: 4/20, step: 16600, training_loss: 2.93432
Epoch: 4/20, step: 16620, training_loss: 1.35611
Epoch: 4/20, step: 16640, training_loss: 2.29082
Epoch: 4/20, step: 16660, training_loss: 2.06530
Epoch: 4/20, step: 16680, training_loss: 1.79030
Epoch: 4/20, step: 16700, training_loss: 1.97307
Epoch: 4/20, step: 16720, training_loss: 1.86955
Epoch: 4/20, step: 16740, training_loss: 2.31041
Epoch: 4/20, step: 16760, training_loss: 1.87094
Epoch: 4/20, step: 16780, training_loss: 2.11241
Epoch: 4/20, step: 16800, training_loss: 3.30567
Epoch: 4/20, step: 16820, training_loss: 2.64464
Epoch: 4/20, step: 16840, training_loss: 2.00138
Epoch: 4/20, step: 16860, training_loss: 1.88452
Epoch: 4/20, step: 16880, training_loss: 2.24121
Epoch: 4/20, step: 16900, training_loss: 2.61354
Epoch: 4/20, step: 16920, training_loss: 2.08997
Epoch: 4/20, step: 16940, training_loss: 2.97915
Epoch: 4/20, step: 16960, training_loss: 2.66186
Epoch: 4/20, step: 16980, training_loss: 2.37473
Epoch: 4/20, step: 17000, training_loss: 2.38696
accuracy: 0.49, validation_loss: 1.9546786546707153, num_samples: 100
Epoch: 4/20, step: 17020, training_loss: 2.67178
Epoch: 4/20, step: 17040, training_loss: 1.88067
Epoch: 4/20, step: 17060, training_loss: 2.61487
Epoch: 4/20, step: 17080, training_loss: 1.87017
Epoch: 4/20, step: 17100, training_loss: 2.31669
Epoch: 4/20, step: 17120, training_loss: 1.43875
Epoch: 4/20, step: 17140, training_loss: 2.37537
Epoch: 4/20, step: 17160, training_loss: 2.68219
Epoch: 4/20, step: 17180, training_loss: 2.95929
Epoch: 4/20, step: 17200, training_loss: 2.22708
Epoch: 4/20, step: 17220, training_loss: 1.81442
Epoch: 4/20, step: 17240, training_loss: 1.57620
Epoch: 4/20, step: 17260, training_loss: 2.44349
Epoch: 4/20, step: 17280, training_loss: 2.10116
Epoch: 4/20, step: 17300, training_loss: 2.03497
Epoch: 4/20, step: 17320, training_loss: 2.50230
Epoch: 4/20, step: 17340, training_loss: 2.22130
Epoch: 4/20, step: 17360, training_loss: 2.19191
Epoch: 4/20, step: 17380, training_loss: 2.54097
Epoch: 4/20, step: 17400, training_loss: 2.13327
Epoch: 4/20, step: 17420, training_loss: 2.43370
Epoch: 4/20, step: 17440, training_loss: 2.78665
Epoch: 4/20, step: 17460, training_loss: 2.37264
Epoch: 4/20, step: 17480, training_loss: 2.37073
Epoch: 4/20, step: 17500, training_loss: 2.87883
Epoch: 4/20, step: 17520, training_loss: 2.22599
Epoch: 4/20, step: 17540, training_loss: 1.31895
Epoch: 4/20, step: 17560, training_loss: 2.55372
Epoch: 4/20, step: 17580, training_loss: 2.73555
Epoch: 4/20, step: 17600, training_loss: 1.42378
Epoch: 4/20, step: 17620, training_loss: 2.24906
Epoch: 4/20, step: 17640, training_loss: 2.86090
Epoch: 4/20, step: 17660, training_loss: 2.02608
Epoch: 4/20, step: 17680, training_loss: 1.28371
Epoch: 4/20, step: 17700, training_loss: 2.69059
Epoch: 4/20, step: 17720, training_loss: 2.66873
Epoch: 4/20, step: 17740, training_loss: 2.31661
Epoch: 4/20, step: 17760, training_loss: 2.15173
Epoch: 4/20, step: 17780, training_loss: 1.77079
Epoch: 4/20, step: 17800, training_loss: 3.21486
Epoch: 4/20, step: 17820, training_loss: 2.49530
Epoch: 4/20, step: 17840, training_loss: 1.31547
Epoch: 4/20, step: 17860, training_loss: 2.40345
Epoch: 4/20, step: 17880, training_loss: 2.91722
Epoch: 4/20, step: 17900, training_loss: 2.08732
Epoch: 4/20, step: 17920, training_loss: 1.66525
Epoch: 4/20, step: 17940, training_loss: 2.35905
Epoch: 4/20, step: 17960, training_loss: 2.04030
Epoch: 4/20, step: 17980, training_loss: 2.63278
Epoch: 4/20, step: 18000, training_loss: 3.04451
accuracy: 0.44, validation_loss: 2.0177276134490967, num_samples: 100
Epoch: 4/20, step: 18020, training_loss: 2.38968
Epoch: 4/20, step: 18040, training_loss: 1.89455
Epoch: 4/20, step: 18060, training_loss: 1.68975
Epoch: 4/20, step: 18080, training_loss: 1.60824
Epoch: 4/20, step: 18100, training_loss: 2.16443
Epoch: 4/20, step: 18120, training_loss: 2.51497
Epoch: 4/20, step: 18140, training_loss: 1.92292
Epoch: 4/20, step: 18160, training_loss: 2.34631
Epoch: 4/20, step: 18180, training_loss: 1.90563
Epoch: 4/20, step: 18200, training_loss: 2.42189
Epoch: 4/20, step: 18220, training_loss: 2.00025
Epoch: 4/20, step: 18240, training_loss: 2.87712
Epoch: 4/20, step: 18260, training_loss: 2.06692
Epoch: 4/20, step: 18280, training_loss: 1.94914
Epoch: 4/20, step: 18300, training_loss: 2.08101
Epoch: 4/20, step: 18320, training_loss: 2.10054
Epoch: 4/20, step: 18340, training_loss: 1.77264
Epoch: 4/20, step: 18360, training_loss: 2.57065
Epoch: 4/20, step: 18380, training_loss: 2.99213
Epoch: 4/20, step: 18400, training_loss: 2.61740
Epoch: 4/20, step: 18420, training_loss: 1.83255
Epoch: 4/20, step: 18440, training_loss: 1.83491
Epoch: 4/20, step: 18460, training_loss: 1.81840
Epoch: 4/20, step: 18480, training_loss: 2.47755
Epoch: 4/20, step: 18500, training_loss: 2.52808
Epoch: 4/20, step: 18520, training_loss: 1.69286
Epoch: 4/20, step: 18540, training_loss: 1.20506
Epoch: 4/20, step: 18560, training_loss: 2.46983
Epoch: 4/20, step: 18580, training_loss: 2.76702
Epoch: 4/20, step: 18600, training_loss: 1.19678
Epoch: 4/20, step: 18620, training_loss: 2.74096
Epoch: 4/20, step: 18640, training_loss: 2.50279
Epoch: 4/20, step: 18660, training_loss: 2.13057
Epoch: 4/20, step: 18680, training_loss: 2.08270
Epoch: 4/20, step: 18700, training_loss: 2.46072
Epoch: 4/20, step: 18720, training_loss: 2.06204
Epoch: 4/20, step: 18740, training_loss: 2.50879
Epoch: 4/20, step: 18760, training_loss: 2.74896
Epoch: 4/20, step: 18780, training_loss: 1.84529
Epoch: 4/20, step: 18800, training_loss: 2.37985
Epoch: 4/20, step: 18820, training_loss: 2.39363
Epoch: 4/20, step: 18840, training_loss: 2.42536
Epoch: 4/20, step: 18860, training_loss: 2.16935
Epoch: 4/20, step: 18880, training_loss: 1.72709
Epoch: 4/20, step: 18900, training_loss: 2.33588
Epoch: 4/20, step: 18920, training_loss: 2.30448
Epoch: 4/20, step: 18940, training_loss: 2.62159
Epoch: 4/20, step: 18960, training_loss: 1.53429
Epoch: 4/20, step: 18980, training_loss: 2.88599
Epoch: 4/20, step: 19000, training_loss: 2.63081
accuracy: 0.38, validation_loss: 2.1709389686584473, num_samples: 100
Epoch: 4/20, step: 19020, training_loss: 2.16708
Epoch: 4/20, step: 19040, training_loss: 2.32940
Epoch: 4/20, step: 19060, training_loss: 2.85770
Epoch: 4/20, step: 19080, training_loss: 2.89875
Epoch: 4/20, step: 19100, training_loss: 1.73926
Epoch: 4/20, step: 19120, training_loss: 2.56297
Epoch: 4/20, step: 19140, training_loss: 2.29023
Epoch: 4/20, step: 19160, training_loss: 2.01020
Epoch: 4/20, step: 19180, training_loss: 1.78755
Epoch: 4/20, step: 19200, training_loss: 1.79664
Epoch: 4/20, step: 19220, training_loss: 1.83604
Epoch: 4/20, step: 19240, training_loss: 1.63407
Epoch: 4/20, step: 19260, training_loss: 2.12488
Epoch: 4/20, step: 19280, training_loss: 1.84487
Epoch: 4/20, step: 19300, training_loss: 2.39845
Epoch: 4/20, step: 19320, training_loss: 2.38325
Epoch: 4/20, step: 19340, training_loss: 1.66800
Epoch: 4/20, step: 19360, training_loss: 2.09253
Epoch: 4/20, step: 19380, training_loss: 2.89932
Epoch: 4/20, step: 19400, training_loss: 2.54921
Epoch: 4/20, step: 19420, training_loss: 2.06225
Epoch: 4/20, step: 19440, training_loss: 2.33535
Epoch: 4/20, step: 19460, training_loss: 2.07034
Epoch: 4/20, step: 19480, training_loss: 1.61085
Epoch: 4/20, step: 19500, training_loss: 2.30598
Epoch: 4/20, step: 19520, training_loss: 1.85693
Epoch: 4/20, step: 19540, training_loss: 2.95473
Epoch: 4/20, step: 19560, training_loss: 1.84362
Epoch: 4/20, step: 19580, training_loss: 1.46817
Epoch: 4/20, step: 19600, training_loss: 1.72259
Epoch: 4/20, step: 19620, training_loss: 2.14240
Epoch: 4/20, step: 19640, training_loss: 2.86952
Epoch: 4/20, step: 19660, training_loss: 2.45654
Epoch: 4/20, step: 19680, training_loss: 2.14096
Epoch: 4/20, step: 19700, training_loss: 2.23032
Epoch: 4/20, step: 19720, training_loss: 1.63462
Epoch: 4/20, step: 19740, training_loss: 2.38625
Epoch: 4/20, step: 19760, training_loss: 2.75768
Epoch: 4/20, step: 19780, training_loss: 2.56246
Epoch: 4/20, step: 19800, training_loss: 2.43474
Epoch: 4/20, step: 19820, training_loss: 2.02893
Epoch: 4/20, step: 19840, training_loss: 1.39334
Epoch: 4/20, step: 19860, training_loss: 2.37131
Epoch: 4/20, step: 19880, training_loss: 2.05639
Epoch: 4/20, step: 19900, training_loss: 1.91099
Epoch: 4/20, step: 19920, training_loss: 2.05977
Epoch: 4/20, step: 19940, training_loss: 2.08631
Epoch: 4/20, step: 19960, training_loss: 1.94215
Epoch: 4/20, step: 19980, training_loss: 2.86049
Epoch: 4/20, step: 20000, training_loss: 2.36384
accuracy: 0.38, validation_loss: 2.0514719486236572, num_samples: 100
Epoch: 4/20, step: 20020, training_loss: 3.23924
Epoch: 4/20, step: 20040, training_loss: 2.21416
Epoch: 4/20, step: 20060, training_loss: 2.07394
Epoch: 4/20, step: 20080, training_loss: 1.60528
Epoch: 4/20, step: 20100, training_loss: 1.82260
Epoch: 4/20, step: 20120, training_loss: 2.16763
Epoch: 4/20, step: 20140, training_loss: 2.26504
Epoch: 4/20, step: 20160, training_loss: 2.44702
Epoch: 4/20, step: 20180, training_loss: 2.57099
Epoch: 4/20, step: 20200, training_loss: 2.37183
Epoch: 4/20, step: 20220, training_loss: 2.11207
Epoch: 4/20, step: 20240, training_loss: 1.96529
Epoch: 4/20, step: 20260, training_loss: 1.85440
Epoch: 4/20, step: 20280, training_loss: 3.07518
Epoch: 4/20, step: 20300, training_loss: 2.07132
Epoch: 4/20, step: 20320, training_loss: 2.31893
Epoch: 4/20, step: 20340, training_loss: 2.19660
Epoch: 4/20, step: 20360, training_loss: 1.79649
Epoch: 4/20, step: 20380, training_loss: 1.95207
Epoch: 4/20, step: 20400, training_loss: 2.53999
Epoch: 4/20, step: 20420, training_loss: 2.01649
Epoch: 4/20, step: 20440, training_loss: 1.76285
Epoch: 4/20, step: 20460, training_loss: 1.62176
Epoch: 4/20, step: 20480, training_loss: 2.21762
Epoch: 4/20, step: 20500, training_loss: 2.15432
Epoch: 4/20, step: 20520, training_loss: 2.02243
Epoch: 4/20, step: 20540, training_loss: 1.60161
Epoch: 4/20, step: 20560, training_loss: 2.16199
Epoch: 4/20, step: 20580, training_loss: 2.13574
Epoch: 4/20, step: 20600, training_loss: 1.66001
Epoch: 4/20, step: 20620, training_loss: 1.92822
Epoch: 4/20, step: 20640, training_loss: 2.42542
Epoch: 4/20, step: 20660, training_loss: 1.79487
Epoch: 4/20, step: 20680, training_loss: 2.44922
Epoch: 4/20, step: 20700, training_loss: 3.00639
Epoch: 4/20, step: 20720, training_loss: 1.64122
Epoch: 4/20, step: 20740, training_loss: 2.55934
Epoch: 4/20, step: 20760, training_loss: 2.13393
Epoch: 4/20, step: 20780, training_loss: 1.89261
Epoch: 4/20, step: 20800, training_loss: 2.46621
Epoch: 4/20, step: 20820, training_loss: 1.69513
Epoch: 4/20, step: 20840, training_loss: 1.91049
Epoch: 4/20, step: 20860, training_loss: 2.04014
Epoch: 4/20, step: 20880, training_loss: 2.65859
Epoch: 4/20, step: 20900, training_loss: 1.85884
Epoch: 4/20, step: 20920, training_loss: 2.44776
Epoch: 4/20, step: 20940, training_loss: 1.88304
Epoch: 4/20, step: 20960, training_loss: 2.26194
Epoch: 4/20, step: 20980, training_loss: 2.55618
Epoch: 4/20, step: 21000, training_loss: 1.54471
accuracy: 0.46, validation_loss: 1.8542529344558716, num_samples: 100
Epoch: 4/20, step: 21020, training_loss: 2.16994
Epoch: 4/20, step: 21040, training_loss: 2.34891
Epoch: 4/20, step: 21060, training_loss: 2.32881
Epoch: 4/20, step: 21080, training_loss: 1.39760
Epoch: 4/20, step: 21100, training_loss: 2.76044
Epoch: 4/20, step: 21120, training_loss: 2.12134
Epoch: 4/20, step: 21140, training_loss: 2.40799
Epoch: 4/20, step: 21160, training_loss: 1.92601
Epoch: 4/20, step: 21180, training_loss: 1.96678
Epoch: 4/20, step: 21200, training_loss: 1.98700
Epoch: 4/20, step: 21220, training_loss: 1.98330
Epoch: 4/20, step: 21240, training_loss: 1.94666
Epoch: 4/20, step: 21260, training_loss: 2.34408
Epoch: 4/20, step: 21280, training_loss: 2.09127
Epoch: 4/20, step: 21300, training_loss: 1.42872
Epoch: 4/20, step: 21320, training_loss: 1.98049
Epoch: 4/20, step: 21340, training_loss: 2.55106
Epoch: 4/20, step: 21360, training_loss: 2.60192
Epoch: 4/20, step: 21380, training_loss: 2.12725
Epoch: 4/20, step: 21400, training_loss: 2.05611
Epoch: 4/20, step: 21420, training_loss: 2.16665
Epoch: 4/20, step: 21440, training_loss: 2.00191
Epoch: 4/20, step: 21460, training_loss: 1.99448
Epoch: 4/20, step: 21480, training_loss: 3.16141
Epoch: 4/20, step: 21500, training_loss: 2.01392
Epoch: 4/20, step: 21520, training_loss: 2.49152
Epoch: 4/20, step: 21540, training_loss: 2.11092
Epoch: 4/20, step: 21560, training_loss: 1.71907
Epoch: 4/20, step: 21580, training_loss: 2.74427
Epoch: 4/20, step: 21600, training_loss: 1.95229
Epoch: 4/20, step: 21620, training_loss: 2.31799
Epoch: 4/20, step: 21640, training_loss: 1.73274
Epoch: 4/20, step: 21660, training_loss: 1.64225
Epoch: 4/20, step: 21680, training_loss: 2.30501
Epoch: 4/20, step: 21700, training_loss: 2.77889
Epoch: 4/20, step: 21720, training_loss: 2.52054
Epoch: 4/20, step: 21740, training_loss: 1.75946
Epoch: 4/20, step: 21760, training_loss: 2.08725
Epoch: 4/20, step: 21780, training_loss: 1.97683
Epoch: 4/20, step: 21800, training_loss: 2.52827
Epoch: 4/20, step: 21820, training_loss: 1.44520
Epoch: 4/20, step: 21840, training_loss: 2.29448
Epoch: 4/20, step: 21860, training_loss: 1.81038
Epoch: 4/20, step: 21880, training_loss: 2.97682
Epoch: 4/20, step: 21900, training_loss: 2.36399
Epoch: 4/20, step: 21920, training_loss: 1.73573
Epoch: 4/20, step: 21940, training_loss: 1.29021
Epoch: 4/20, step: 21960, training_loss: 2.35890
Epoch: 4/20, step: 21980, training_loss: 1.78852
Epoch: 4/20, step: 22000, training_loss: 2.48502
accuracy: 0.41, validation_loss: 2.169783115386963, num_samples: 100
Epoch: 4/20, step: 22020, training_loss: 2.27146
Epoch: 4/20, step: 22040, training_loss: 2.15753
Epoch: 4/20, step: 22060, training_loss: 2.34479
Epoch: 4/20, step: 22080, training_loss: 2.22121
Epoch: 4/20, step: 22100, training_loss: 2.75797
Epoch: 4/20, step: 22120, training_loss: 2.20677
Epoch: 4/20, step: 22140, training_loss: 2.47359
Epoch: 4/20, step: 22160, training_loss: 2.46926
Epoch: 4/20, step: 22180, training_loss: 2.20602
Epoch: 4/20, step: 22200, training_loss: 1.77174
Epoch: 4/20, step: 22220, training_loss: 2.34545
Epoch: 4/20, step: 22240, training_loss: 3.09755
Epoch: 4/20, step: 22260, training_loss: 2.60546
Epoch: 4/20, step: 22280, training_loss: 1.88305
Epoch: 4/20, step: 22300, training_loss: 2.60982
Epoch: 4/20, step: 22320, training_loss: 2.28402
Epoch: 4/20, step: 22340, training_loss: 3.05027
Epoch: 4/20, step: 22360, training_loss: 1.85203
Epoch: 4/20, step: 22380, training_loss: 1.71986
Epoch: 4/20, step: 22400, training_loss: 1.65555
Epoch: 4/20, step: 22420, training_loss: 1.63100
Epoch: 4/20, step: 22440, training_loss: 1.88532
Epoch: 4/20, step: 22460, training_loss: 2.64232
Epoch: 4/20, step: 22480, training_loss: 1.82970
Epoch: 4/20, step: 22500, training_loss: 2.80143
Epoch: 4/20, step: 22520, training_loss: 1.79986
Epoch: 4/20, step: 22540, training_loss: 3.01758
Epoch: 4/20, step: 22560, training_loss: 2.19479
Epoch: 4/20, step: 22580, training_loss: 1.59496
Epoch: 4/20, step: 22600, training_loss: 1.97009
Epoch: 4/20, step: 22620, training_loss: 1.87260
Epoch: 4/20, step: 22640, training_loss: 2.06662
Epoch: 4/20, step: 22660, training_loss: 2.37609
Epoch: 4/20, step: 22680, training_loss: 1.92277
Epoch: 4/20, step: 22700, training_loss: 1.81362
Epoch: 4/20, step: 22720, training_loss: 1.75481
Epoch: 4/20, step: 22740, training_loss: 3.00358
Epoch: 4/20, step: 22760, training_loss: 2.61815
Epoch: 4/20, step: 22780, training_loss: 1.78558
Epoch: 4/20, step: 22800, training_loss: 2.08036
Epoch: 4/20, step: 22820, training_loss: 2.53845
Epoch: 4/20, step: 22840, training_loss: 2.15742
Epoch: 4/20, step: 22860, training_loss: 1.97675
Epoch: 4/20, step: 22880, training_loss: 2.65727
Epoch: 4/20, step: 22900, training_loss: 2.23938
Epoch: 4/20, step: 22920, training_loss: 2.21191
Epoch: 4/20, step: 22940, training_loss: 2.71008
Epoch: 4/20, step: 22960, training_loss: 2.40761
Epoch: 4/20, step: 22980, training_loss: 1.72814
Epoch: 4/20, step: 23000, training_loss: 1.61527
accuracy: 0.33, validation_loss: 2.556100845336914, num_samples: 100
Epoch: 4/20, step: 23020, training_loss: 2.16704
Epoch: 4/20, step: 23040, training_loss: 2.04162
Epoch: 4/20, step: 23060, training_loss: 2.38964
Epoch: 4/20, step: 23080, training_loss: 2.05953
Epoch: 4/20, step: 23100, training_loss: 1.92556
Epoch: 4/20, step: 23120, training_loss: 2.53297
Epoch: 4/20, step: 23140, training_loss: 1.87673
Epoch: 4/20, step: 23160, training_loss: 1.41233
Epoch: 4/20, step: 23180, training_loss: 1.98545
Epoch: 4/20, step: 23200, training_loss: 2.48662
Epoch: 4/20, step: 23220, training_loss: 2.05294
Epoch: 4/20, step: 23240, training_loss: 3.02995
Epoch: 4/20, step: 23260, training_loss: 2.43301
Epoch: 4/20, step: 23280, training_loss: 1.47482
Epoch: 4/20, step: 23300, training_loss: 1.29768
Epoch: 4/20, step: 23320, training_loss: 2.63332
Epoch: 4/20, step: 23340, training_loss: 2.11490
Epoch: 4/20, step: 23360, training_loss: 2.43079
Epoch: 4/20, step: 23380, training_loss: 1.62934
Epoch: 4/20, step: 23400, training_loss: 2.59574
Epoch: 4/20, step: 23420, training_loss: 1.92921
Epoch: 4/20, step: 23440, training_loss: 2.06049
Epoch: 4/20, step: 23460, training_loss: 2.53226
Epoch: 4/20, step: 23480, training_loss: 2.18875
Epoch: 4/20, step: 23500, training_loss: 2.26129
Epoch: 4/20, step: 23520, training_loss: 2.36649
Epoch: 4/20, step: 23540, training_loss: 2.69549
Epoch: 4/20, step: 23560, training_loss: 1.87620
Epoch: 4/20, step: 23580, training_loss: 2.92301
Epoch: 4/20, step: 23600, training_loss: 2.09481
Epoch: 4/20, step: 23620, training_loss: 1.36121
Epoch: 4/20, step: 23640, training_loss: 2.24416
Epoch: 4/20, step: 23660, training_loss: 2.18626
Epoch: 4/20, step: 23680, training_loss: 2.11209
Epoch: 4/20, step: 23700, training_loss: 1.47196
Epoch: 4/20, step: 23720, training_loss: 2.44289
Epoch: 4/20, step: 23740, training_loss: 1.68635
Epoch: 4/20, step: 23760, training_loss: 2.02400
Epoch: 4/20, step: 23780, training_loss: 1.65679
Epoch: 4/20, step: 23800, training_loss: 2.53292
Epoch: 4/20, step: 23820, training_loss: 1.85855
Epoch: 4/20, step: 23840, training_loss: 1.78513
Epoch: 4/20, step: 23860, training_loss: 2.48938
Epoch: 4/20, step: 23880, training_loss: 2.88343
Epoch: 4/20, step: 23900, training_loss: 2.01858
Epoch: 4/20, step: 23920, training_loss: 1.67092
Epoch: 4/20, step: 23940, training_loss: 2.08778
Epoch: 4/20, step: 23960, training_loss: 2.11835
Epoch: 4/20, step: 23980, training_loss: 1.92011
Epoch: 4/20, step: 24000, training_loss: 2.38357
accuracy: 0.43, validation_loss: 2.193726062774658, num_samples: 100
Epoch: 4/20, step: 24020, training_loss: 2.53150
Epoch: 4/20, step: 24040, training_loss: 2.80863
Epoch: 4/20, step: 24060, training_loss: 2.84693
Epoch: 4/20, step: 24080, training_loss: 1.32485
Epoch: 4/20, step: 24100, training_loss: 2.18775
Epoch: 4/20, step: 24120, training_loss: 2.26140
Epoch: 4/20, step: 24140, training_loss: 2.14437
Epoch: 4/20, step: 24160, training_loss: 2.38919
Epoch: 4/20, step: 24180, training_loss: 1.96673
Epoch: 4/20, step: 24200, training_loss: 2.37267
Epoch: 4/20, step: 24220, training_loss: 2.66330
Epoch: 4/20, step: 24240, training_loss: 2.56829
Epoch: 4/20, step: 24260, training_loss: 2.42050
Epoch: 4/20, step: 24280, training_loss: 2.58249
Epoch: 4/20, step: 24300, training_loss: 2.28426
Epoch: 4/20, step: 24320, training_loss: 1.16270
Epoch: 4/20, step: 24340, training_loss: 2.68164
Epoch: 4/20, step: 24360, training_loss: 1.87085
Epoch: 4/20, step: 24380, training_loss: 2.03498
Epoch: 4/20, step: 24400, training_loss: 2.37538
Epoch: 4/20, step: 24420, training_loss: 1.85867
Epoch: 4/20, step: 24440, training_loss: 2.09943
Epoch: 4/20, step: 24460, training_loss: 1.90715
Epoch: 4/20, step: 24480, training_loss: 1.96889
Epoch: 4/20, step: 24500, training_loss: 1.92377
Epoch: 4/20, step: 24520, training_loss: 2.70436
Epoch: 4/20, step: 24540, training_loss: 2.68163
Epoch: 4/20, step: 24560, training_loss: 2.60431
Epoch: 4/20, step: 24580, training_loss: 1.96030
Epoch: 4/20, step: 24600, training_loss: 1.69106
Epoch: 4/20, step: 24620, training_loss: 2.36831
Epoch: 4/20, step: 24640, training_loss: 2.24132
Epoch: 4/20, step: 24660, training_loss: 1.71193
Epoch: 4/20, step: 24680, training_loss: 2.58183
Epoch: 4/20, step: 24700, training_loss: 2.09164
Epoch: 4/20, step: 24720, training_loss: 1.91028
Epoch: 4/20, step: 24740, training_loss: 1.92133
Epoch: 4/20, step: 24760, training_loss: 2.83614
Epoch: 4/20, step: 24780, training_loss: 2.12417
Epoch: 4/20, step: 24800, training_loss: 2.84642
Epoch: 4/20, step: 24820, training_loss: 2.46830
Epoch: 4/20, step: 24840, training_loss: 1.88966
Epoch: 4/20, step: 24860, training_loss: 2.33883
Epoch: 4/20, step: 24880, training_loss: 2.30252
Epoch: 4/20, step: 24900, training_loss: 1.75119
Epoch: 4/20, step: 24920, training_loss: 1.72912
Epoch: 4/20, step: 24940, training_loss: 2.81822
Epoch: 4/20, step: 24960, training_loss: 2.47813
Epoch: 4/20, step: 24980, training_loss: 3.28099
Epoch: 4/20, step: 25000, training_loss: 1.13912
accuracy: 0.46, validation_loss: 2.1131632328033447, num_samples: 100
Epoch: 4/20, step: 25020, training_loss: 1.93398
Epoch: 4/20, step: 25040, training_loss: 2.46741
Epoch: 4/20, step: 25060, training_loss: 1.79384
Epoch: 4/20, step: 25080, training_loss: 1.78158
Epoch: 4/20, step: 25100, training_loss: 2.89859
Epoch: 4/20, step: 25120, training_loss: 2.23955
Epoch: 4/20, step: 25140, training_loss: 2.39459
Epoch: 4/20, step: 25160, training_loss: 2.13160
Epoch: 4/20, step: 25180, training_loss: 2.29435
Epoch: 4/20, step: 25200, training_loss: 1.95056
Epoch: 4/20, step: 25220, training_loss: 2.47872
Epoch: 4/20, step: 25240, training_loss: 2.74117
Epoch: 4/20, step: 25260, training_loss: 1.66185
Epoch: 4/20, step: 25280, training_loss: 1.51611
Epoch: 4/20, step: 25300, training_loss: 1.32373
Epoch: 4/20, step: 25320, training_loss: 2.29933
Epoch: 4/20, step: 25340, training_loss: 2.48826
Epoch: 4/20, step: 25360, training_loss: 1.66022
Epoch: 4/20, step: 25380, training_loss: 2.26849
Epoch: 4/20, step: 25400, training_loss: 2.45870
Epoch: 4/20, step: 25420, training_loss: 1.72034
Epoch: 4/20, step: 25440, training_loss: 2.78604
Epoch: 4/20, step: 25460, training_loss: 2.81818
Epoch: 4/20, step: 25480, training_loss: 2.17958
Epoch: 4/20, step: 25500, training_loss: 2.02539
Epoch: 4/20, step: 25520, training_loss: 2.14494
Epoch: 4/20, step: 25540, training_loss: 1.55443
Epoch: 4/20, step: 25560, training_loss: 1.92128
Epoch: 4/20, step: 25580, training_loss: 2.35758
Epoch: 4/20, step: 25600, training_loss: 2.33953
Epoch: 4/20, step: 25620, training_loss: 2.56969
Epoch: 4/20, step: 25640, training_loss: 2.05247
Epoch: 4/20, step: 25660, training_loss: 1.93271
Epoch: 4/20, step: 25680, training_loss: 2.48366
Epoch: 4/20, step: 25700, training_loss: 1.38359
Epoch: 4/20, step: 25720, training_loss: 2.34282
Epoch: 4/20, step: 25740, training_loss: 2.49937
Epoch: 4/20, step: 25760, training_loss: 2.21820
Epoch: 4/20, step: 25780, training_loss: 1.77890
Epoch: 4/20, step: 25800, training_loss: 2.51767
Epoch: 4/20, step: 25820, training_loss: 2.16031
Epoch: 4/20, step: 25840, training_loss: 2.97913
Epoch: 4/20, step: 25860, training_loss: 2.20875
Epoch: 4/20, step: 25880, training_loss: 2.23341
Epoch: 4/20, step: 25900, training_loss: 2.71923
Epoch: 4/20, step: 25920, training_loss: 1.82421
Epoch: 4/20, step: 25940, training_loss: 1.93138
Epoch: 4/20, step: 25960, training_loss: 2.07001
Epoch: 4/20, step: 25980, training_loss: 3.02051
Epoch: 4/20, step: 26000, training_loss: 2.05371
accuracy: 0.27, validation_loss: 2.636706829071045, num_samples: 100
Epoch: 4/20, step: 26020, training_loss: 1.63569
Epoch: 4/20, step: 26040, training_loss: 1.89147
Epoch: 4/20, step: 26060, training_loss: 2.73844
Epoch: 4/20, step: 26080, training_loss: 1.00961
Epoch: 4/20, step: 26100, training_loss: 2.14584
Epoch: 4/20, step: 26120, training_loss: 2.37025
Epoch: 4/20, step: 26140, training_loss: 1.26025
Epoch: 4/20, step: 26160, training_loss: 2.05740
Epoch: 4/20, step: 26180, training_loss: 2.61474
Epoch: 4/20, step: 26200, training_loss: 2.67845
Epoch: 4/20, step: 26220, training_loss: 1.88814
Epoch: 4/20, step: 26240, training_loss: 2.11798
Epoch: 4/20, step: 26260, training_loss: 2.46017
Epoch: 4/20, step: 26280, training_loss: 2.23730
Epoch: 4/20, step: 26300, training_loss: 1.67399
Epoch: 4/20, step: 26320, training_loss: 1.75882
Epoch: 4/20, step: 26340, training_loss: 2.49058
Epoch: 4/20, step: 26360, training_loss: 2.25938
Epoch: 4/20, step: 26380, training_loss: 2.15150
Epoch: 4/20, step: 26400, training_loss: 2.37098
Epoch: 4/20, step: 26420, training_loss: 2.51042
Epoch: 4/20, step: 26440, training_loss: 2.35375
Epoch: 4/20, step: 26460, training_loss: 2.30665
Epoch: 4/20, step: 26480, training_loss: 2.21714
Epoch: 4/20, step: 26500, training_loss: 1.83893
Epoch: 4/20, step: 26520, training_loss: 1.66084
Epoch: 4/20, step: 26540, training_loss: 1.52752
Epoch: 4/20, step: 26560, training_loss: 2.29479
Epoch: 4/20, step: 26580, training_loss: 2.79560
Epoch: 4/20, step: 26600, training_loss: 1.80341
Epoch: 4/20, step: 26620, training_loss: 2.83992
Epoch: 4/20, step: 26640, training_loss: 2.91485
Epoch: 4/20, step: 26660, training_loss: 2.35123
Epoch: 4/20, step: 26680, training_loss: 2.26388
Epoch: 4/20, step: 26700, training_loss: 2.42621
Epoch: 4/20, step: 26720, training_loss: 2.25203
Epoch: 4/20, step: 26740, training_loss: 2.03495
Epoch: 4/20, step: 26760, training_loss: 2.24795
Epoch: 4/20, step: 26780, training_loss: 2.09205
Epoch: 4/20, step: 26800, training_loss: 1.82032
Epoch: 4/20, step: 26820, training_loss: 1.82906
Epoch: 4/20, step: 26840, training_loss: 2.77084
Epoch: 4/20, step: 26860, training_loss: 1.46910
Epoch: 4/20, step: 26880, training_loss: 1.93149
Epoch: 4/20, step: 26900, training_loss: 1.87504
Epoch: 4/20, step: 26920, training_loss: 1.53152
Epoch: 4/20, step: 26940, training_loss: 2.04653
Epoch: 4/20, step: 26960, training_loss: 2.31184
Epoch: 4/20, step: 26980, training_loss: 2.83071
Epoch: 4/20, step: 27000, training_loss: 2.05526
accuracy: 0.41, validation_loss: 2.131594657897949, num_samples: 100
Epoch: 4/20, step: 27020, training_loss: 2.21462
Epoch: 4/20, step: 27040, training_loss: 2.61500
Epoch: 4/20, step: 27060, training_loss: 1.23083
Epoch: 4/20, step: 27080, training_loss: 2.05200
Epoch: 4/20, step: 27100, training_loss: 2.23599
Epoch: 4/20, step: 27120, training_loss: 1.50193
Epoch: 4/20, step: 27140, training_loss: 2.01304
Epoch: 4/20, step: 27160, training_loss: 2.37847
Epoch: 4/20, step: 27180, training_loss: 1.70519
Epoch: 4/20, step: 27200, training_loss: 2.49376
Epoch: 4/20, step: 27220, training_loss: 1.70373
Epoch: 4/20, step: 27240, training_loss: 2.50014
Epoch: 4/20, step: 27260, training_loss: 2.04094
Epoch: 4/20, step: 27280, training_loss: 1.68282
Epoch: 4/20, step: 27300, training_loss: 2.82026
Epoch: 4/20, step: 27320, training_loss: 2.12904
Epoch: 4/20, step: 27340, training_loss: 2.48615
Epoch: 4/20, step: 27360, training_loss: 2.09142
Epoch: 4/20, step: 27380, training_loss: 1.65706
Epoch: 4/20, step: 27400, training_loss: 3.07817
Epoch: 4/20, step: 27420, training_loss: 1.50975
Epoch: 4/20, step: 27440, training_loss: 2.64572
Epoch: 4/20, step: 27460, training_loss: 1.90983
Epoch: 4/20, step: 27480, training_loss: 2.34831
Epoch: 4/20, step: 27500, training_loss: 2.36435
Epoch: 4/20, step: 27520, training_loss: 2.13295
Epoch: 4/20, step: 27540, training_loss: 2.76768
Epoch: 4/20, step: 27560, training_loss: 2.20467
Epoch: 4/20, step: 27580, training_loss: 1.31172
Epoch: 4/20, step: 27600, training_loss: 2.39888
Epoch: 4/20, step: 27620, training_loss: 1.97137
Epoch: 4/20, step: 27640, training_loss: 1.88521
Epoch: 4/20, step: 27660, training_loss: 1.81862
Epoch: 4/20, step: 27680, training_loss: 2.09698
Epoch: 4/20, step: 27700, training_loss: 2.18192
Epoch: 4/20, step: 27720, training_loss: 2.07291
Epoch: 4/20, step: 27740, training_loss: 2.46105
Epoch: 4/20, step: 27760, training_loss: 1.37015
Epoch: 4/20, step: 27780, training_loss: 1.88094
Epoch: 4/20, step: 27800, training_loss: 2.09158
Epoch: 4/20, step: 27820, training_loss: 2.01208
Epoch: 4/20, step: 27840, training_loss: 2.74159
Epoch: 4/20, step: 27860, training_loss: 1.83848
Epoch: 4/20, step: 27880, training_loss: 2.32859
Epoch: 4/20, step: 27900, training_loss: 2.15305
Epoch: 4/20, step: 27920, training_loss: 2.30174
Epoch: 4/20, step: 27940, training_loss: 2.80108
Epoch: 4/20, step: 27960, training_loss: 2.00353
Epoch: 4/20, step: 27980, training_loss: 2.36230
Epoch: 4/20, step: 28000, training_loss: 2.37089
accuracy: 0.42, validation_loss: 2.2497551441192627, num_samples: 100
Epoch: 4/20, step: 28020, training_loss: 2.56460
Epoch: 4/20, step: 28040, training_loss: 2.16626
Epoch: 4/20, step: 28060, training_loss: 2.14862
Epoch: 4/20, step: 28080, training_loss: 2.14154
Epoch: 4/20, step: 28100, training_loss: 2.76033
Epoch: 4/20, step: 28120, training_loss: 2.19858
Epoch: 4/20, step: 28140, training_loss: 2.36494
Epoch: 4/20, step: 28160, training_loss: 2.22437
Epoch: 4/20, step: 28180, training_loss: 1.78179
Epoch: 4/20, step: 28200, training_loss: 2.69428
Epoch: 4/20, step: 28220, training_loss: 1.98565
Epoch: 4/20, step: 28240, training_loss: 2.22604
Epoch: 4/20, step: 28260, training_loss: 2.01330
Epoch: 4/20, step: 28280, training_loss: 2.49032
Epoch: 4/20, step: 28300, training_loss: 1.60257
Epoch: 4/20, step: 28320, training_loss: 1.28316
Epoch: 4/20, step: 28340, training_loss: 1.78989
Epoch: 4/20, step: 28360, training_loss: 2.59031
Epoch: 4/20, step: 28380, training_loss: 2.35525
Epoch: 4/20, step: 28400, training_loss: 1.91165
Epoch: 4/20, step: 28420, training_loss: 1.57190
Epoch: 4/20, step: 28440, training_loss: 2.60688
Epoch: 4/20, step: 28460, training_loss: 1.37759
Epoch: 4/20, step: 28480, training_loss: 2.41946
Epoch: 4/20, step: 28500, training_loss: 1.90463
Epoch: 4/20, step: 28520, training_loss: 1.41846
Epoch: 4/20, step: 28540, training_loss: 2.26422
Epoch: 4/20, step: 28560, training_loss: 2.08436
Epoch: 4/20, step: 28580, training_loss: 2.09474
Epoch: 4/20, step: 28600, training_loss: 2.29691
Epoch: 4/20, step: 28620, training_loss: 2.75981
Epoch: 4/20, step: 28640, training_loss: 2.55612
Epoch: 4/20, step: 28660, training_loss: 2.39242
Epoch: 4/20, step: 28680, training_loss: 2.58389
Epoch: 4/20, step: 28700, training_loss: 1.66528
Epoch: 4/20, step: 28720, training_loss: 2.44742
Epoch: 4/20, step: 28740, training_loss: 2.33154
Epoch: 4/20, step: 28760, training_loss: 1.57267
Epoch: 4/20, step: 28780, training_loss: 2.27340
Epoch: 4/20, step: 28800, training_loss: 1.24271
Epoch: 4/20, step: 28820, training_loss: 2.68013
Epoch: 4/20, step: 28840, training_loss: 2.25623
Epoch: 4/20, step: 28860, training_loss: 2.54664
Epoch: 4/20, step: 28880, training_loss: 2.00290
Epoch: 4/20, step: 28900, training_loss: 2.29210
Epoch: 4/20, step: 28920, training_loss: 1.85674
Epoch: 4/20, step: 28940, training_loss: 1.73776
Epoch: 4/20, step: 28960, training_loss: 2.46323
Epoch: 4/20, step: 28980, training_loss: 2.43710
Epoch: 4/20, step: 29000, training_loss: 1.72762
accuracy: 0.37, validation_loss: 2.010374069213867, num_samples: 100
Epoch: 4/20, step: 29020, training_loss: 2.51026
Epoch: 4/20, step: 29040, training_loss: 2.64627
Epoch: 4/20, step: 29060, training_loss: 3.20356
Epoch: 4/20, step: 29080, training_loss: 2.86354
Epoch: 4/20, step: 29100, training_loss: 1.31827
Epoch: 4/20, step: 29120, training_loss: 1.51498
Epoch: 4/20, step: 29140, training_loss: 1.46226
Epoch: 4/20, step: 29160, training_loss: 1.09725
Epoch: 4/20, step: 29180, training_loss: 2.74462
Epoch: 4/20, step: 29200, training_loss: 2.63611
Epoch: 4/20, step: 29220, training_loss: 2.67903
Epoch: 4/20, step: 29240, training_loss: 2.16270
Epoch: 4/20, step: 29260, training_loss: 2.23200
Epoch: 4/20, step: 29280, training_loss: 1.83647
Epoch: 4/20, step: 29300, training_loss: 2.72027
Epoch: 4/20, step: 29320, training_loss: 2.40262
Epoch: 4/20, step: 29340, training_loss: 3.19071
Epoch: 4/20, step: 29360, training_loss: 2.20035
Epoch: 4/20, step: 29380, training_loss: 2.60509
Epoch: 4/20, step: 29400, training_loss: 1.85622
Epoch: 4/20, step: 29420, training_loss: 3.79015
Epoch: 4/20, step: 29440, training_loss: 2.42422
Epoch: 4/20, step: 29460, training_loss: 2.08298
Epoch: 4/20, step: 29480, training_loss: 3.37365
Epoch: 4/20, step: 29500, training_loss: 1.75578
Epoch: 4/20, step: 29520, training_loss: 1.40702
Epoch: 4/20, step: 29540, training_loss: 1.64992
Epoch: 4/20, step: 29560, training_loss: 1.33050
Epoch: 4/20, step: 29580, training_loss: 2.50703
Epoch: 4/20, step: 29600, training_loss: 2.63857
Epoch: 4/20, step: 29620, training_loss: 1.90495
Epoch: 4/20, step: 29640, training_loss: 2.24665
Epoch: 4/20, step: 29660, training_loss: 1.77975
Epoch: 4/20, step: 29680, training_loss: 3.45397
Epoch: 4/20, step: 29700, training_loss: 1.73348
Epoch: 4/20, step: 29720, training_loss: 2.24972
Epoch: 4/20, step: 29740, training_loss: 2.68490
Epoch: 4/20, step: 29760, training_loss: 2.19663
Epoch: 4/20, step: 29780, training_loss: 1.86071
Epoch: 4/20, step: 29800, training_loss: 2.03676
Epoch: 4/20, step: 29820, training_loss: 1.86199
Epoch: 4/20, step: 29840, training_loss: 3.08984
Epoch: 4/20, step: 29860, training_loss: 2.31036
Epoch: 4/20, step: 29880, training_loss: 1.67865
Epoch: 4/20, step: 29900, training_loss: 2.75030
Epoch: 4/20, step: 29920, training_loss: 1.59442
Epoch: 4/20, step: 29940, training_loss: 2.21159
Epoch: 4/20, step: 29960, training_loss: 2.72042
Epoch: 4/20, step: 29980, training_loss: 1.91480
Epoch: 4/20, step: 30000, training_loss: 1.45204
accuracy: 0.44, validation_loss: 2.012746810913086, num_samples: 100
Epoch: 4/20, step: 30020, training_loss: 1.98101
Epoch: 4/20, step: 30040, training_loss: 1.61957
Epoch: 4/20, step: 30060, training_loss: 2.10963
Epoch: 4/20, step: 30080, training_loss: 1.96763
Epoch: 4/20, step: 30100, training_loss: 3.04326
Epoch: 4/20, step: 30120, training_loss: 2.44979
Epoch: 4/20, step: 30140, training_loss: 2.50594
Epoch: 4/20, step: 30160, training_loss: 2.11149
Epoch: 4/20, step: 30180, training_loss: 1.98279
Epoch: 4/20, step: 30200, training_loss: 2.13778
Epoch: 4/20, step: 30220, training_loss: 1.14906
Epoch: 4/20, step: 30240, training_loss: 1.82994
Epoch: 4/20, step: 30260, training_loss: 1.69163
Epoch: 4/20, step: 30280, training_loss: 2.02084
Epoch: 4/20, step: 30300, training_loss: 2.10672
Epoch: 4/20, step: 30320, training_loss: 2.16704
Epoch: 4/20, step: 30340, training_loss: 2.57564
Epoch: 4/20, step: 30360, training_loss: 2.18362
Epoch: 4/20, step: 30380, training_loss: 2.84141
Epoch: 4/20, step: 30400, training_loss: 2.08662
Epoch: 4/20, step: 30420, training_loss: 2.93523
Epoch: 4/20, step: 30440, training_loss: 2.97812
Epoch: 4/20, step: 30460, training_loss: 2.16079
Epoch: 4/20, step: 30480, training_loss: 2.48255
Epoch: 4/20, step: 30500, training_loss: 1.99834
Epoch: 4/20, step: 30520, training_loss: 1.65853
Epoch: 4/20, step: 30540, training_loss: 2.37632
Epoch: 4/20, step: 30560, training_loss: 1.89548
Epoch: 4/20, step: 30580, training_loss: 2.15816
Epoch: 4/20, step: 30600, training_loss: 2.47233
Epoch: 4/20, step: 30620, training_loss: 3.31648
Epoch: 4/20, step: 30640, training_loss: 1.61264
Epoch: 4/20, step: 30660, training_loss: 2.33211
Epoch: 4/20, step: 30680, training_loss: 3.27285
Epoch: 4/20, step: 30700, training_loss: 1.82092
Epoch: 4/20, step: 30720, training_loss: 2.27584
Epoch: 4/20, step: 30740, training_loss: 1.69788
Epoch: 4/20, step: 30760, training_loss: 2.24598
Epoch: 4/20, step: 30780, training_loss: 1.83434
Epoch: 4/20, step: 30800, training_loss: 1.37776
Epoch: 4/20, step: 30820, training_loss: 1.64424
Epoch: 4/20, step: 30840, training_loss: 1.88098
Epoch: 4/20, step: 30860, training_loss: 1.44754
Epoch: 4/20, step: 30880, training_loss: 1.68189
Epoch: 4/20, step: 30900, training_loss: 2.96939
Epoch: 4/20, step: 30920, training_loss: 1.88430
Epoch: 4/20, step: 30940, training_loss: 2.03128
Epoch: 4/20, step: 30960, training_loss: 2.23594
Epoch: 4/20, step: 30980, training_loss: 2.22830
Epoch: 4/20, step: 31000, training_loss: 2.20985
accuracy: 0.46, validation_loss: 2.061448574066162, num_samples: 100
Epoch: 4/20, step: 31020, training_loss: 1.72259
Epoch: 4/20, step: 31040, training_loss: 2.58676
Epoch: 4/20, step: 31060, training_loss: 1.30452
Epoch: 4/20, step: 31080, training_loss: 2.69292
Epoch: 4/20, step: 31100, training_loss: 2.37668
Epoch: 4/20, step: 31120, training_loss: 2.72082
Epoch: 4/20, step: 31140, training_loss: 2.25894
Epoch: 4/20, step: 31160, training_loss: 2.06584
Epoch: 4/20, step: 31180, training_loss: 1.62466
Epoch: 4/20, step: 31200, training_loss: 2.48692
Epoch: 4/20, step: 31220, training_loss: 2.48798
Epoch: 4/20, step: 31240, training_loss: 1.77136
Epoch: 4/20, step: 31260, training_loss: 1.42320
Epoch: 4/20, step: 31280, training_loss: 1.94756
Epoch: 4/20, step: 31300, training_loss: 1.40026
Epoch: 4/20, step: 31320, training_loss: 2.23238
Epoch: 4/20, step: 31340, training_loss: 2.01549
Epoch: 4/20, step: 31360, training_loss: 2.26534
Epoch: 4/20, step: 31380, training_loss: 2.89026
Epoch: 4/20, step: 31400, training_loss: 2.16295
Epoch: 4/20, step: 31420, training_loss: 2.83971
Epoch: 4/20, step: 31440, training_loss: 1.46151
Epoch: 4/20, step: 31460, training_loss: 2.29309
Epoch: 4/20, step: 31480, training_loss: 2.23751
Epoch: 4/20, step: 31500, training_loss: 2.05689
Epoch: 4/20, step: 31520, training_loss: 2.10269
Epoch: 4/20, step: 31540, training_loss: 2.31655
Epoch: 4/20, step: 31560, training_loss: 1.92584
Epoch: 4/20, step: 31580, training_loss: 2.70295
Epoch: 4/20, step: 31600, training_loss: 2.50562
Epoch: 4/20, step: 31620, training_loss: 1.68596
Epoch: 4/20, step: 31640, training_loss: 1.70253
Epoch: 4/20, step: 31660, training_loss: 2.84014
Epoch: 4/20, step: 31680, training_loss: 1.83597
Epoch: 4/20, step: 31700, training_loss: 1.61304
Epoch: 4/20, step: 31720, training_loss: 2.32005
Epoch: 4/20, step: 31740, training_loss: 1.77842
Epoch: 4/20, step: 31760, training_loss: 2.04587
Epoch: 4/20, step: 31780, training_loss: 1.75409
Epoch: 4/20, step: 31800, training_loss: 1.56475
Epoch: 4/20, step: 31820, training_loss: 2.19341
Epoch: 4/20, step: 31840, training_loss: 2.46707
Epoch: 4/20, step: 31860, training_loss: 1.91622
Epoch: 4/20, step: 31880, training_loss: 2.61406
Epoch: 4/20, step: 31900, training_loss: 2.28803
Epoch: 4/20, step: 31920, training_loss: 2.11978
Epoch: 4/20, step: 31940, training_loss: 1.84949
Epoch: 4/20, step: 31960, training_loss: 1.72231
Epoch: 4/20, step: 31980, training_loss: 1.47049
Epoch: 4/20, step: 32000, training_loss: 3.09279
accuracy: 0.5, validation_loss: 1.8546507358551025, num_samples: 100
Epoch: 4/20, step: 32020, training_loss: 3.01550
Epoch: 4/20, step: 32040, training_loss: 1.66274
Epoch: 4/20, step: 32060, training_loss: 2.41771
Epoch: 4/20, step: 32080, training_loss: 2.13670
Epoch: 4/20, step: 32100, training_loss: 2.28289
Epoch: 4/20, step: 32120, training_loss: 2.50339
Epoch: 4/20, step: 32140, training_loss: 2.77405
Epoch: 4/20, step: 32160, training_loss: 2.62833
Epoch: 4/20, step: 32180, training_loss: 2.41197
Epoch: 4/20, step: 32200, training_loss: 2.15126
Epoch: 4/20, step: 32220, training_loss: 2.06847
Epoch: 4/20, step: 32240, training_loss: 1.87319
Epoch: 4/20, step: 32260, training_loss: 2.65739
Epoch: 4/20, step: 32280, training_loss: 2.44534
Epoch: 4/20, step: 32300, training_loss: 2.03486
Epoch: 4/20, step: 32320, training_loss: 2.47093
Epoch: 4/20, step: 32340, training_loss: 2.51637
Epoch: 4/20, step: 32360, training_loss: 2.08652
Epoch: 4/20, step: 32380, training_loss: 1.76819
Epoch: 4/20, step: 32400, training_loss: 2.86249
Epoch: 4/20, step: 32420, training_loss: 2.64654
Epoch: 4/20, step: 32440, training_loss: 2.28395
Epoch: 4/20, step: 32460, training_loss: 1.93358
Epoch: 4/20, step: 32480, training_loss: 2.06140
Epoch: 4/20, step: 32500, training_loss: 2.09668
Epoch: 4/20, step: 32520, training_loss: 1.52192
Epoch: 4/20, step: 32540, training_loss: 1.92765
Epoch: 4/20, step: 32560, training_loss: 2.93658
Epoch: 4/20, step: 32580, training_loss: 2.99233
Epoch: 4/20, step: 32600, training_loss: 1.77401
Epoch: 4/20, step: 32620, training_loss: 2.54533
Epoch: 4/20, step: 32640, training_loss: 2.32124
Epoch: 4/20, step: 32660, training_loss: 2.08145
Epoch: 4/20, step: 32680, training_loss: 1.84950
Epoch: 4/20, step: 32700, training_loss: 1.53424
Epoch: 4/20, step: 32720, training_loss: 2.37654
Epoch: 4/20, step: 32740, training_loss: 3.04265
Epoch: 4/20, step: 32760, training_loss: 2.41222
Epoch: 4/20, step: 32780, training_loss: 2.56795
Epoch: 4/20, step: 32800, training_loss: 2.37934
Epoch: 4/20, step: 32820, training_loss: 1.59879
Epoch: 4/20, step: 32840, training_loss: 2.58139
Epoch: 4/20, step: 32860, training_loss: 2.00315
Epoch: 4/20, step: 32880, training_loss: 2.21742
Epoch: 4/20, step: 32900, training_loss: 1.24369
Epoch: 4/20, step: 32920, training_loss: 1.85298
Epoch: 4/20, step: 32940, training_loss: 2.77968
Epoch: 4/20, step: 32960, training_loss: 2.32271
Epoch: 4/20, step: 32980, training_loss: 1.79179
Epoch: 4/20, step: 33000, training_loss: 2.19113
accuracy: 0.47, validation_loss: 1.889656662940979, num_samples: 100
Epoch: 4/20, step: 33020, training_loss: 1.67213
Epoch: 4/20, step: 33040, training_loss: 2.09963
Epoch: 4/20, step: 33060, training_loss: 2.35912
Epoch: 4/20, step: 33080, training_loss: 1.05102
Epoch: 4/20, step: 33100, training_loss: 2.17062
Epoch: 4/20, step: 33120, training_loss: 1.94762
Epoch: 4/20, step: 33140, training_loss: 2.32195
Epoch: 4/20, step: 33160, training_loss: 2.14025
Epoch: 4/20, step: 33180, training_loss: 2.14618
Epoch: 4/20, step: 33200, training_loss: 1.74918
Epoch: 4/20, step: 33220, training_loss: 1.21074
Epoch: 4/20, step: 33240, training_loss: 2.39446
Epoch: 4/20, step: 33260, training_loss: 1.95660
Epoch: 4/20, step: 33280, training_loss: 1.66328
Epoch: 4/20, step: 33300, training_loss: 2.94185
Epoch: 4/20, step: 33320, training_loss: 1.73081
Epoch: 4/20, step: 33340, training_loss: 3.11893
Epoch: 4/20, step: 33360, training_loss: 2.89863
Epoch: 4/20, step: 33380, training_loss: 2.80823
Epoch: 4/20, step: 33400, training_loss: 2.21194
Epoch: 4/20, step: 33420, training_loss: 2.44296
Epoch: 4/20, step: 33440, training_loss: 2.00352
Epoch: 4/20, step: 33460, training_loss: 1.83380
Epoch: 4/20, step: 33480, training_loss: 2.27570
Epoch: 4/20, step: 33500, training_loss: 1.23957
Epoch: 4/20, step: 33520, training_loss: 2.73078
Epoch: 4/20, step: 33540, training_loss: 2.17245
Epoch: 4/20, step: 33560, training_loss: 2.01873
Epoch: 4/20, step: 33580, training_loss: 2.69853
Epoch: 4/20, step: 33600, training_loss: 1.62380
Epoch: 4/20, step: 33620, training_loss: 2.31761
Epoch: 4/20, step: 33640, training_loss: 1.99528
Epoch: 4/20, step: 33660, training_loss: 2.04671
Epoch: 4/20, step: 33680, training_loss: 1.77328
Epoch: 4/20, step: 33700, training_loss: 1.11100
Epoch: 4/20, step: 33720, training_loss: 2.17972
Epoch: 4/20, step: 33740, training_loss: 1.83186
Epoch: 4/20, step: 33760, training_loss: 2.27191
Epoch: 4/20, step: 33780, training_loss: 2.42650
Epoch: 4/20, step: 33800, training_loss: 1.45793
Epoch: 4/20, step: 33820, training_loss: 1.93243
Epoch: 4/20, step: 33840, training_loss: 2.54180
Epoch: 4/20, step: 33860, training_loss: 1.61030
Epoch: 4/20, step: 33880, training_loss: 2.50192
Epoch: 4/20, step: 33900, training_loss: 1.66274
Epoch: 4/20, step: 33920, training_loss: 2.43808
Epoch: 4/20, step: 33940, training_loss: 2.13132
Epoch: 4/20, step: 33960, training_loss: 1.99747
Epoch: 4/20, step: 33980, training_loss: 2.02194
Epoch: 4/20, step: 34000, training_loss: 1.79776
accuracy: 0.37, validation_loss: 2.1171507835388184, num_samples: 100
Epoch: 4/20, step: 34020, training_loss: 2.10207
Epoch: 4/20, step: 34040, training_loss: 1.94501
Epoch: 4/20, step: 34060, training_loss: 2.08795
Epoch: 4/20, step: 34080, training_loss: 2.43740
Epoch: 4/20, step: 34100, training_loss: 2.56564
Epoch: 4/20, step: 34120, training_loss: 1.76300
Epoch: 4/20, step: 34140, training_loss: 2.76237
Epoch: 4/20, step: 34160, training_loss: 2.18095
Epoch: 4/20, step: 34180, training_loss: 1.91758
Epoch: 4/20, step: 34200, training_loss: 2.59757
Epoch: 4/20, step: 34220, training_loss: 2.13930
Epoch: 4/20, step: 34240, training_loss: 1.83563
Epoch: 4/20, step: 34260, training_loss: 1.85662
Epoch: 4/20, step: 34280, training_loss: 2.04707
Epoch: 4/20, step: 34300, training_loss: 2.40439
Epoch: 4/20, step: 34320, training_loss: 2.04095
Epoch: 4/20, step: 34340, training_loss: 2.58992
Epoch: 4/20, step: 34360, training_loss: 1.44246
Epoch: 4/20, step: 34380, training_loss: 1.59320
Epoch: 4/20, step: 34400, training_loss: 2.91673
Epoch: 4/20, step: 34420, training_loss: 2.64900
Epoch: 4/20, step: 34440, training_loss: 1.84213
Epoch: 4/20, step: 34460, training_loss: 2.53467
Epoch: 4/20, step: 34480, training_loss: 2.70538
Epoch: 4/20, step: 34500, training_loss: 2.75223
Epoch: 4/20, step: 34520, training_loss: 2.00433
Epoch: 4/20, step: 34540, training_loss: 1.79973
Epoch: 4/20, step: 34560, training_loss: 2.13516
Epoch: 4/20, step: 34580, training_loss: 1.46609
Epoch: 4/20, step: 34600, training_loss: 3.08321
Epoch: 4/20, step: 34620, training_loss: 1.70375
Epoch: 4/20, step: 34640, training_loss: 2.66449
Epoch: 4/20, step: 34660, training_loss: 2.31695
Epoch: 4/20, step: 34680, training_loss: 2.41568
Epoch: 4/20, step: 34700, training_loss: 2.50242
Epoch: 4/20, step: 34720, training_loss: 2.05344
Epoch: 4/20, step: 34740, training_loss: 2.30503
Epoch: 4/20, step: 34760, training_loss: 1.81936
Epoch: 4/20, step: 34780, training_loss: 2.80465
Epoch: 4/20, step: 34800, training_loss: 1.90398
Epoch: 4/20, step: 34820, training_loss: 1.59034
Epoch: 4/20, step: 34840, training_loss: 2.30815
Epoch: 4/20, step: 34860, training_loss: 2.30874
Epoch: 4/20, step: 34880, training_loss: 1.54119
Epoch: 4/20, step: 34900, training_loss: 2.22466
Epoch: 4/20, step: 34920, training_loss: 2.97046
Epoch: 4/20, step: 34940, training_loss: 2.28769
Epoch: 4/20, step: 34960, training_loss: 1.21087
Epoch: 4/20, step: 34980, training_loss: 2.06750
Epoch: 4/20, step: 35000, training_loss: 2.45488
accuracy: 0.5, validation_loss: 1.9143681526184082, num_samples: 100
Epoch: 4/20, step: 35020, training_loss: 1.86743
Epoch: 4/20, step: 35040, training_loss: 1.45504
Epoch: 4/20, step: 35060, training_loss: 2.17091
Epoch: 4/20, step: 35080, training_loss: 2.12005
Epoch: 4/20, step: 35100, training_loss: 2.48763
Epoch: 4/20, step: 35120, training_loss: 1.23875
Epoch: 4/20, step: 35140, training_loss: 2.53614
Epoch: 4/20, step: 35160, training_loss: 1.67579
Epoch: 4/20, step: 35180, training_loss: 1.42710
Epoch: 4/20, step: 35200, training_loss: 2.12255
Epoch: 4/20, step: 35220, training_loss: 2.29253
Epoch: 4/20, step: 35240, training_loss: 2.80607
Epoch: 4/20, step: 35260, training_loss: 2.28817
Epoch: 4/20, step: 35280, training_loss: 2.45116
Epoch: 4/20, step: 35300, training_loss: 2.34433
Epoch: 4/20, step: 35320, training_loss: 2.43409
Epoch: 4/20, step: 35340, training_loss: 2.59317
Epoch: 4/20, step: 35360, training_loss: 2.50296
Epoch: 4/20, step: 35380, training_loss: 2.47617
Epoch: 4/20, step: 35400, training_loss: 1.07571
Epoch: 4/20, step: 35420, training_loss: 2.24959
Epoch: 4/20, step: 35440, training_loss: 2.06554
Epoch: 4/20, step: 35460, training_loss: 2.92451
Epoch: 4/20, step: 35480, training_loss: 2.76357
Epoch: 4/20, step: 35500, training_loss: 1.87773
Epoch: 4/20, step: 35520, training_loss: 3.17850
Epoch: 4/20, step: 35540, training_loss: 2.46639
Epoch: 4/20, step: 35560, training_loss: 2.38968
Epoch: 4/20, step: 35580, training_loss: 2.50384
Epoch: 4/20, step: 35600, training_loss: 1.87208
Epoch: 4/20, step: 35620, training_loss: 1.79023
Epoch: 4/20, step: 35640, training_loss: 2.46926
Epoch: 4/20, step: 35660, training_loss: 2.38816
Epoch: 4/20, step: 35680, training_loss: 2.10705
Epoch: 4/20, step: 35700, training_loss: 2.25277
Epoch: 4/20, step: 35720, training_loss: 2.35891
Epoch: 4/20, step: 35740, training_loss: 2.33272
Epoch: 4/20, step: 35760, training_loss: 2.27271
Epoch: 4/20, step: 35780, training_loss: 2.20939
Epoch: 4/20, step: 35800, training_loss: 2.92849
Epoch: 4/20, step: 35820, training_loss: 2.34625
Epoch: 4/20, step: 35840, training_loss: 1.76743
Epoch: 4/20, step: 35860, training_loss: 1.99348
Epoch: 4/20, step: 35880, training_loss: 2.68239
Epoch: 4/20, step: 35900, training_loss: 1.97685
Epoch: 4/20, step: 35920, training_loss: 2.74108
Epoch: 4/20, step: 35940, training_loss: 2.63328
Epoch: 4/20, step: 35960, training_loss: 2.22624
Epoch: 4/20, step: 35980, training_loss: 2.53731
Epoch: 4/20, step: 36000, training_loss: 1.29788
accuracy: 0.35, validation_loss: 2.2821598052978516, num_samples: 100
Epoch: 4/20, step: 36020, training_loss: 1.97981
Epoch: 4/20, step: 36040, training_loss: 1.54530
Epoch: 4/20, step: 36060, training_loss: 2.58634
Epoch: 4/20, step: 36080, training_loss: 2.11641
Epoch: 4/20, step: 36100, training_loss: 3.14133
Epoch: 4/20, step: 36120, training_loss: 2.61557
Epoch: 4/20, step: 36140, training_loss: 3.03672
Epoch: 4/20, step: 36160, training_loss: 2.10670
Epoch: 4/20, step: 36180, training_loss: 1.80033
Epoch: 4/20, step: 36200, training_loss: 1.74690
Epoch: 4/20, step: 36220, training_loss: 1.93952
Epoch: 4/20, step: 36240, training_loss: 1.39238
Epoch: 4/20, step: 36260, training_loss: 2.40066
Epoch: 4/20, step: 36280, training_loss: 2.33738
Epoch: 4/20, step: 36300, training_loss: 1.77973
Epoch: 4/20, step: 36320, training_loss: 1.57765
Epoch: 4/20, step: 36340, training_loss: 2.26452
Epoch: 4/20, step: 36360, training_loss: 2.34655
Epoch: 4/20, step: 36380, training_loss: 2.60098
Epoch: 4/20, step: 36400, training_loss: 1.48964
Epoch: 4/20, step: 36420, training_loss: 3.27901
Epoch: 4/20, step: 36440, training_loss: 1.87340
Epoch: 4/20, step: 36460, training_loss: 1.66258
Epoch: 4/20, step: 36480, training_loss: 2.58467
Epoch: 4/20, step: 36500, training_loss: 2.50104
Epoch: 4/20, step: 36520, training_loss: 2.41411
Epoch: 4/20, step: 36540, training_loss: 1.44173
Epoch: 4/20, step: 36560, training_loss: 2.11187
Epoch: 4/20, step: 36580, training_loss: 2.22275
Epoch: 4/20, step: 36600, training_loss: 2.15091
Epoch: 4/20, step: 36620, training_loss: 2.41811
Epoch: 4/20, step: 36640, training_loss: 2.45244
Epoch: 4/20, step: 36660, training_loss: 1.77938
Epoch: 4/20, step: 36680, training_loss: 1.94107
Epoch: 4/20, step: 36700, training_loss: 1.79512
Epoch: 4/20, step: 36720, training_loss: 2.59723
Epoch: 4/20, step: 36740, training_loss: 2.15968
Epoch: 4/20, step: 36760, training_loss: 2.65664
Epoch: 4/20, step: 36780, training_loss: 1.54992
Epoch: 4/20, step: 36800, training_loss: 1.95796
Epoch: 4/20, step: 36820, training_loss: 1.61941
Epoch: 4/20, step: 36840, training_loss: 2.54761
Epoch: 4/20, step: 36860, training_loss: 2.33703
Epoch: 4/20, step: 36880, training_loss: 2.28392
Epoch: 4/20, step: 36900, training_loss: 2.57501
Epoch: 4/20, step: 36920, training_loss: 1.99305
Epoch: 4/20, step: 36940, training_loss: 2.39760
Epoch: 4/20, step: 36960, training_loss: 2.64962
Epoch: 4/20, step: 36980, training_loss: 2.32185
Epoch: 4/20, step: 37000, training_loss: 2.37251
accuracy: 0.48, validation_loss: 1.9628169536590576, num_samples: 100
Epoch: 4/20, step: 37020, training_loss: 1.98696
Epoch: 4/20, step: 37040, training_loss: 1.93623
Epoch: 4/20, step: 37060, training_loss: 2.33118
Epoch: 4/20, step: 37080, training_loss: 3.29873
Epoch: 4/20, step: 37100, training_loss: 1.79065
Epoch: 4/20, step: 37120, training_loss: 3.05939
Epoch: 4/20, step: 37140, training_loss: 1.56483
Epoch: 4/20, step: 37160, training_loss: 2.88831
Epoch: 4/20, step: 37180, training_loss: 2.45043
Epoch: 4/20, step: 37200, training_loss: 2.05619
Epoch: 4/20, step: 37220, training_loss: 2.14844
Epoch: 4/20, step: 37240, training_loss: 1.62779
Epoch: 4/20, step: 37260, training_loss: 1.61911
Epoch: 4/20, step: 37280, training_loss: 1.73524
Epoch: 4/20, step: 37300, training_loss: 1.55294
Epoch: 4/20, step: 37320, training_loss: 2.88834
Epoch: 4/20, step: 37340, training_loss: 2.59184
Epoch: 4/20, step: 37360, training_loss: 1.86826
Epoch: 4/20, step: 37380, training_loss: 2.91066
Epoch: 4/20, step: 37400, training_loss: 2.75014
Epoch: 4/20, step: 37420, training_loss: 2.61505
Epoch: 4/20, step: 37440, training_loss: 2.82417
Epoch: 4/20, step: 37460, training_loss: 2.57239
Epoch: 4/20, step: 37480, training_loss: 2.16777
Epoch: 4/20, step: 37500, training_loss: 2.25121
Epoch: 4/20, step: 37520, training_loss: 1.65336
Epoch: 4/20, step: 37540, training_loss: 2.41387
Epoch: 4/20, step: 37560, training_loss: 1.72861
Epoch: 4/20, step: 37580, training_loss: 2.54649
Epoch: 4/20, step: 37600, training_loss: 1.76252
Epoch: 4/20, step: 37620, training_loss: 1.62844
Epoch: 4/20, step: 37640, training_loss: 2.14019
Epoch: 4/20, step: 37660, training_loss: 3.03630
Epoch: 4/20, step: 37680, training_loss: 2.37437
Epoch: 4/20, step: 37700, training_loss: 2.22703
Epoch: 4/20, step: 37720, training_loss: 1.99183
Epoch: 4/20, step: 37740, training_loss: 1.84446
Epoch: 4/20, step: 37760, training_loss: 2.65558
Epoch: 4/20, step: 37780, training_loss: 1.82749
Epoch: 4/20, step: 37800, training_loss: 2.93063
Epoch: 4/20, step: 37820, training_loss: 2.64252
Epoch: 4/20, step: 37840, training_loss: 2.06468
Epoch: 4/20, step: 37860, training_loss: 2.77580
Epoch: 4/20, step: 37880, training_loss: 2.35458
Epoch: 4/20, step: 37900, training_loss: 1.83493
Epoch: 4/20, step: 37920, training_loss: 1.69917
Epoch: 4/20, step: 37940, training_loss: 2.47559
Epoch: 4/20, step: 37960, training_loss: 2.13973
Epoch: 4/20, step: 37980, training_loss: 2.39657
Epoch: 4/20, step: 38000, training_loss: 2.09620
accuracy: 0.39, validation_loss: 2.174670457839966, num_samples: 100
Epoch: 4/20, step: 38020, training_loss: 2.11705
Epoch: 4/20, step: 38040, training_loss: 1.57233
Epoch: 4/20, step: 38060, training_loss: 2.62609
Epoch: 4/20, step: 38080, training_loss: 1.75234
Epoch: 4/20, step: 38100, training_loss: 2.31839
Epoch: 4/20, step: 38120, training_loss: 2.07874
Epoch: 4/20, step: 38140, training_loss: 2.06178
Epoch: 4/20, step: 38160, training_loss: 1.68832
Epoch: 4/20, step: 38180, training_loss: 2.39101
Epoch: 4/20, step: 38200, training_loss: 2.28144
Epoch: 4/20, step: 38220, training_loss: 2.04430
Epoch: 4/20, step: 38240, training_loss: 2.14878
Epoch: 4/20, step: 38260, training_loss: 1.97402
Epoch: 4/20, step: 38280, training_loss: 2.77743
Epoch: 4/20, step: 38300, training_loss: 1.76827
Epoch: 4/20, step: 38320, training_loss: 3.07836
Epoch: 4/20, step: 38340, training_loss: 2.75465
Epoch: 4/20, step: 38360, training_loss: 2.62448
Epoch: 4/20, step: 38380, training_loss: 2.37824
Epoch: 4/20, step: 38400, training_loss: 2.59538
Epoch: 4/20, step: 38420, training_loss: 2.37249
Epoch: 4/20, step: 38440, training_loss: 2.54826
Epoch: 4/20, step: 38460, training_loss: 2.45736
Epoch: 4/20, step: 38480, training_loss: 2.05925
Epoch: 4/20, step: 38500, training_loss: 2.66932
Epoch: 4/20, step: 38520, training_loss: 2.74850
Epoch: 4/20, step: 38540, training_loss: 1.54655
Epoch: 4/20, step: 38560, training_loss: 2.43181
Epoch: 4/20, step: 38580, training_loss: 2.21337
Epoch: 4/20, step: 38600, training_loss: 2.08257
Epoch: 4/20, step: 38620, training_loss: 2.43590
Epoch: 4/20, step: 38640, training_loss: 2.87373
Epoch: 4/20, step: 38660, training_loss: 1.84676
Epoch: 4/20, step: 38680, training_loss: 2.29272
Epoch: 4/20, step: 38700, training_loss: 2.30996
Epoch: 4/20, step: 38720, training_loss: 2.31201
Epoch: 4/20, step: 38740, training_loss: 2.34950
Epoch: 4/20, step: 38760, training_loss: 2.14612
Epoch: 4/20, step: 38780, training_loss: 1.33495
Epoch: 4/20, step: 38800, training_loss: 2.65159
Epoch: 4/20, step: 38820, training_loss: 2.63365
Epoch: 4/20, step: 38840, training_loss: 1.95197
Epoch: 4/20, step: 38860, training_loss: 2.25560
Epoch: 4/20, step: 38880, training_loss: 2.36552
Epoch: 4/20, step: 38900, training_loss: 2.25239
Epoch: 4/20, step: 38920, training_loss: 2.34400
Epoch: 4/20, step: 38940, training_loss: 2.24597
Epoch: 4/20, step: 38960, training_loss: 2.65439
Epoch: 4/20, step: 38980, training_loss: 2.62081
Epoch: 4/20, step: 39000, training_loss: 2.07832
accuracy: 0.36, validation_loss: 2.186208486557007, num_samples: 100
Epoch: 4/20, step: 39020, training_loss: 2.20515
Epoch: 4/20, step: 39040, training_loss: 2.35832
Epoch: 4/20, step: 39060, training_loss: 2.46207
Epoch: 4/20, step: 39080, training_loss: 2.15306
Epoch: 4/20, step: 39100, training_loss: 1.85457
Epoch: 4/20, step: 39120, training_loss: 2.83291
Epoch: 4/20, step: 39140, training_loss: 1.69391
Epoch: 4/20, step: 39160, training_loss: 1.80537
Epoch: 4/20, step: 39180, training_loss: 2.09665
Epoch: 4/20, step: 39200, training_loss: 1.35451
Epoch: 4/20, step: 39220, training_loss: 1.87949
Epoch: 4/20, step: 39240, training_loss: 2.23235
Epoch: 4/20, step: 39260, training_loss: 2.76457
Epoch: 4/20, step: 39280, training_loss: 1.44931
Epoch: 4/20, step: 39300, training_loss: 2.31013
Epoch: 4/20, step: 39320, training_loss: 2.13003
Epoch: 4/20, step: 39340, training_loss: 1.99583
Epoch: 4/20, step: 39360, training_loss: 2.89344
Epoch: 4/20, step: 39380, training_loss: 1.83366
Epoch: 4/20, step: 39400, training_loss: 2.59448
Epoch: 4/20, step: 39420, training_loss: 1.51882
Epoch: 4/20, step: 39440, training_loss: 2.74731
Epoch: 4/20, step: 39460, training_loss: 2.30954
Epoch: 4/20, step: 39480, training_loss: 2.39815
Epoch: 4/20, step: 39500, training_loss: 2.41812
Epoch: 4/20, step: 39520, training_loss: 2.20120
Epoch: 4/20, step: 39540, training_loss: 2.52267
Epoch: 4/20, step: 39560, training_loss: 2.00722
Epoch: 4/20, step: 39580, training_loss: 1.58650
Epoch: 4/20, step: 39600, training_loss: 2.17509
Epoch: 4/20, step: 39620, training_loss: 2.20785
Epoch: 4/20, step: 39640, training_loss: 2.00262
Epoch: 4/20, step: 39660, training_loss: 2.12435
Epoch: 4/20, step: 39680, training_loss: 2.44468
Epoch: 4/20, step: 39700, training_loss: 2.94988
Epoch: 4/20, step: 39720, training_loss: 1.94244
Epoch: 4/20, step: 39740, training_loss: 2.12464
Epoch: 4/20, step: 39760, training_loss: 1.89398
Epoch: 4/20, step: 39780, training_loss: 2.58125
Epoch: 4/20, step: 39800, training_loss: 1.93129
Epoch: 4/20, step: 39820, training_loss: 2.36224
Epoch: 4/20, step: 39840, training_loss: 2.67632
Epoch: 4/20, step: 39860, training_loss: 1.93354
Epoch: 4/20, step: 39880, training_loss: 1.70572
Epoch: 4/20, step: 39900, training_loss: 2.10791
Epoch: 4/20, step: 39920, training_loss: 3.22837
Epoch: 4/20, step: 39940, training_loss: 1.79894
Epoch: 4/20, step: 39960, training_loss: 2.38490
Epoch: 4/20, step: 39980, training_loss: 1.63159
Epoch: 4/20, step: 40000, training_loss: 1.33592
accuracy: 0.32, validation_loss: 2.262969732284546, num_samples: 100
Epoch: 4/20, step: 40020, training_loss: 2.33238
Epoch: 4/20, step: 40040, training_loss: 2.25623
Epoch: 4/20, step: 40060, training_loss: 2.19153
Epoch: 4/20, step: 40080, training_loss: 2.59346
Epoch: 4/20, step: 40100, training_loss: 1.94460
Epoch: 4/20, step: 40120, training_loss: 2.50704
Epoch: 4/20, step: 40140, training_loss: 2.69675
Epoch: 4/20, step: 40160, training_loss: 2.39505
Epoch: 4/20, step: 40180, training_loss: 2.92159
Epoch: 4/20, step: 40200, training_loss: 2.25857
Epoch: 4/20, step: 40220, training_loss: 2.13347
Epoch: 4/20, step: 40240, training_loss: 1.81916
Epoch: 4/20, step: 40260, training_loss: 2.23444
Epoch: 4/20, step: 40280, training_loss: 2.08664
Epoch: 4/20, step: 40300, training_loss: 2.03778
Epoch: 4/20, step: 40320, training_loss: 2.59210
Epoch: 4/20, step: 40340, training_loss: 0.81712
Epoch: 4/20, step: 40360, training_loss: 1.99489
Epoch: 4/20, step: 40380, training_loss: 2.69501
Epoch: 4/20, step: 40400, training_loss: 3.11051
Epoch: 4/20, step: 40420, training_loss: 2.66426
Epoch: 4/20, step: 40440, training_loss: 2.77419
Epoch: 4/20, step: 40460, training_loss: 1.90413
Epoch: 4/20, step: 40480, training_loss: 1.30847
Epoch: 4/20, step: 40500, training_loss: 1.96783
Epoch: 4/20, step: 40520, training_loss: 2.09310
Epoch: 4/20, step: 40540, training_loss: 1.39871
Epoch: 4/20, step: 40560, training_loss: 3.18266
Epoch: 4/20, step: 40580, training_loss: 2.48937
Epoch: 4/20, step: 40600, training_loss: 2.00719
Epoch: 4/20, step: 40620, training_loss: 2.32857
Epoch: 4/20, step: 40640, training_loss: 1.89066
Epoch: 4/20, step: 40660, training_loss: 2.15034
Epoch: 4/20, step: 40680, training_loss: 1.82377
Epoch: 4/20, step: 40700, training_loss: 2.08109
Epoch: 4/20, step: 40720, training_loss: 2.36622
Epoch: 4/20, step: 40740, training_loss: 2.61705
Epoch: 4/20, step: 40760, training_loss: 2.19918
Epoch: 4/20, step: 40780, training_loss: 2.60999
Epoch: 4/20, step: 40800, training_loss: 1.62156
Epoch: 4/20, step: 40820, training_loss: 2.83708
Epoch: 4/20, step: 40840, training_loss: 2.09153
Epoch: 4/20, step: 40860, training_loss: 1.81020
Epoch: 4/20, step: 40880, training_loss: 2.59768
Epoch: 4/20, step: 40900, training_loss: 2.10283
Epoch: 4/20, step: 40920, training_loss: 1.74112
Epoch: 4/20, step: 40940, training_loss: 2.45823
Epoch: 4/20, step: 40960, training_loss: 2.25954
Epoch: 4/20, step: 40980, training_loss: 1.73693
Epoch: 4/20, step: 41000, training_loss: 2.47899
accuracy: 0.51, validation_loss: 1.93699049949646, num_samples: 100
Epoch: 4/20, step: 41020, training_loss: 2.04708
Epoch: 4/20, step: 41040, training_loss: 2.47320
Epoch: 4/20, step: 41060, training_loss: 2.50313
Epoch: 4/20, step: 41080, training_loss: 2.16919
Epoch: 4/20, step: 41100, training_loss: 1.62179
Epoch: 4/20, step: 41120, training_loss: 1.90352
Epoch: 4/20, step: 41140, training_loss: 1.87941
Epoch: 4/20, step: 41160, training_loss: 1.52358
Epoch: 4/20, step: 41180, training_loss: 2.50832
Epoch: 4/20, step: 41200, training_loss: 2.79239
Epoch: 4/20, step: 41220, training_loss: 2.47081
Epoch: 4/20, step: 41240, training_loss: 1.98327
Epoch: 4/20, step: 41260, training_loss: 1.86072
Epoch: 4/20, step: 41280, training_loss: 2.13597
Epoch: 4/20, step: 41300, training_loss: 2.19259
Epoch: 4/20, step: 41320, training_loss: 1.55011
Epoch: 4/20, step: 41340, training_loss: 2.37029
Epoch: 4/20, step: 41360, training_loss: 1.91328
Epoch: 4/20, step: 41380, training_loss: 1.71897
Epoch: 4/20, step: 41400, training_loss: 2.41442
Epoch: 4/20, step: 41420, training_loss: 2.00196
Epoch: 4/20, step: 41440, training_loss: 1.98121
Epoch: 4/20, step: 41460, training_loss: 1.51149
Epoch: 4/20, step: 41480, training_loss: 2.30435
Epoch: 4/20, step: 41500, training_loss: 2.27069
Epoch: 4/20, step: 41520, training_loss: 2.07999
Epoch: 4/20, step: 41540, training_loss: 2.92661
Epoch: 4/20, step: 41560, training_loss: 1.59660
Epoch: 4/20, step: 41580, training_loss: 1.32857
Epoch: 4/20, step: 41600, training_loss: 1.70858
Epoch: 4/20, step: 41620, training_loss: 2.10992
Epoch: 4/20, step: 41640, training_loss: 2.71285
Epoch: 4/20, step: 41660, training_loss: 1.84989
Epoch: 4/20, step: 41680, training_loss: 1.84290
Epoch: 4/20, step: 41700, training_loss: 1.73551
Epoch: 4/20, step: 41720, training_loss: 1.72864
Epoch: 4/20, step: 41740, training_loss: 2.06345
Epoch: 4/20, step: 41760, training_loss: 2.04380
Epoch: 4/20, step: 41780, training_loss: 1.94667
Epoch: 4/20, step: 41800, training_loss: 2.72475
Epoch: 4/20, step: 41820, training_loss: 2.87386
Epoch: 4/20, step: 41840, training_loss: 2.63138
Epoch: 4/20, step: 41860, training_loss: 2.73299
Epoch: 4/20, step: 41880, training_loss: 1.54002
Epoch: 4/20, step: 41900, training_loss: 1.74126
Epoch: 4/20, step: 41920, training_loss: 2.40010
Epoch: 4/20, step: 41940, training_loss: 2.48124
Epoch: 4/20, step: 41960, training_loss: 1.46483
Epoch: 4/20, step: 41980, training_loss: 1.71705
Epoch: 4/20, step: 42000, training_loss: 2.83495
accuracy: 0.48, validation_loss: 2.123324394226074, num_samples: 100
Epoch: 4/20, step: 42020, training_loss: 2.67462
Epoch: 4/20, step: 42040, training_loss: 1.97550
Epoch: 4/20, step: 42060, training_loss: 2.31451
Epoch: 4/20, step: 42080, training_loss: 2.19491
Epoch: 4/20, step: 42100, training_loss: 2.55105
Epoch: 4/20, step: 42120, training_loss: 1.61903
Epoch: 4/20, step: 42140, training_loss: 2.04772
Epoch: 4/20, step: 42160, training_loss: 1.89177
Epoch: 4/20, step: 42180, training_loss: 1.39529
Epoch: 4/20, step: 42200, training_loss: 2.35089
Epoch: 4/20, step: 42220, training_loss: 2.44295
Epoch: 4/20, step: 42240, training_loss: 2.28911
Epoch: 4/20, step: 42260, training_loss: 2.87830
Epoch: 4/20, step: 42280, training_loss: 2.08463
Epoch: 4/20, step: 42300, training_loss: 2.04027
Epoch: 4/20, step: 42320, training_loss: 1.99341
Epoch: 4/20, step: 42340, training_loss: 2.28850
Epoch: 4/20, step: 42360, training_loss: 1.00556
Epoch: 4/20, step: 42380, training_loss: 1.88472
Epoch: 4/20, step: 42400, training_loss: 2.77789
Epoch: 4/20, step: 42420, training_loss: 1.38740
Epoch: 4/20, step: 42440, training_loss: 1.62574
Epoch: 4/20, step: 42460, training_loss: 2.13614
Epoch: 4/20, step: 42480, training_loss: 1.59373
Epoch: 4/20, step: 42500, training_loss: 1.65862
Epoch: 4/20, step: 42520, training_loss: 2.31229
Epoch: 4/20, step: 42540, training_loss: 2.26571
Epoch: 4/20, step: 42560, training_loss: 2.07990
Epoch: 4/20, step: 42580, training_loss: 2.95443
Epoch: 4/20, step: 42600, training_loss: 3.01271
Epoch: 4/20, step: 42620, training_loss: 2.14358
Epoch: 4/20, step: 42640, training_loss: 1.46646
Epoch: 4/20, step: 42660, training_loss: 2.09937
Epoch: 4/20, step: 42680, training_loss: 2.53253
Epoch: 4/20, step: 42700, training_loss: 2.46989
Epoch: 4/20, step: 42720, training_loss: 2.06972
Epoch: 4/20, step: 42740, training_loss: 2.86907
Epoch: 4/20, step: 42760, training_loss: 3.03905
Epoch: 4/20, step: 42780, training_loss: 1.29307
Epoch: 4/20, step: 42800, training_loss: 2.44373
Epoch: 4/20, step: 42820, training_loss: 2.47670
Epoch: 4/20, step: 42840, training_loss: 1.87449
Epoch: 4/20, step: 42860, training_loss: 2.35690
Epoch: 4/20, step: 42880, training_loss: 2.74700
Epoch: 4/20, step: 42900, training_loss: 1.77195
Epoch: 4/20, step: 42920, training_loss: 2.02236
Epoch: 4/20, step: 42940, training_loss: 1.76317
Epoch: 4/20, step: 42960, training_loss: 1.88997
Epoch: 4/20, step: 42980, training_loss: 3.20900
Epoch: 4/20, step: 43000, training_loss: 2.21918
accuracy: 0.41, validation_loss: 2.2426083087921143, num_samples: 100
Epoch: 4/20, step: 43020, training_loss: 2.17716
Epoch: 4/20, step: 43040, training_loss: 2.40522
Epoch: 4/20, step: 43060, training_loss: 1.90437
Epoch: 4/20, step: 43080, training_loss: 1.66376
Epoch: 4/20, step: 43100, training_loss: 2.42207
Epoch: 4/20, step: 43120, training_loss: 2.80068
Epoch: 4/20, step: 43140, training_loss: 2.04636
Epoch: 4/20, step: 43160, training_loss: 1.94599
Epoch: 4/20, step: 43180, training_loss: 2.73196
Epoch: 4/20, step: 43200, training_loss: 1.82395
Epoch: 4/20, step: 43220, training_loss: 1.80251
Epoch: 4/20, step: 43240, training_loss: 1.96630
Epoch: 4/20, step: 43260, training_loss: 1.98380
Epoch: 4/20, step: 43280, training_loss: 2.77026
Epoch: 4/20, step: 43300, training_loss: 2.21759
Epoch: 4/20, step: 43320, training_loss: 1.54608
Epoch: 4/20, step: 43340, training_loss: 2.31157
Epoch: 4/20, step: 43360, training_loss: 2.65713
Epoch: 4/20, step: 43380, training_loss: 2.13510
Epoch: 4/20, step: 43400, training_loss: 1.90472
Epoch: 4/20, step: 43420, training_loss: 1.83235
Epoch: 4/20, step: 43440, training_loss: 2.92724
Epoch: 4/20, step: 43460, training_loss: 2.31745
Epoch: 4/20, step: 43480, training_loss: 2.44506
Epoch: 4/20, step: 43500, training_loss: 2.14980
Epoch: 4/20, step: 43520, training_loss: 2.45999
Epoch: 4/20, step: 43540, training_loss: 1.98271
Epoch: 4/20, step: 43560, training_loss: 2.14167
Epoch: 4/20, step: 43580, training_loss: 1.92616
Epoch: 4/20, step: 43600, training_loss: 1.89058
Epoch: 4/20, step: 43620, training_loss: 2.20705
Epoch: 4/20, step: 43640, training_loss: 2.89806
Epoch: 4/20, step: 43660, training_loss: 1.97158
Epoch: 4/20, step: 43680, training_loss: 2.76815
Epoch: 4/20, step: 43700, training_loss: 1.80495
Epoch: 4/20, step: 43720, training_loss: 1.28092
Epoch: 4/20, step: 43740, training_loss: 2.05651
Epoch: 4/20, step: 43760, training_loss: 1.62778
Epoch: 4/20, step: 43780, training_loss: 2.00611
Epoch: 4/20, step: 43800, training_loss: 0.87188
Epoch: 4/20, step: 43820, training_loss: 1.83104
Epoch: 4/20, step: 43840, training_loss: 1.76427
Epoch: 4/20, step: 43860, training_loss: 2.08181
Epoch: 4/20, step: 43880, training_loss: 2.76970
Epoch: 4/20, step: 43900, training_loss: 2.03795
Epoch: 4/20, step: 43920, training_loss: 2.20130
Epoch: 4/20, step: 43940, training_loss: 2.58506
Epoch: 4/20, step: 43960, training_loss: 2.70598
Epoch: 4/20, step: 43980, training_loss: 2.64222

